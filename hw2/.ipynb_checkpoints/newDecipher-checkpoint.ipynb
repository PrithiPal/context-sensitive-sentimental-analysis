{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Decipherment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import ngram\n",
    "from ngram import *\n",
    "import collections\n",
    "import pprint\n",
    "import math\n",
    "import bz2\n",
    "import numpy\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pp = pprint.PrettyPrinter(width=45, compact=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    if filename[-4:] == \".bz2\":\n",
    "        with bz2.open(filename, 'rt') as f:\n",
    "            content = f.read()\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(filename, 'r') as f:\n",
    "            content = f.read()\n",
    "            f.close()\n",
    "    return content\n",
    "\n",
    "def get_statistics(content, cipher=True):\n",
    "    stats = {}\n",
    "    content = list(content)\n",
    "    split_content = [x for x in content if x != '\\n' and x!=' ']\n",
    "    length = len(split_content)\n",
    "    symbols = set(split_content)\n",
    "    uniq_sym = len(list(symbols))\n",
    "    freq = collections.Counter(split_content)\n",
    "    rel_freq = {}\n",
    "    for sym, frequency in freq.items():\n",
    "        rel_freq[sym] = (frequency/length)*100\n",
    "        \n",
    "    if cipher:\n",
    "        stats = {'content':split_content, 'length':length, 'vocab':list(symbols), 'vocab_length':uniq_sym, 'frequencies':freq, 'relative_freq':rel_freq}\n",
    "    else:\n",
    "        stats = {'length':length, 'vocab':list(symbols), 'vocab_length':uniq_sym, 'frequencies':freq, 'relative_freq':rel_freq}\n",
    "    return stats\n",
    "\n",
    "def find_mappings(ciphertext, plaintext):\n",
    "    mappings = defaultdict(dict)\n",
    "    hypotheses = defaultdict(dict)\n",
    "    \n",
    "    for symbol in ciphertext['vocab']:\n",
    "        for letter in plaintext['vocab']:\n",
    "            hypotheses[symbol][letter] = abs(math.log((ciphertext['relative_freq'][symbol]/plaintext['relative_freq'][letter])))\n",
    "    \n",
    "    for sym in hypotheses.keys():\n",
    "        winner = sorted(hypotheses[sym].items(), key=lambda kv: kv[1])\n",
    "        mappings[sym] = winner[1][0]\n",
    "    \n",
    "    return mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dmmbgbuumgbubbgbugggububgdgmbyyluugbubumgvlbbbyubggbkbduumbugumvuylggbgggbbbybbgggugubglbbdymgglgggkbkubbmugybglbubybuugbbmuubglgggubuugbgylbmgglyggggbduumbugxbgybkuguggbgbbbggmggbggybuggmdbugbubybubbgbygmggguubmggbggygbbbmybggdgggybgggkmkubggduuggyggbbbmbbbbyvuugvbkbmmmgbggbbgbdmmgvgmuugbuglglgbugbgbdgdumbbguubggbulgbblgggubuyggbugdmugbggybugdkggbbyvgyblgubuugugmbugybmbbgbbbblggbmgbumygggggbdgmglggggbumg\n"
     ]
    }
   ],
   "source": [
    "cipher = read_file(\"data/cipher.txt\")\n",
    "plaintxt = read_file(\"data/default.wiki.txt.bz2\")\n",
    "\n",
    "\n",
    "cipher_desc = get_statistics(cipher, cipher=True)\n",
    "plaintxt_desc = get_statistics(plaintxt, cipher=False)\n",
    "\n",
    "mapping = find_mappings(cipher_desc, plaintxt_desc)\n",
    "\n",
    "english_text = []\n",
    "for symbol in cipher_desc['content']:\n",
    "    english_text.append(mapping[symbol])\n",
    "decipherment = ('').join(english_text)\n",
    "print(decipherment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_list = []; \n",
    "symbol_relFreq = []; \n",
    "for x, y in cipher_desc[\"relative_freq\"].items():\n",
    "    symbol_list.append(x)\n",
    "    symbol_relFreq.append(y)\n",
    "    \n",
    "index_names = {}\n",
    "for i in range(54):\n",
    "    index_names[i] = symbol_list[i]\n",
    "    \n",
    "test_data = numpy.ones((54,26))\n",
    "df = pd.DataFrame(test_data, columns = plaintxt_desc['vocab'])\n",
    "df=df.rename(index = index_names )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def satisfy_ext_limits(phi_obj,nkeep) : \n",
    "    \n",
    "    l = dict([(i[0],0) for i in phi_obj])\n",
    "    for elem in phi_obj : \n",
    "        l[str(elem[0])]+=1\n",
    "   \n",
    "    n_lengths=list(filter(lambda x:x>nkeep,list(l.values())))\n",
    "   \n",
    "    if n_lengths == [] : \n",
    "        return True \n",
    "    else : \n",
    "        return False\n",
    "    \n",
    "def score_partial_hypothesis(cipher, phi,lm) :\n",
    "    \n",
    "    reverse_phi= dict([(i[1],i[0]) for i in phi ])\n",
    "    f_phi_list = [i[1] for i in phi]\n",
    "    \n",
    "    deciphered_tokens=[]\n",
    "    overall_score=0\n",
    "    \n",
    "    for f in cipher : \n",
    "       \n",
    "        if f in f_phi_list : \n",
    "            deciphered_tokens.append(reverse_phi[str(f)])\n",
    "        else : \n",
    "            deciphered_tokens.append(\"_\")  \n",
    "        \n",
    "    \n",
    "    arg1 = \"\".join(deciphered_tokens)\n",
    "    #print('arg1 = ',arg1)\n",
    "    score = lm.score_seq(arg1)     \n",
    "    return score\n",
    "\n",
    "def hist_prune(H,top_n) : \n",
    "    \n",
    "    scores = [float(i[1]) for i in H]\n",
    "    scores_s = sorted(H,key=lambda x:x[1])\n",
    "    return scores_s[-top_n:]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#SAMPLE_PHI=[('', ''), ('e', '—'), ('e', 'º'), ('u', 'B'), ('v', 'R')]\n",
    "#satisfy_ext_limits(SAMPLE_PHI,3)\n",
    "\n",
    "\n",
    "#PHI=[('b','B')]\n",
    "#score_partial_hypothesis(\"BURGER\",PHI,lm)\n",
    "\n",
    "#SAMPLE=[([('', ''), ('e', 'O'), ('h', 'T')], -32.71637948), ([('', ''), ('e', 'O'), ('s', 'T')], -21.396480099999998), ([('', ''), ('e', 'O'), ('o', 'T')], -39.44501403), ([('', ''), ('e', 'O'), ('j', 'T')], -33.47798432999999), ([('', ''), ('e', 'O'), ('d', 'T')], -20.987173), ([('', ''), ('e', 'O'), ('v', 'T')], -30.673667710000004), ([('', ''), ('e', 'O'), ('g', 'T')], -29.781602661000004), ([('', ''), ('e', 'O'), ('f', 'T')], -28.604963550999997), ([('', ''), ('e', 'O'), ('a', 'T')], -29.75088907), ([('', ''), ('e', 'O'), ('r', 'T')], -24.553558436), ([('', ''), ('e', 'O'), ('x', 'T')], -32.67641992), ([('', ''), ('e', 'O'), ('m', 'T')], -30.1644232), ([('', ''), ('e', 'O'), ('t', 'T')], -29.95256448), ([('', ''), ('e', 'O'), ('b', 'T')], -26.972342800000003), ([('', ''), ('e', 'O'), ('u', 'T')], -34.10374856), ([('', ''), ('e', 'O'), ('c', 'T')], -33.261454670000006), ([('', ''), ('e', 'O'), ('p', 'T')], -29.9686529), ([('', ''), ('e', 'O'), ('q', 'T')], -38.0978153), ([('', ''), ('e', 'O'), ('w', 'T')], -31.314459720000002), ([('', ''), ('e', 'O'), ('k', 'T')], -29.489249459999996), ([('', ''), ('e', 'O'), ('n', 'T')], -27.4332253), ([('', ''), ('e', 'O'), ('z', 'T')], -31.70670717), ([('', ''), ('e', 'O'), ('i', 'T')], -30.30622924), ([('', ''), ('e', 'O'), ('y', 'T')], -32.38038479), ([('', ''), ('e', 'O'), ('l', 'T')], -26.465809439999997)]\n",
    "#hist_prune(SAMPLE,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading language model from data/6-gram-wiki-char.lm.bz2...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "freq_dict=[ (k,v) for k,v in zip(cipher_desc['frequencies'].keys(),cipher_desc['frequencies'].values())]\n",
    "sorted_freq_dict=sorted(freq_dict, key=lambda x:max([v[1] for v in freq_dict])-x[1])\n",
    "sorted_symbols=[s[0] for s in sorted_freq_dict]\n",
    "\n",
    "lm = ngram.LM(\"data/6-gram-wiki-char.lm.bz2\", n=6, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def beam_search(ext_order, ext_limits,Vf,nkeep):\n",
    "    Hs = []\n",
    "    Ht = []\n",
    "    cardinality = 0\n",
    "    Hs.append(([('','')],0))\n",
    "    Ve = plaintxt_desc['vocab']\n",
    "    new_phi=[]\n",
    "    while cardinality < len(Vf) - 1:\n",
    "        f = ext_order[cardinality]\n",
    "        print(\"Hs = \",Hs)\n",
    "        print(\"Ht = \",Ht)\n",
    "        for h in Hs:\n",
    "            phi=h[0]\n",
    "            for e in Ve:\n",
    "                for p in phi : \n",
    "                    new_phi.append(p)\n",
    "                new_phi.append((e,f))\n",
    "                print('newphi={}'.format(new_phi))\n",
    "                if satisfy_ext_limits(new_phi,ext_limits):\n",
    "                    SCORE=score_partial_hypothesis(\"\".join(cipher_desc['content']),new_phi,lm)\n",
    "                    Ht.append((new_phi,SCORE))\n",
    "                new_phi=[]\n",
    "            \n",
    "        Ht = hist_prune(Ht,nkeep)\n",
    "        Hs=Ht\n",
    "        Ht=[]\n",
    "        cardinality = cardinality + 1\n",
    "    return Hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hs =  [([('', '')], 0)]\n",
      "Ht =  []\n",
      "Hs =  [([('', ''), ('h', 'O'), ('s', 'O'), ('o', 'O'), ('j', 'O'), ('d', 'O'), ('v', 'O'), ('g', 'O'), ('f', 'O'), ('a', 'O'), ('r', 'O'), ('x', 'O'), ('m', 'O'), ('t', 'O'), ('b', 'O'), ('e', 'O'), ('u', 'O'), ('c', 'O'), ('p', 'O'), ('q', 'O'), ('w', 'O'), ('k', 'O'), ('n', 'O'), ('z', 'O'), ('i', 'O'), ('y', 'O'), ('l', 'O')], -8.53472662)]\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n",
      "Hs =  []\n",
      "Ht =  []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## TESTING BEAM SEARCH ON SIMPLE 1:1 SUBSITUTION CIPHER\n",
    "\n",
    "sample_text=\"thescoreestimationfunctionneedstopredicthowgoodorbadapartialhypothesis\"\n",
    "cipher_text = sample_text.upper()\n",
    "\n",
    "s1 = get_statistics(sample_text,cipher=False)\n",
    "s2 = get_statistics(cipher_text,cipher=True)\n",
    "\n",
    "def get_sorted_syms(x1,x2) : \n",
    "    freq_dict=[ (k,v) for k,v in zip(x2['frequencies'].keys(),x2['frequencies'].values())]\n",
    "    sorted_freq_dict=sorted(freq_dict, key=lambda x:max([v[1] for v in freq_dict])-x[1])\n",
    "    sorted_symbols=[s[0] for s in sorted_freq_dict]\n",
    "    return sorted_symbols\n",
    "\n",
    "#beam_search(sorted_symbols, 1,cipher_desc['vocab'])\n",
    "ss = get_sorted_syms(s1,s2)\n",
    "\n",
    "beam_search(ss,1,s2['vocab'],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-5412.5280160000175,\n",
       " -4964.34780800002,\n",
       " -4858.411599999999,\n",
       " -9907.329568000006,\n",
       " -5650.728688000006,\n",
       " -7652.8057119999785,\n",
       " -6658.504223999993,\n",
       " -6543.461887999999,\n",
       " -4584.423407999982,\n",
       " -4960.800608000018,\n",
       " -10063.048272,\n",
       " -6322.216527999989,\n",
       " -4557.870319999997,\n",
       " -7034.856336000015,\n",
       " -4084.861689599994,\n",
       " -6332.424063999991,\n",
       " -5977.461696000011,\n",
       " -6671.305472000022,\n",
       " -11104.128079999973,\n",
       " -6887.577728000008,\n",
       " -8201.991504000032,\n",
       " -4812.038864,\n",
       " -10781.95116799998,\n",
       " -4826.4549760000045,\n",
       " -7083.239311999987,\n",
       " -5648.251311999994]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores=[float(i[1]) for i in HT]\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
