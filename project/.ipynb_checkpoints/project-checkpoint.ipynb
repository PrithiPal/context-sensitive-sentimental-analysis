{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context Sensitive Lexicon Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation\n",
    "\n",
    "Broadly, we started our exploration in techniques used in Aspect Based Sentimental analysis which is a crucial part of Opinion mining. The Aspect based concerns itself with two major tasks : Identifying features (or feature extraction) and score the sentiment the user has about the feature. Now the latter part involves using pre-defined lexicon libraries to put a “score” which justifies user’s feeling and normally accepted methods are lexicon-based. \n",
    "\n",
    "This way we went specific to improving the lexicon libraries and particularly the scores they have because it greatly effects the score later on in  Aspect SA stage. We were particularly interested in the “localness” or the effect of a context on a lexicon score. The Paper “Inducing Domain-Specific Sentiment Lexicons from Unlaabeled Copora” exactly reviews this problem and tackles it variety of method.\n",
    "\n",
    "Mainly we,\n",
    "\t- Re-obtain the results from the paper and the understand underlying algorithms.\n",
    "\t- Researched on the evaluation methods for improved lexicons because we were not satisfied with it. \n",
    "\t- Change the parameter to models such as Word-embeddings or seed words to produce new context-Sensitive lexicons.\n",
    "\n",
    "A very intuitive explanation behind this method is that the model take existing lexicons(binary or continuous scores), associate their word-vectors and use the random-walk label propagation algorithm to score words.\n",
    "\n",
    "The random-walk propagation essentially leverages the edge-weights as provided by distance between two words in embeddings. These distances are the source of context-sensitive information which the model make uses of. Starting from the Positive seed words, the decay in the polarity of score (from +1/-1) increases with the edges and essentially diminishes after traversing certain number of edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach \n",
    "\n",
    "The main approach is using word-embeddings to get a semantic space where the weight edges defined by : \n",
    "\n",
    "$E_{i,j}\\ =\\ \\arccos\\big( -\\frac{w_i w_j}{|w_i| |w_j|}\\ \\big)$\n",
    "\n",
    "In this semantic space, the label propagation algorithm propagates the sentiment of seedwords to its neighbors where the sentiment of a word is proportional to the number of edges between the seed-word and the word i. Or \n",
    "\n",
    "$score_w\\ \\propto \\ distance(w,score_{seed_i}) \\forall i \\in number\\ of\\ seeds (positive\\ or\\ negative) $. \n",
    "\n",
    "#### The code for label propagation probabilistic algorithm is as follows (reverse engineered psudocode from python code) : \n",
    "\n",
    "    \n",
    "    words = embeddings.iw  ## unique tokens\n",
    "    \n",
    "    ## dense embeddings or M*N matrix (M=tokens, N=embedding vector of tokens)\n",
    "    M = transition_matrix(embeddings, **kwargs) \n",
    "    \n",
    "    ## subset of words which are positive. subset of words which are negative\n",
    "    pos, neg = teleport_set(words, positive_seeds), teleport_set(words, negative_seeds) \n",
    "    \n",
    "    ## iterative changes to seed values\n",
    "    def update_seeds(r):\n",
    "        r[pos] = 1\n",
    "        r[neg] = -1\n",
    "    r = run_iterative(M, np.zeros(M.shape[0]), update_seeds, **kwargs)\n",
    "    return {w: r[i] for i, w in enumerate(words)}\n",
    "    \n",
    "    where \n",
    "    \n",
    "    ## R_new = M*R until |R_new-R|>tolerance\n",
    "    def run_iterative(M, r, update_seeds, max_iter=50, epsilon=1e-6, **kwargs):\n",
    "    for i in range(max_iter):\n",
    "        last_r = np.array(r)\n",
    "        r = np.dot(M, r)\n",
    "        update_seeds(r)\n",
    "        if np.abs(r - last_r).sum() < epsilon:\n",
    "            break\n",
    "    return r\n",
    "\n",
    "\n",
    "\n",
    "Note : I tried writing it into a pseudo-code format, the lines were not stacking nicely as they were in Anoop's hw description, so I wrote python and pretty close to pseudo code format\n",
    "\n",
    "\n",
    "#### Other approach is the Graph Propagation algorithm. Going through the code, it was evident that graph prop, created a similarity matrix and propagated on that "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "https://github.com/williamleif/socialsent \n",
    "The main framework of the project comes from this. We made tonnes of change to adapt to our specific situation. Based on the paper “Inducing Domain-Specific Sentiment Lexicons from Unlabeled Corpora”. \n",
    "\n",
    "https://github.com/zeeeyang/lexicon_rnn\n",
    "This is the code base for the paper “Context-Sensitive Lexicon Features for Neural Sentiment Analysis”\n",
    "\n",
    "The right lexicon libraries comes from these two repositories. \n",
    "\n",
    "https://www.kaggle.com/yelp-dataset/yelp-dataset\n",
    "Took the yelp_academic_dataset_review.json, extracted the 100k sentences out of it. \n",
    "\\ ## head -n 100000 > yelp_100k.json\n",
    "\n",
    "https://github.com/cmasch/word-embeddings-from-scratch/blob/master/Create_Embeddings.ipynb\n",
    "We altered this code to create our own embeddings on the yelp_100k data. \n",
    "\n",
    "https://github.com/dipanjanS/text-analytics-with-python \n",
    "We took the movie_review.csv from this GitHub which is the codebase for the book “Text-analytics with python”. We read the chapter on Sentimental Analysis and borrowed the code for unsupervised lexicon-based evaluation.\n",
    "\n",
    "\n",
    "Research Papers : \n",
    "\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "As we explained in broad about the paper’s approach that we considered. We initially thought it would not take much time to implement the paper as all the code was made open-source. However it came to our surprise that the code was very dense and un-documented for whole part. Also being written in python2 adds to the already worsened situation.\n",
    "\n",
    "To deal, we first went over the package and understood the files and the interaction between them. Extracted those files into the project’s directory, made changes which are python version specific and assembled a version of the paper’s algorithm. Our majority of the time took for this purpose. \n",
    "\n",
    "\n",
    "Our side of contribution mainly lined up on gathering the code in different places as we referred four different GitHub works and putting up together to have a consistent and logical project workflow. In addition to it, we altered a lot of hyper-paramter(free-variable) for the models such as word-embeddings, lexicons and approach \n",
    "\n",
    "From the scratch our team did the following : \n",
    "\n",
    "1. Pre-train the word-embeddings on a yelp-dataset\n",
    "2. Conduct an evaluation on a Corpus with new and old lexicons for comparisons.\n",
    "3. Expanded the lexicons libraries as stated in [GOOGLE PAPER]\n",
    "4. Tuning hyper-parameters to see the effect on the final lexicon scoring. This included : \n",
    "    1. Generating our own WE and tagging the sentiment scores on a held-out data.\n",
    "    2. Experimented with the Seedwords as given originally in the paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental Setup\n",
    "\n",
    "The default evaluations as cited in the paper are : \n",
    "\t\t1. AUC score on Binary classification (positive and negative) : \n",
    "\t\t2. Ternrary score (positive, negative and neural words)\n",
    "\t\t3. Kendall rank-correlation\n",
    "\n",
    " \t\n",
    "\n",
    "The actual SENTPROP package which have all the methods defined for the new lexical polarity induction were available, but they required specific word-embeddings which were not available in the documentation. \n",
    "\n",
    "However, the best algorithm SENTPROP which is random-walk using label-propagation method we were able to use because it relied on popular embeddings such as Google or Glove. \n",
    "\n",
    "Other prominent methods included PMI, densify (orthagonal projection of embeddings) \n",
    "\n",
    "So to the code we were able to run, we choose to do this : \n",
    "\n",
    "1. Load Lexicons (Financial, Standard-English, Sentiwn140, SST and others)\n",
    "2. Use Glove and custom trained WE on yelp 100k sentences. Although we have options to incorporate other embeddings such as twitter but big embeddings were not accomodate nicely in our VM. So just restricted to the embeddings< 1 GB.\n",
    "4. See the AUC, ternary evaluation and kendall scores for all lexicons. we have three small vocab lexicons for easy testing(finance,standard-english,twitter) and three large vocab lexicons (sst,senti140 and sentiwordnet)\n",
    "5. We get the three metrics for five lexicons (three are small vocab, three are big vocab) and change word-embedding. These gives us 3*5*2 = 30 numbers in the end\n",
    "\n",
    "6. Evaluated the impact of the lexicon change through generating lexicon based sentiments through scoring function : \n",
    "\n",
    "$polarity_{sentence}= \\frac{\\sum_{i=1}^{n} Score_{positive_{i}} - \\sum_{i=1}^{n}Score_{negative_{i}}}{T}$\n",
    "where\n",
    "$Score_{positive_i} and Score_{negative_i} $ are negative and positive score for token $i$ in a review\n",
    "\n",
    "Obtain the Confusion Matrix with Precision,Recall,F1-score on correct/incorrect sentiment predictions. For this we uses movie_reviews dataset from Text_Analytics_With_python github. In this study if we vary lexicon or word-embedding or seedwords or multiple, we will get different classification scores. This is sort of grid search with lexicon,word-embedding,seedwords as hyperparameters. But we do not cover all the search-space. \n",
    "\n",
    "We use embedding = glove, seedwords = default and just varies the lexicon libraries and see how new calculated compare classification scores compared to old lexicons.\n",
    "\n",
    "7. If this succeeds then advance it and replace with a context-lexicon based neural network as mentioned in paper \"Context-Sensitive Lexicon Features for Neural Sentiment Analysis\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Analysis\n",
    "\n",
    "Because the primary purpose of the study is to experiment with the variables and see their effects on the final metrics. So first of all we calculated the arc,ternrary and kendall correlation for three lexicon sets finance, standard-english and twitter.\n",
    "\n",
    "The results are as below : \n",
    "\n",
    "|  Embedding | Yelp       | Glove | Yelp       | Glove    \n",
    "| :-------   | :-------   | :----------:| :-------   | :----------: \n",
    "| Technique  |  Label-propagation | Graph-propagation |  Label-propagation | Graph-propagation\n",
    "| Finance    |  (0.94, 0.46, 0.33) | (0.76, 0.46, 0.32) |  (0.94, 0.46, 0.33) | (0.76, 0.46, 0.32)\n",
    "| SE         |  (0.81, 0.24, 0.37) | (0.59, 0.24, 0.21) |  (0.94, 0.46, 0.33) | (0.59, 0.24, 0.21) \n",
    "| Twitter    |  (0.86, 0.38, 0.4) | (0.71, 0.38, 0.39) |  (0.74, 0.38, 0.40) | (0.71, 0.38, 0.39)\n",
    "\n",
    "The research author gave these numbers for these lexicons (Only SENTPROP method) : \n",
    "\n",
    "|  Embedding |  auc | ternrary F1 | tao (kendall)      \n",
    "| :-------   | :-------   | :----------:| :----------:|\n",
    "| Finance | 0.91 | 0.63 | -\n",
    "| SE | 0.83 | 0.53 | 0.28\n",
    "| Twitter   |  0.86 | 0.6 | 0.5\n",
    "\n",
    "All these lexicons were trained in different word-embeddings which were domain-sensitive with vocab > $2*10^7$ tokens.\n",
    "\n",
    "As we can see, we calculated the three evaluation metrics by changing the word-embedding settings, lexicons and propagation technique. Some inferrences : \n",
    "\n",
    "- Changing the embedding does not influence the metric scores in Graph-Propagation method\n",
    "- Comparing the results of two techniques, among all the three lexicons, the Graph propagation method gave worsened scores as compared the label propagation.\n",
    "- Comparing again the three lexicons, the fiance lexicon have the most promising scores with highest three metrics compared two the other lexicons libraries. \n",
    "- Again to note that these three are small vocab lexcicons (<10k)\n",
    "\n",
    "At average for all three lexicons and other params, the three-metric pair is worse in our exerperiment when compared to authors. The reason being the limited word-embedding size and using same embeddings for different domains. Author literally emphasised on the fact to use domain-specific word-embedding to induce lexicons to perform sentiment analysis on that domain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work\n",
    "\n",
    "What could be fixed in your approach. What you did not have time to finish, but you think would be a useful addition to your project. In the future we think the following things can be expanded because either we did not had enough computational resource or time to dig deeper.\n",
    "\n",
    "1. Implement the Bi-directional LSTM as stated in the paper “Context-Sensitive Lexicon Features for Neural Sentiment Analysis”. This will act as another re-affirmation to the effect of re-scored lexicons by label-propagation method. If conducted. If we had more time, this probably had made study more complete.\n",
    "\n",
    "2. Write an automated method for the seed-generation given a context. The probable heuristic includes frequency method for adjectives, pairs coming near the known sentiment words. Can use the Google paper for lexicon expansion (with synonyms, antonym, hyponyms,hypernym relations).\n",
    "\n",
    "3. Use some techniques as given in the google paper (which builds a summarize in the end), for the aspect and dynamic extraction of features. This is more on the side of lexicon creationg rather than scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "data =  /home/ubuntu/workspace/nlpclass-1187-g-Mad_Titans/project/embeddings_socialsent/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from normalization import normalize_accented_characters, html_parser, strip_html\n",
    "from utils import display_evaluation_metrics, display_confusion_matrix, display_classification_report\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sentiwordnet')\n",
    "import dynet as dy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "from nltk.collocations import *\n",
    "import socialsent_util\n",
    "import polarity_induction_methods\n",
    "from representations.representation_factory import create_representation\n",
    "import constants\n",
    "from evaluate_methods import run_method\n",
    "import polarity_induction_methods\n",
    "from evaluate_methods import binary_metrics,ternary_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the lexicons from stanford paper \"Incuding Domain-Specific Sentiment Lexicons from Unalabeled Copora\"\n",
    "\n",
    "\n",
    "It uses a simple heuristic to compare lexicons. For two given lexicon libraries, it counts the common words between thme and the scores. The agreement heuristic measures the mutual lexicons in both libs having same scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(name, remove_neutral=True):\n",
    "    lexicon = socialsent_util.load_json(\"./lexicons_socialsent/\"+ name + '.json')\n",
    "    return {w: p for w, p in lexicon.items() if p != 0} if remove_neutral else lexicon\n",
    "\n",
    "def compare_lexicons_binary(print_disagreements=False):\n",
    "    lexicons = {\n",
    "        \"inquirer\": load_lexicon(\"inquirer\", False),\n",
    "        \"mpqa\": load_lexicon(\"mpqa\", False),\n",
    "        \"bingliu\": load_lexicon(\"bingliu\", False),\n",
    "    }\n",
    "\n",
    "    for l in lexicons:\n",
    "        print( l, len(lexicons[l]), len([w for w in lexicons[l] if lexicons[l][w] != 0]))\n",
    "\n",
    "    for l1, l2 in itertools.combinations(lexicons.keys(), 2):\n",
    "        ps1, ps2 = lexicons[l1], lexicons[l2]\n",
    "        common_words = set(ps1.keys()) & set(ps2.keys())\n",
    "        print( l1, l2, \"agreement: {:.2f}\".format(\n",
    "            100.0 * sum(1 if ps1[w] == ps2[w] else 0 for w in common_words) / len(common_words)))\n",
    "        common_words = set([word for word in ps1.keys() if ps1[word] != 0]) & \\\n",
    "                       set([word for word in ps2.keys() if ps2[word] != 0])  \n",
    "        print (l1, l2, \"agreement ignoring neutral: {:.2f}\".format(\n",
    "            100.0 * sum(1 if ps1[w] * ps2[w] == 1 else 0 for w in common_words) / len(common_words)))\n",
    "        \n",
    "        if print_disagreements and l1 == 'opinion' and l2 == 'inquirer':\n",
    "            for w in common_words:\n",
    "                if lexicons[l1][w] != lexicons[l2][w]:\n",
    "                    print (w, lexicons[l1][w], lexicons[l2][w])\n",
    "      \n",
    "    \n",
    "## ALL THESE LEXICONS ARE 2-CLASS SENTIMENTS. 1 = POSITIVE; -1 = NEGATIVE\n",
    "finance_lexicons=load_lexicon('finance')\n",
    "bingliu_lexicons=load_lexicon('bingliu')\n",
    "inquirer_lexicons=load_lexicon('inquirer')\n",
    "mpqa_lexicons=load_lexicon('mpqa')\n",
    "twitter_lexicons=load_lexicon('twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we import the lexicons as mentioned in the paper \"Context-Sensitive Lexicons Features for Neural Sentiment Analysis\".\n",
    "\n",
    "- senti140 : built from the point-wise mutual information using distant supervision \n",
    "- sentiwn : From the SentimentWordNet3.0. Originally was probability between 0 and 1 but scaled to -2,2 but author\n",
    "- sst : SD-Lex as mentioned in the paper, excluding the all neutral words and adding the aforementioned offset -2 to each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZEEYANG_LEXICONS='lexicons_zeeyang'\n",
    "def read_zeeyang_lexicons(fname) : \n",
    "    \n",
    "    polarities=defaultdict()\n",
    "    for line in open(fname,'r') : \n",
    "        token=line.split(\" \")[0]\n",
    "        score=line.split(\" \")[1]\n",
    "        polarities[token]=score\n",
    "        \n",
    "    return polarities\n",
    "\n",
    "## THESE LEXICONS HAVE CONTINOUS SCORES (BETWEEN -1 AND 1 )\n",
    "senti140_lexicons=read_zeeyang_lexicons(ZEEYANG_LEXICONS+\"/sentiment140.lex\")\n",
    "sentiwn_lexicons=read_zeeyang_lexicons(ZEEYANG_LEXICONS+\"/sentiwordnet.lex\")\n",
    "sst_lexicons=read_zeeyang_lexicons(ZEEYANG_LEXICONS+\"/stanford.tree.lexicon\")\n",
    "\n",
    "\n",
    "## THE CODE CORRECTS THE SENTI140 and SENTIWORDNET\n",
    "\n",
    "def correct_format(lexicons) : \n",
    "    new_lexicons=defaultdict()\n",
    "    for w in lexicons: \n",
    "        score=lexicons[w]\n",
    "        score=score[:-2]\n",
    "        score=float(score)\n",
    "        new_lexicons[w]=score\n",
    "    return new_lexicons\n",
    "\n",
    "senti140_lexicons=correct_format(senti140_lexicons)\n",
    "sentiwn_lexicons=correct_format(sentiwn_lexicons)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polarity values of imported lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLARTTY VALUES OF IMPORTED LEXICONS\n",
      "Finance\n",
      "[-1  1]\n",
      "Bingliu\n",
      "[-1  1]\n",
      "Inquirer\n",
      "[-1  1]\n",
      "Twitter\n",
      "[-1  1]\n",
      "Senti140\n",
      "[-1.25  -0.798  0.049 ... -1.033 -1.876 -2.   ]\n",
      "SentiWordNet\n",
      "[2.25 0.75 1.75 3.25 1.   0.25 1.25 2.5  0.5  1.5  3.   2.75 3.5  3.75\n",
      " 4.   0.  ]\n",
      "SST\n",
      "['2\\n' '3\\n' '1\\n' '4\\n' '0\\n']\n",
      " VOCAB LENGTH OF THE LEXION LIBRARIES \n",
      "========== Finance ========== \n",
      "2709\n",
      "========== Bingliu ========== \n",
      "6785\n",
      "========== Inquirer ========== \n",
      "3457\n",
      "========== Twitter ========== \n",
      "1277\n",
      "========== Senti140 ========== \n",
      "62468\n",
      "========== Sentiwordnet ========== \n",
      "32980\n",
      "========== SST ========== \n",
      "19465\n"
     ]
    }
   ],
   "source": [
    "print(\"POLARTTY VALUES OF IMPORTED LEXICONS\")\n",
    "print(\"Finance\")\n",
    "print(pd.DataFrame(list(finance_lexicons.values()),columns=['score'])['score'].unique())\n",
    "print(\"Bingliu\")\n",
    "print(pd.DataFrame(list(bingliu_lexicons.values()),columns=['score'])['score'].unique())\n",
    "print(\"Inquirer\")\n",
    "print(pd.DataFrame(list(inquirer_lexicons.values()),columns=['score'])['score'].unique())\n",
    "print(\"Twitter\")\n",
    "print(pd.DataFrame(list(twitter_lexicons.values()),columns=['score'])['score'].unique())\n",
    "\n",
    "print(\"Senti140\")\n",
    "print(pd.DataFrame(list(senti140_lexicons.values()),columns=['score'])['score'].unique())\n",
    "print(\"SentiWordNet\")\n",
    "print(pd.DataFrame(list(sentiwn_lexicons.values()),columns=['score'])['score'].unique())\n",
    "print(\"SST\")\n",
    "print(pd.DataFrame(list(sst_lexicons.values()),columns=['score'])['score'].unique())\n",
    "\n",
    "\n",
    "LEXICON_LIST=[finance_lexicons,bingliu_lexicons,inquirer_lexicons,twitter_lexicons,senti140_lexicons,sentiwn_lexicons,sst_lexicons]\n",
    "LEXICON_LABELS=['Finance','Bingliu','Inquirer','Twitter','Senti140','Sentiwordnet','SST']\n",
    "\n",
    "print(\" VOCAB LENGTH OF THE LEXION LIBRARIES \")\n",
    "for i,l in enumerate(LEXICON_LIST) : \n",
    "    \n",
    "    \n",
    "    print(\"========== {} ========== \".format(LEXICON_LABELS[i]))\n",
    "    print(len(LEXICON_LIST[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Lexicons with mutual word scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inquirer 8640 3457\n",
      "mpqa 6886 6462\n",
      "bingliu 6785 6785\n",
      "inquirer mpqa agreement: 82.47\n",
      "inquirer mpqa agreement ignoring neutral: 98.50\n",
      "inquirer bingliu agreement: 84.39\n",
      "inquirer bingliu agreement ignoring neutral: 98.74\n",
      "mpqa bingliu agreement: 99.19\n",
      "mpqa bingliu agreement ignoring neutral: 99.44\n"
     ]
    }
   ],
   "source": [
    "## COMPARING THE BINARY LEXICONS \n",
    "compare_lexicons_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Induction : the idea is to generate the lexicons provided the corpus. This method makes sure that the lexicon are sensitive to the context they are drawn from. They may prove useful if we would like to assess them in a simiar context. For instance, financial lexicons will reflect better sentiments than using general lexicons such as SentiWordNet. Three ways purposed for induction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLARITY INDUCTION METHOD : This is used for re-scoring of the lexicons(tokens) by taking information from the word-embeddings (domain-specific), positive and the negative seed words.\n",
    "\n",
    "This just calls the actual method (method=polarity_induction_methods.label_propagation with seeds and embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### THIS IS THE FUNCTION FOR INDUCING LEXICONS GIVEN THE SEEDS, EMBEDDINGS AND THE METHOD.\n",
    "def run_method(positive_seeds, negative_seeds, embeddings, transform_embeddings=False, post_densify=False,\n",
    "        method=polarity_induction_methods.densify, **kwargs):\n",
    "    \n",
    "    print(\"THE INTERNAL RUN_METHOD IS RUNNING...\")\n",
    "    \n",
    "    if transform_embeddings:\n",
    "        print (\"Transforming embeddings...\")\n",
    "        embeddings = embedding_transformer.apply_embedding_transformation(embeddings, positive_seeds, negative_seeds, n_dim=50)\n",
    "    \n",
    "    print(\"AFTER EMBEDDING TRANSFORM \",embeddings)\n",
    "    \n",
    "    ## using densify method\n",
    "    if post_densify:\n",
    "        polarities = method(embeddings, positive_seeds, negative_seeds, **kwargs)\n",
    "        top_pos = [word for word in \n",
    "                sorted(polarities, key = lambda w : -polarities[w])[:150]]\n",
    "        top_neg = [word for word in \n",
    "                sorted(polarities, key = lambda w : polarities[w])[:150]]\n",
    "        top_pos.extend(positive_seeds)\n",
    "        top_neg.extend(negative_seeds)\n",
    "        return polarity_induction_methods.densify(embeddings, top_pos, top_neg)\n",
    "    \n",
    "    \n",
    "    positive_seeds = [s for s in positive_seeds if s in embeddings]\n",
    "    negative_seeds = [s for s in negative_seeds if s in embeddings]\n",
    "    \n",
    "    \n",
    "    return method(embeddings, positive_seeds, negative_seeds, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEXICON INDUCTION ON STANDARD ENGLISH \n",
    "\n",
    "Paper referred here is \"Inducing Domain-Specific Sentiment Lexicons from Unlabaled Corpora\"\n",
    "\n",
    "In down here, The SENTPROP as mentioned in the primary lexicon induction paper uses label propagation with probabilities. the method itself can be found in the polarity_induction_methods.py with approaches too.\n",
    "\n",
    "We were able to run the label_propagate_prob on our system because it compute the GIGA embeddings (see the representation/embeddings.py ) to prepare lexicon embeddings and put inside the algorithm.\n",
    "\n",
    "Other wonderful methods such as pmi or graph_propagation used the SVD Embedding which needs more input than a standard word2vec libraries. It expected some numpy files but there was no documentation in the code about what those files denote, So we were not able to run. \n",
    "\n",
    "As cited in the paper, the SVD embedding style gave superior results. But we implemented it due to above bottleneck. Even people have strong opinions to deviate away from Neural word2vec models to more models such as SVD.\n",
    "https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/\n",
    "\n",
    "\n",
    "Only working model possible is the label_propagate_prob. But if you want to incorporate other methods, possibly look into polarity_induction_methods.py, each methods use embedding styles described in the representation/embeddings.py. The embeddings.py expectes some files whose path is defined in constants.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_lexicon_polarities(parent_lexicon,positive_seeds,negative_seeds,technique,embedding_type) : \n",
    "    \n",
    "    ## LOAD THE WORD-EMBEDDINGS : \n",
    "    EMBEDDEING_TYPE=''\n",
    "    if embedding_type=='GLOVE' : \n",
    "        EMBEDDING_TYPE=constants.GLOVE_EMBEDDINGS\n",
    "    elif embedding_type=='YELP' : \n",
    "        EMBEDDING_TYPE=constants.YELP_EMBEDDINGS\n",
    "        \n",
    "        \n",
    "    eval_words = set(parent_lexicon.keys())\n",
    "\n",
    "    #EMBEDDING_TYPE = constants.GLOVE_EMBEDDINGS\n",
    "    EMBEDDING = create_representation(\"GIGA\", constants.GLOVE_EMBEDDINGS,eval_words.union(positive_seeds).union(negative_seeds))\n",
    "\n",
    "    embed_words = set(EMBEDDING.iw)\n",
    "    eval_words = eval_words.intersection(EMBEDDING)\n",
    "    eval_words = [word for word in eval_words  if not word in positive_seeds and not word in negative_seeds]\n",
    "\n",
    "    ## TRAIN THE BEST ALGORITHM : SENTPROP and get polarities re-scored\n",
    "    \n",
    "\n",
    "    \n",
    "    polarities=defaultdict()\n",
    "    if technique=='label_propagate_prob' : \n",
    "        \n",
    "        polarities = run_method(positive_seeds, negative_seeds, \n",
    "                    EMBEDDING.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "                    method=polarity_induction_methods.label_propagate_probabilistic,beta=0.99, nn=10)\n",
    "        \n",
    "        return polarities,eval_words\n",
    "    \n",
    "    \n",
    "    elif technique=='graph_propagate' : \n",
    "        polarities = run_method(positive_seeds, negative_seeds, \n",
    "                    EMBEDDING.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "                    method=polarity_induction_methods.graph_propagate.T=10)\n",
    "        \n",
    "        return polarities,eval_words\n",
    "        \n",
    "    \n",
    "    elif technique == 'pmi' : \n",
    "        \n",
    "        #hist_words = set(hist_embed.iw)\n",
    "        #eval_words = eval_words.intersection(hist_words)\n",
    "\n",
    "        #eval_words = [word for word in eval_words if not word in positive_seeds and not word in negative_seeds] \n",
    "        print( \"Evaluating with \", len(eval_words), \"out of\", len(lexicon))\n",
    "\n",
    "        print (\"PMI\")\n",
    "        polarities = run_method(positive_seeds, \n",
    "                                negative_seeds,\n",
    "                                EMBEDDING.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "                                method=polarity_induction_methods.bootstrap,\n",
    "                                score_method=polarity_induction_methods.pmi,\n",
    "                                )\n",
    "        return polarities,eval_words\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SEEDS WORDS\n",
    "\n",
    "We took the default seed words for initializing the label propagation. There are some works such as https://pdfs.semanticscholar.org/a30b/57d80e9f5665f2cd5a2e65887c673e32e8b1.pdf?fbclid=IwAR3bBQIRDf_Sy9fh5Aj1xVVvt2Ki_L6YmzsLs2XBaPR3hMXkLq_V3pwnuJI \n",
    "\n",
    "to have more rationale thought into choosing the seeds words. However we kept it as it is and suggested as a future work suggestion in the end. These Lexicon are from the https://github.com/williamleif/socialsent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## These Lexicon are from the https://github.com/williamleif/socialsent\n",
    "## TRAINING THE LABEL-PROPAGATION FOR THE RE-SCORING OF POLARITIES FROM PRE-DETERMINED LEXICONS (MADE FROM WORD EMBEDDINGS)\n",
    "\n",
    "## INQUIRER is a standard english lexicon\n",
    "INQUIRER = load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "\n",
    "FINANCE_LEXICONS=load_lexicon('finance')\n",
    "TWITTER_LEXICONS=load_lexicon('twitter')\n",
    "\n",
    "THREE_WAY_LEXICON = kuperman = load_lexicon(\"kuperman\", remove_neutral=False)\n",
    "\n",
    "POSITIVE_FINANCE = [\"successful\", \"excellent\", \"profit\", \"beneficial\", \"improving\", \"improved\", \"success\", \"gains\", \"positive\"]\n",
    "NEGATIVE_FINANCE = [\"negligent\", \"loss\", \"volatile\", \"wrong\", \"losses\", \"damages\", \"bad\", \"litigation\", \"failure\", \"down\", \"negative\"]\n",
    "\n",
    "POSITIVE_SE = [\"good\", \"lovely\", \"excellent\", \"fortunate\", \"pleasant\", \"delightful\", \"perfect\", \"loved\", \"love\", \"happy\"] \n",
    "NEGATIVE_SE = [\"bad\", \"horrible\", \"poor\",  \"unfortunate\", \"unpleasant\", \"disgusting\", \"evil\", \"hated\", \"hate\", \"unhappy\"]\n",
    "\n",
    "\n",
    "POSITIVE_TWEET = [\"love\", \"loved\", \"loves\", \"awesome\",  \"nice\", \"amazing\", \"best\", \"fantastic\", \"correct\", \"happy\"]\n",
    "NEGATIVE_TWEET = [\"hate\", \"hated\", \"hates\", \"terrible\",  \"nasty\", \"awful\", \"worst\", \"horrible\", \"wrong\", \"sad\"]\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING THE EFFECTIVENESS OF THE NEW LEXICON POLARITIES.\n",
    "\n",
    "- Calculates the ROC auc scores with the new polarities comparing to the earlier lexicon binary classification (1 = positive and 0 = negative).\n",
    "- Interpretation of the score. Higher the score, it means that new polarities (continous sentiment scores) confirms with the binary sentiment scores\n",
    "- Metrics are : \n",
    "\n",
    "    - AUC score : Treats the original lexicons as true labels and new lexicons continous scores as predicted probabilities.\n",
    "     \n",
    "     #### $auc\\_score(y_{true}, y_{probability})$\n",
    "      where $y_{true}$ is old lexicon value and $y_{probability}$ is new lexicon continous values.\n",
    "     #### $best\\ accuracy\\ = \\max(\\{\\frac{1+i-\\sum{j=1}^{i}+p}{n}\\}_i) \\forall i= \\{1..n\\}$\n",
    "    \n",
    "    - Ternary score : Also considers the neutral words through incorporating another third-lexicon library denoted by tau_lexicon. We have little understanding of this part though.\n",
    "    \n",
    "     ##### $cmn_{f1} = F1_{score}(y_{true},\\ \\{ mode(y_{true}) \\}_{i=1..y_{true}})$ \n",
    "     ##### $maj_{f1} = F1_{score}(y_{true},l)\\ ,\\ l=\\ 1\\ if y_{prob}>thresh_{+} otherwise 0$\n",
    "     \n",
    "     ##### $thresh\\ defined\\ for\\ positives\\ and\\ negatives$\n",
    "    \n",
    "    - *Kendall Correlation* : Ranking confidence scores. As cited in the paper \"Kendall t-rank correlation with continuous human-annotated polarity scores\". It is used to compare to order sets and higher confidence score mean that order is same of objectis in the two sets. Here original \n",
    "    \n",
    "    ### $\\tau=\\frac{n_c-n_d}{n(n-1)/2}$\n",
    "    where $n_c$ is concordant or same pairs and $n_d$ is discordant pairs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method_performance(polarities,INITIAL_LEXICON_LIB,domain,eval_words) : \n",
    "    \n",
    "    ## EVALUATING THE EFFECTIVENESS OF THE NEW LEXICON POLARITIES.\n",
    "    \n",
    "\n",
    "    acc, auc, avg_prec = binary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words)\n",
    "    if auc < 0.5:\n",
    "        polarities = {word:-1*polarities[word] for word in polarities}\n",
    "        acc, auc, avg_prec = binary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words)\n",
    "\n",
    "    print(\"============== DOMAIN : {} ==============\".format(domain))\n",
    "    print (\"Binary metrics:\")\n",
    "    print( \"==============\")\n",
    "    print (\"Accuracy with optimal threshold: {:.4f}\".format(acc))\n",
    "    print (\"ROC AUC Score: {:.4f}\".format(auc))\n",
    "    print (\"Average Precision Score: {:.4f}\".format(avg_prec))\n",
    "\n",
    "\n",
    "    tau, cmn_f1, maj_f1, conf_mat = ternary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words, tau_lexicon=THREE_WAY_LEXICON)\n",
    "    print (\"Ternary metrics:\")\n",
    "    print( \"==============\")\n",
    "    print (\"Majority macro F1 baseline {:.4f}\".format(maj_f1))\n",
    "    print (\"Macro F1 with cmn threshold: {:.4f}\".format(cmn_f1))\n",
    "    if tau:\n",
    "        print (\"Kendall Tau {:.4f}\".format(tau))\n",
    "    print (\"Confusion matrix: \")\n",
    "    print (conf_mat)\n",
    "    print( \"Neg :\", float(conf_mat[0,0]) / np.sum(conf_mat[0,:]))\n",
    "    print (\"Neut :\", float(conf_mat[1,1]) / np.sum(conf_mat[1,:]))\n",
    "    print (\"Pos :\", float(conf_mat[2,2]) / np.sum(conf_mat[2,:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique=LabelPropagation, Embedding=Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE INTERNAL RUN_METHOD IS RUNNING...\n",
      "AFTER EMBEDDING TRANSFORM  <representations.embedding.Embedding object at 0x7f9db0d6a8d0>\n",
      "============== DOMAIN : FINANCE ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.7760\n",
      "ROC AUC Score: 0.9452\n",
      "Average Precision Score: 0.7901\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.4642\n",
      "Macro F1 with cmn threshold: 0.1177\n",
      "Kendall Tau 0.3314\n",
      "Confusion matrix: \n",
      "[[   0    1 2239]\n",
      " [   0    0    0]\n",
      " [   0    0  345]]\n",
      "Neg : 0.0\n",
      "Neut : nan\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE INTERNAL RUN_METHOD IS RUNNING...\n",
      "AFTER EMBEDDING TRANSFORM  <representations.embedding.Embedding object at 0x7f9db0a23c50>\n",
      "============== DOMAIN :  STANDARD ENGLISH  ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.1756\n",
      "ROC AUC Score: 0.8184\n",
      "Average Precision Score: 0.7847\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.2497\n",
      "Macro F1 with cmn threshold: 0.1024\n",
      "Kendall Tau 0.3770\n",
      "Confusion matrix: \n",
      "[[   0    1 1874]\n",
      " [   0    0 5106]\n",
      " [   0    0 1547]]\n",
      "Neg : 0.0\n",
      "Neut : 0.0\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE INTERNAL RUN_METHOD IS RUNNING...\n",
      "AFTER EMBEDDING TRANSFORM  <representations.embedding.Embedding object at 0x7f9db1075b70>\n",
      "============== DOMAIN :  TWITTER ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 0.8612\n",
      "ROC AUC Score: 0.7370\n",
      "Average Precision Score: 0.8119\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.3844\n",
      "Macro F1 with cmn threshold: 0.3844\n",
      "Kendall Tau 0.4029\n",
      "Confusion matrix: \n",
      "[[  0   1 313]\n",
      " [  0   0   0]\n",
      " [  0   0 522]]\n",
      "Neg : 0.0\n",
      "Neut : nan\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_polarities,finance_eval=calculate_new_lexicon_polarities(FINANCE_LEXICONS,POSITIVE_FINANCE,NEGATIVE_FINANCE,'label_propagate_prob','YELP')\n",
    "evaluate_method_performance(finance_polarities,FINANCE_LEXICONS,'FINANCE',finance_eval)\n",
    "gc.collect()\n",
    "\n",
    "standard_english_polarities,se_eval=calculate_new_lexicon_polarities(INQUIRER,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob','YELP')\n",
    "evaluate_method_performance(standard_english_polarities,INQUIRER,' STANDARD ENGLISH ',se_eval)\n",
    "gc.collect()\n",
    "\n",
    "twitter_polarities,se_eval=calculate_new_lexicon_polarities(TWITTER_LEXICONS,POSITIVE_TWEET,NEGATIVE_TWEET,'label_propagate_prob','YELP')\n",
    "evaluate_method_performance(twitter_polarities,TWITTER_LEXICONS,' TWITTER',se_eval)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique=LabelPropagation, Embedding=Glove\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== DOMAIN : FINANCE ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.7760\n",
      "ROC AUC Score: 0.9479\n",
      "Average Precision Score: 0.7981\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.4642\n",
      "Macro F1 with cmn threshold: 0.1177\n",
      "Kendall Tau 0.3313\n",
      "Confusion matrix: \n",
      "[[   0    1 2239]\n",
      " [   0    0    0]\n",
      " [   0    0  345]]\n",
      "Neg : 0.0\n",
      "Neut : nan\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== DOMAIN :  STANDARD ENGLISH  ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.1777\n",
      "ROC AUC Score: 0.8146\n",
      "Average Precision Score: 0.7821\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.2497\n",
      "Macro F1 with cmn threshold: 0.1024\n",
      "Kendall Tau 0.3752\n",
      "Confusion matrix: \n",
      "[[   0    1 1874]\n",
      " [   0    0 5106]\n",
      " [   0    0 1547]]\n",
      "Neg : 0.0\n",
      "Neut : 0.0\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== DOMAIN :  TWITTER ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 0.8624\n",
      "ROC AUC Score: 0.7406\n",
      "Average Precision Score: 0.8141\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.3844\n",
      "Macro F1 with cmn threshold: 0.3844\n",
      "Kendall Tau 0.4053\n",
      "Confusion matrix: \n",
      "[[  0   1 313]\n",
      " [  0   0   0]\n",
      " [  0   0 522]]\n",
      "Neg : 0.0\n",
      "Neut : nan\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_polarities,finance_eval=calculate_new_lexicon_polarities(FINANCE_LEXICONS,POSITIVE_FINANCE,NEGATIVE_FINANCE,'label_propagate_prob','GLOVE')\n",
    "evaluate_method_performance(finance_polarities,FINANCE_LEXICONS,'FINANCE',finance_eval)\n",
    "gc.collect()\n",
    "\n",
    "standard_english_polarities,se_eval=calculate_new_lexicon_polarities(INQUIRER,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob','GLOVE')\n",
    "evaluate_method_performance(standard_english_polarities,INQUIRER,' STANDARD ENGLISH ',se_eval)\n",
    "gc.collect()\n",
    "\n",
    "twitter_polarities,se_eval=calculate_new_lexicon_polarities(TWITTER_LEXICONS,POSITIVE_TWEET,NEGATIVE_TWEET,'label_propagate_prob','GLOVE')\n",
    "evaluate_method_performance(twitter_polarities,TWITTER_LEXICONS,' TWITTER',se_eval)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique=Graphpropagation, Embedding=Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE INTERNAL RUN_METHOD IS RUNNING...\n",
      "AFTER EMBEDDING TRANSFORM  <representations.embedding.Embedding object at 0x7f94798e8b70>\n",
      "Getting positive scores..\n",
      "Getting negative scores..\n",
      "Computing final scores...\n",
      "2605/2605 [==============================] - 0s     \n",
      "============== DOMAIN : FINANCE ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.7563\n",
      "ROC AUC Score: 0.7687\n",
      "Average Precision Score: 0.5258\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.4642\n",
      "Macro F1 with cmn threshold: 0.1177\n",
      "Kendall Tau 0.3284\n",
      "Confusion matrix: \n",
      "[[   0    1 2239]\n",
      " [   0    0    0]\n",
      " [   0    0  345]]\n",
      "Neg : 0.0\n",
      "Neut : nan\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE INTERNAL RUN_METHOD IS RUNNING...\n",
      "AFTER EMBEDDING TRANSFORM  <representations.embedding.Embedding object at 0x7f94798e8b70>\n",
      "Getting positive scores..\n",
      "Getting negative scores..\n",
      "Computing final scores...\n",
      "8548/8548 [==============================] - 0s     \n",
      "============== DOMAIN :  STANDARD ENGLISH  ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.1300\n",
      "ROC AUC Score: 0.5905\n",
      "Average Precision Score: 0.5290\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.2497\n",
      "Macro F1 with cmn threshold: 0.1024\n",
      "Kendall Tau 0.2193\n",
      "Confusion matrix: \n",
      "[[   0    1 1874]\n",
      " [   0    0 5106]\n",
      " [   0    0 1547]]\n",
      "Neg : 0.0\n",
      "Neut : 0.0\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE INTERNAL RUN_METHOD IS RUNNING...\n",
      "AFTER EMBEDDING TRANSFORM  <representations.embedding.Embedding object at 0x7f94799261d0>\n",
      "Getting positive scores..\n",
      "Getting negative scores..\n",
      "Computing final scores...\n",
      "856/856 [==============================] - 1s     \n",
      "============== DOMAIN :  TWITTER ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 0.8648\n",
      "ROC AUC Score: 0.7130\n",
      "Average Precision Score: 0.7803\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.3844\n",
      "Macro F1 with cmn threshold: 0.3844\n",
      "Kendall Tau 0.3993\n",
      "Confusion matrix: \n",
      "[[  0   1 313]\n",
      " [  0   0   0]\n",
      " [  0   0 522]]\n",
      "Neg : 0.0\n",
      "Neut : nan\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_polarities,finance_eval=calculate_new_lexicon_polarities(FINANCE_LEXICONS,POSITIVE_FINANCE,NEGATIVE_FINANCE,'graph_propagate','YELP')\n",
    "evaluate_method_performance(finance_polarities,FINANCE_LEXICONS,'FINANCE',finance_eval)\n",
    "gc.collect()\n",
    "\n",
    "standard_english_polarities,se_eval=calculate_new_lexicon_polarities(INQUIRER,POSITIVE_SE,NEGATIVE_SE,'graph_propagate','YELP')\n",
    "evaluate_method_performance(standard_english_polarities,INQUIRER,' STANDARD ENGLISH ',se_eval)\n",
    "gc.collect()\n",
    "\n",
    "twitter_polarities,se_eval=calculate_new_lexicon_polarities(TWITTER_LEXICONS,POSITIVE_TWEET,NEGATIVE_TWEET,'graph_propagate','YELP')\n",
    "evaluate_method_performance(twitter_polarities,TWITTER_LEXICONS,' TWITTER',se_eval)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technique=GraphPropagation, Embedding=Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE INTERNAL RUN_METHOD IS RUNNING...\n",
      "AFTER EMBEDDING TRANSFORM  <representations.embedding.Embedding object at 0x7f94799777f0>\n",
      "Getting positive scores..\n",
      "Getting negative scores..\n",
      "Computing final scores...\n",
      "2605/2605 [==============================] - 0s     \n",
      "============== DOMAIN : FINANCE ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.7563\n",
      "ROC AUC Score: 0.7687\n",
      "Average Precision Score: 0.5258\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.4642\n",
      "Macro F1 with cmn threshold: 0.1177\n",
      "Kendall Tau 0.3284\n",
      "Confusion matrix: \n",
      "[[   0    1 2239]\n",
      " [   0    0    0]\n",
      " [   0    0  345]]\n",
      "Neg : 0.0\n",
      "Neut : nan\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE INTERNAL RUN_METHOD IS RUNNING...\n",
      "AFTER EMBEDDING TRANSFORM  <representations.embedding.Embedding object at 0x7f9479982e80>\n",
      "Getting positive scores..\n",
      "Getting negative scores..\n",
      "Computing final scores...\n",
      "8548/8548 [==============================] - 0s     \n",
      "============== DOMAIN :  STANDARD ENGLISH  ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.1300\n",
      "ROC AUC Score: 0.5905\n",
      "Average Precision Score: 0.5290\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.2497\n",
      "Macro F1 with cmn threshold: 0.1024\n",
      "Kendall Tau 0.2193\n",
      "Confusion matrix: \n",
      "[[   0    1 1874]\n",
      " [   0    0 5106]\n",
      " [   0    0 1547]]\n",
      "Neg : 0.0\n",
      "Neut : 0.0\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THE INTERNAL RUN_METHOD IS RUNNING...\n",
      "AFTER EMBEDDING TRANSFORM  <representations.embedding.Embedding object at 0x7f9500328dd8>\n",
      "Getting positive scores..\n",
      "Getting negative scores..\n",
      "Computing final scores...\n",
      "856/856 [==============================] - 1s     \n",
      "============== DOMAIN :  TWITTER ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 0.8648\n",
      "ROC AUC Score: 0.7130\n",
      "Average Precision Score: 0.7803\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.3844\n",
      "Macro F1 with cmn threshold: 0.3844\n",
      "Kendall Tau 0.3993\n",
      "Confusion matrix: \n",
      "[[  0   1 313]\n",
      " [  0   0   0]\n",
      " [  0   0 522]]\n",
      "Neg : 0.0\n",
      "Neut : nan\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_polarities,finance_eval=calculate_new_lexicon_polarities(FINANCE_LEXICONS,POSITIVE_FINANCE,NEGATIVE_FINANCE,'graph_propagate','GLOVE')\n",
    "evaluate_method_performance(finance_polarities,FINANCE_LEXICONS,'FINANCE',finance_eval)\n",
    "gc.collect()\n",
    "\n",
    "standard_english_polarities,se_eval=calculate_new_lexicon_polarities(INQUIRER,POSITIVE_SE,NEGATIVE_SE,'graph_propagate','GLOVE')\n",
    "evaluate_method_performance(standard_english_polarities,INQUIRER,' STANDARD ENGLISH ',se_eval)\n",
    "gc.collect()\n",
    "\n",
    "twitter_polarities,se_eval=calculate_new_lexicon_polarities(TWITTER_LEXICONS,POSITIVE_TWEET,NEGATIVE_TWEET,'graph_propagate','GLOVE')\n",
    "evaluate_method_performance(twitter_polarities,TWITTER_LEXICONS,' TWITTER',se_eval)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now senti140, wordnet and sst have huge vocabs.\n",
    "\n",
    "We have ran the lexicon comparison for senti140_polarities and sentiwn_polarities below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent140_polarities,s140eval = calculate_new_lexicon_polarities(senti140_lexicons,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob')\n",
    "#gc.collect()\n",
    "\n",
    "#sentiwn_polarities,swn_eval = calculate_new_lexicon_polarities(sentiwn_lexicons,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob')\n",
    "#gc.collect()\n",
    "\n",
    "#sst_lexicons,sst_eval = calculate_new_lexicon_polarities(sst_lexicons,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob')\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Evaluation for unsupervised Lexicon sentiment tagging\n",
    "\n",
    "This is to compute the effectiveness of binary sentiment scores provided a lexicon library.\n",
    "This can be used to see which lexicon libraries help achieving the closest sentiment scores.\n",
    "Thus a supervised algorithm and evaluation is the \n",
    "\n",
    "compare against the sentence tagging (already provided in the dataset )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BORROWED FROM THE AR_SARKAR METRIC\n",
    "def analyze_sentiment_sentiwordnet_lexicon(review,verbose=False):\n",
    "    \n",
    "    \n",
    "    #review = normalize_accented_characters(review)\n",
    "    #review = review.decode('utf-8')\n",
    "    review = html_parser.unescape(review)\n",
    "    review = strip_html(review)\n",
    "    \n",
    "    text_tokens = nltk.word_tokenize(review)\n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and swn.senti_synsets(word, 'n'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'VB' in tag and swn.senti_synsets(word, 'v'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'JJ' in tag and swn.senti_synsets(word, 'a'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'RB' in tag and swn.senti_synsets(word, 'r'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        \n",
    "        if ss_set:\n",
    "            \n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    \n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        \n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score,\n",
    "                                         norm_pos_score, norm_neg_score,\n",
    "                                         norm_final_score]],\n",
    "                                         columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment', 'Objectivity',\n",
    "                                                                       'Positive', 'Negative', 'Overall']], \n",
    "                                                              labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print (sentiment_frame)   \n",
    "    return final_sentiment\n",
    "            \n",
    "                                                               \n",
    "def evaluate_lexicons(TRUE_LABELS,PREDICTED_LABELS,POS_CLASS,NEG_CLASS) : \n",
    "\n",
    "    print ('Performance metrics:')\n",
    "    display_evaluation_metrics(true_labels=TRUE_LABELS,\n",
    "                               predicted_labels=PREDICTED_LABELS,\n",
    "                               positive_class=str(POS_CLASS))  \n",
    "    print ('\\nConfusion Matrix:'             )              \n",
    "    display_confusion_matrix(true_labels=TRUE_LABELS,\n",
    "                             predicted_labels=PREDICTED_LABELS,\n",
    "                             classes=[str(POS_CLASS),str(NEG_CLASS)])\n",
    "    print ('\\nClassification report:' )                        \n",
    "    display_classification_report(true_labels=TRUE_LABELS,\n",
    "                                  predicted_labels=PREDICTED_LABELS,\n",
    "                                  classes=[str(POS_CLASS),str(NEG_CLASS)])\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASELINE LEXICON : Unsupervised Classification nn the Movie Reviews using senti-wordnet lexicons\n",
    "\n",
    "-  Accuracy: 0.6\n",
    "-  Precision: 0.56\n",
    "-  Recall: 0.93\n",
    "-  F1 Score: 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  50000\n",
      "Train_X :  1000\n",
      "Test_X  :  1000\n",
      "Performance metrics:\n",
      "Accuracy: 0.6\n",
      "Precision: 0.56\n",
      "Recall: 0.93\n",
      "F1 Score: 0.7\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        470       34\n",
      "        negative        365      131\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.56      0.93      0.70       504\n",
      "    negative       0.79      0.26      0.40       496\n",
      "\n",
      "   micro avg       0.60      0.60      0.60      1000\n",
      "   macro avg       0.68      0.60      0.55      1000\n",
      "weighted avg       0.68      0.60      0.55      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_x,test_x,test_y=prepare_movie_dataset(0,1000,1000,2000)\n",
    "sentiwordnet_predictions = [analyze_sentiment_sentiwordnet_lexicon(review) for review in test_x]\n",
    "evaluate_lexicons(test_y.tolist(),sentiwordnet_predictions,'positive','negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we wanted to test the effect of new scores on the supervised lexicon-based sentiment scores of a corpus.  Again because our same lexicons have are very small < 10k, most of the words from corpus are not found in the lexicons and does not returns the proper sentence sentiment. \n",
    "\n",
    " This part can be redone when we have large lexicon library. So, the impact can be seen as in the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS : \n",
    "## review = single sentence \n",
    "## lexicon_dict = dict of the lexicon with key as word and value as the polarity\n",
    "\n",
    "def analyze_sentiment_domain(review,lexicon_dict,verbose=False):\n",
    "    \n",
    "    \n",
    "    #review = normalize_accented_characters(review)\n",
    "    #review = review.decode('utf-8')\n",
    "    review = html_parser.unescape(review)\n",
    "    review = strip_html(review)\n",
    "    \n",
    "    text_tokens = nltk.word_tokenize(review)\n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "\n",
    "    ## postitve polarity counts as positive and negative polarities counts as negative\n",
    "    \n",
    "    \n",
    "    for token in text_tokens : \n",
    "        \n",
    "        if token in lexicon_dict : \n",
    "            \n",
    "            if lexicon_dict[token]>0 : \n",
    "                pos_score+=1\n",
    "            elif lexicon_dict[token]<0:\n",
    "                neg_score+=1\n",
    "\n",
    "        token_count+=1\n",
    "            \n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment,\n",
    "                                         norm_pos_score, norm_neg_score,\n",
    "                                         norm_final_score]],\n",
    "                                         columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment',\n",
    "                                                                       'Positive', 'Negative', 'Overall']], \n",
    "                                                              labels=[[0,0,0,0],[0,1,2,3]]))\n",
    "        print (sentiment_frame)   \n",
    "    return final_sentiment\n",
    "            \n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we wanted to see the supervised classification scores with initial lexicons and after label-propagation scored lexicons.However due to our system limitations, because the sentiwordnet 150 has ~68,000 tokens, the kernal crashes in attempt to run algorith on this lexicon \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- ORIGINAL SENTI140 LEXICONS----\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'prepare_movie_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-c3d2923913f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-- ORIGINAL SENTI140 LEXICONS----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_movie_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0msentiwordnet_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msenti140_lexicons\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mevaluate_lexicons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiwordnet_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prepare_movie_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "## THE PROGRAM CRASHES.: \n",
    "## IF PROGRAM CAN RUN, THEN WE SEE changes in supervised sentiment scoring on the movie_reviews dataset\n",
    "\n",
    "\n",
    "print(\"-- ORIGINAL SENTI140 LEXICONS----\")\n",
    "train_x,test_x,test_y=prepare_movie_dataset(0,1000,1000,2000)\n",
    "sentiwordnet_predictions = [analyze_sentiment_domain(review,senti140_lexicons) for review in test_x]\n",
    "evaluate_lexicons(test_y.tolist(),sentiwordnet_predictions,'positive','negative')\n",
    "\n",
    "new_senti140_lexicons,se_eval=calculate_new_lexicon_polarities(senti140_lexicons,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob','YELP')\n",
    "\n",
    "print(\"-- RE-CALCULATED SENTI140 LEXICONS----\")\n",
    "train_x,test_x,test_y=prepare_movie_dataset(0,1000,1000,2000)\n",
    "sentiwordnet_predictions = [analyze_sentiment_domain(review,new_senti140_lexicons) for review in test_x]\n",
    "evaluate_lexicons(test_y.tolist(),sentiwordnet_predictions,'positive','negative')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The classification scored difference using two different lexicon dictionaries may prove the impact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE WHOLE USE OF THE METHODS IS THAT TO GET INSIGHT INTO THE CONTEXT-SENSITIVE INFORMATION.\n",
    "\n",
    "\n",
    "### HOW? \n",
    "\n",
    "### 1. ARRANGE THE PRE-TAGGED LEXICONS (ATLEAST POSITIVE/NEGATIVE)\n",
    "\n",
    "### 2. WORD EMBEDDINGS TRAINED ON THE CONTEXT-MATERIAL. \n",
    "\n",
    "### 3. LABEL-PROPAGATE ALGORITHM TO MAKE LEXICONS SCORE LEXICONS TO THE CONTINOUS SENTIMENT SCORES.\n",
    "\n",
    "### 4. USE SUM (P+V)/T OR NEURAL NETWORK TO OBTAIN THE SCORE FOR THE WHOLE SENTENCE SENTIMENT \n",
    "\n",
    "##### Here, we earlier thought that we would be able to implement this phase if we had more time. The paper \"Context-Sensitive Lexicon Features for Neural Sentiment Analysis\" we can test baseline with normal lexicons and improvement as label-propagated lexicons with LSTM for scores, and evaluate them back on the binary classification scores "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
