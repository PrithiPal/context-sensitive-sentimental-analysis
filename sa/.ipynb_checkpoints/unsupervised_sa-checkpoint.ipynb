{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from normalization import normalize_accented_characters, html_parser, strip_html\n",
    "from utils import display_evaluation_metrics, display_confusion_matrix, display_classification_report\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sentiwordnet')\n",
    "import dynet as dy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_movie_dataset(train_start,train_end,test_start,test_end) : \n",
    "\n",
    "    dataset = pd.read_csv(r'datasets/movie_reviews.csv')\n",
    "    print('dataset size : ',dataset.shape[0])\n",
    "\n",
    "    train_data = dataset[train_start:train_end]\n",
    "    test_data = dataset[test_start:test_end]\n",
    "    \n",
    "    print('Train_X : ',train_data.shape[0])\n",
    "    print('Test_X  : ',test_data.shape[0])\n",
    "\n",
    "    test_reviews = np.array(test_data['review'])\n",
    "    test_sentiments = np.array(test_data['sentiment'])\n",
    "\n",
    "    return train_data,test_reviews,test_sentiments\n",
    "\n",
    "def prepare_labeled_data(train_start,train_end,test_start,test_end) : \n",
    "    \n",
    "    labeled_data=open(\"datasets/labeledTrainData.tsv\",\"r\")\n",
    "    data=labeled_data.readlines()\n",
    "    data=[d.split(\"\\t\") for d in data]\n",
    "    sa_data=pd.DataFrame(data,columns=['ind','sentiment','review'])\n",
    "    sa_data=sa_data[['sentiment','review']]\n",
    "    \n",
    "    print('dataset size : ',sa_data.shape[0])\n",
    "\n",
    "    train_data = sa_data[train_start:train_end]\n",
    "    test_data = sa_data[test_start:test_end]\n",
    "    \n",
    "    print('Train_X : ',train_data.shape[0])\n",
    "    print('Test_X  : ',test_data.shape[0])\n",
    "\n",
    "    test_reviews = np.array(test_data['review'])\n",
    "    test_sentiments = np.array(test_data['sentiment'])\n",
    "\n",
    "    return train_data,test_reviews,test_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  25001\n",
      "Train_X :  999\n",
      "Test_X  :  1000\n"
     ]
    }
   ],
   "source": [
    "train_x,test_x,test_y=prepare_labeled_data(1,1000,1000,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for unsupervised Lexicon sentiment tagging\n",
    "\n",
    "#### compare against the sentence tagging (already provided in the dataset )\n",
    "\n",
    "[add markdown #11 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BORROWED FROM THE AR_SARKAR METRIC\n",
    "def analyze_sentiment_sentiwordnet_lexicon(review,verbose=False):\n",
    "    \n",
    "    \n",
    "    #review = normalize_accented_characters(review)\n",
    "    #review = review.decode('utf-8')\n",
    "    review = html_parser.unescape(review)\n",
    "    review = strip_html(review)\n",
    "    \n",
    "    text_tokens = nltk.word_tokenize(review)\n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and swn.senti_synsets(word, 'n'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'VB' in tag and swn.senti_synsets(word, 'v'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'JJ' in tag and swn.senti_synsets(word, 'a'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'RB' in tag and swn.senti_synsets(word, 'r'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        \n",
    "        if ss_set:\n",
    "            \n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    \n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        \n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score,\n",
    "                                         norm_pos_score, norm_neg_score,\n",
    "                                         norm_final_score]],\n",
    "                                         columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment', 'Objectivity',\n",
    "                                                                       'Positive', 'Negative', 'Overall']], \n",
    "                                                              labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print (sentiment_frame)   \n",
    "    return final_sentiment\n",
    "            \n",
    "                                                               \n",
    "def evaluate_lexicons(TRUE_LABELS,PREDICTED_LABELS,POS_CLASS,NEG_CLASS) : \n",
    "\n",
    "    print ('Performance metrics:')\n",
    "    display_evaluation_metrics(true_labels=TRUE_LABELS,\n",
    "                               predicted_labels=PREDICTED_LABELS,\n",
    "                               positive_class=str(POS_CLASS))  \n",
    "    print ('\\nConfusion Matrix:'             )              \n",
    "    display_confusion_matrix(true_labels=TRUE_LABELS,\n",
    "                             predicted_labels=PREDICTED_LABELS,\n",
    "                             classes=[str(POS_CLASS),str(NEG_CLASS)])\n",
    "    print ('\\nClassification report:' )                        \n",
    "    display_classification_report(true_labels=TRUE_LABELS,\n",
    "                                  predicted_labels=PREDICTED_LABELS,\n",
    "                                  classes=[str(POS_CLASS),str(NEG_CLASS)])\n",
    "    return\n",
    "\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basline lexicon evaluation\n",
    "\n",
    "#### movie dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  50000\n",
      "Train_X :  1000\n",
      "Test_X  :  1000\n",
      "Performance metrics:\n",
      "Accuracy: 0.6\n",
      "Precision: 0.56\n",
      "Recall: 0.93\n",
      "F1 Score: 0.7\n",
      "\n",
      "Confusion Matrix:\n",
      "                 Predicted:         \n",
      "                   positive negative\n",
      "Actual: positive        470       34\n",
      "        negative        365      131\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.56      0.93      0.70       504\n",
      "    negative       0.79      0.26      0.40       496\n",
      "\n",
      "   micro avg       0.60      0.60      0.60      1000\n",
      "   macro avg       0.68      0.60      0.55      1000\n",
      "weighted avg       0.68      0.60      0.55      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_x,test_x,test_y=prepare_movie_dataset(0,1000,1000,2000)\n",
    "sentiwordnet_predictions = [analyze_sentiment_sentiwordnet_lexicon(review) for review in test_x]\n",
    "evaluate_lexicons(test_y.tolist(),sentiwordnet_predictions,'positive','negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labeled dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  25001\n",
      "Train_X :  1000\n",
      "Test_X  :  1000\n",
      "Performance metrics:\n",
      "Accuracy: 0.59\n",
      "Precision: 0.56\n",
      "Recall: 0.92\n",
      "F1 Score: 0.7\n",
      "\n",
      "Confusion Matrix:\n",
      "          Predicted:     \n",
      "                   1    0\n",
      "Actual: 1        476   39\n",
      "        0        371  114\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.92      0.70       515\n",
      "           0       0.75      0.24      0.36       485\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      1000\n",
      "   macro avg       0.65      0.58      0.53      1000\n",
      "weighted avg       0.65      0.59      0.53      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_x,test_x,test_y=prepare_labeled_data(0,1000,1000,2000)\n",
    "sentiwordnet_predictions = [analyze_sentiment_sentiwordnet_lexicon(review) for review in test_x]\n",
    "binary_predicted=['1' if p=='positive' else '0' for p in sentiwordnet_predictions ]\n",
    "evaluate_lexicons(test_y.tolist(),binary_predicted,'1','0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple network for learning (do afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss before step is: 0.4169560670852661\n",
      "the loss after step is: 0.3646645247936249\n"
     ]
    }
   ],
   "source": [
    "## SIMPLE NETWORK WITH THE sigma(V*tanh(WX+B)) ## for the XOR problem\n",
    "# create a parameter collection and add the parameters.\n",
    "m = dy.ParameterCollection()\n",
    "W = m.add_parameters((8,2))\n",
    "V = m.add_parameters((1,8))\n",
    "b = m.add_parameters((8))\n",
    "\n",
    "dy.renew_cg() # new computation graph. not strictly needed here, but good practice.\n",
    "b.value() ## bias values\n",
    "x=dy.vecInput(2) ## 2 sized inputs \n",
    "output=dy.logistic(V*(dy.tanh(W*x)+b)) ## output node\n",
    "\n",
    "y = dy.scalarInput(0) ## objective function\n",
    "loss = dy.binary_log_loss(output,y) ## loss function\n",
    "## trainer with the initialized parameters m \n",
    "trainer=dy.SimpleSGDTrainer(m)\n",
    "x.set([1,0])\n",
    "y.set(1)\n",
    "loss_value = loss.value() # this performs a forward through the network.\n",
    "print(\"the loss before step is:\",loss_value)\n",
    "\n",
    "loss.backward()  # compute the gradients\n",
    "trainer.update()\n",
    "\n",
    "loss_value = loss.value(recalculate=True) \n",
    "print(\"the loss after step is:\",loss_value)\n",
    "pc = dy.ParameterCollection()\n",
    "NUM_LAYERS=2\n",
    "INPUT_DIM=50\n",
    "HIDDEN_DIM=10\n",
    "builder = dy.LSTMBuilder(NUM_LAYERS, INPUT_DIM, HIDDEN_DIM, pc)\n",
    "s0 = builder.initial_state()\n",
    "x1 = dy.vecInput(INPUT_DIM)\n",
    "s1=s0.add_input(x1)\n",
    "y1 = s1.output()\n",
    "s2=s1.add_input(x1) # we can add another input\n",
    "y2=s2.output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING 2.1 SECTION OF GOOGLE PAPER FOR LEXICON EXPANSION [this is for the lexicon expansion ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORDS=list(wn.words())\n",
    "SCORES=defaultdict()\n",
    "df=pd.DataFrame(ALL_WORDS,columns=['word'])\n",
    "\n",
    "swn.all_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the lexicons from stanford paper \"Incuding Domain-Specific Sentiment Lexicons from Unalabeled Copora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socialsent_util\n",
    "def load_lexicon(name, remove_neutral=True):\n",
    "    lexicon = socialsent_util.load_json(\"./lexicons_socialsent/\"+ name + '.json')\n",
    "    return {w: p for w, p in lexicon.items() if p != 0} if remove_neutral else lexicon\n",
    "\n",
    "def compare_lexicons(print_disagreements=False):\n",
    "    lexicons = {\n",
    "        \"inquirer\": load_lexicon(\"inquirer\", False),\n",
    "        \"mpqa\": load_lexicon(\"mpqa\", False),\n",
    "        \"bingliu\": load_lexicon(\"bingliu\", False),\n",
    "    }\n",
    "\n",
    "    for l in lexicons:\n",
    "        print( l, len(lexicons[l]), len([w for w in lexicons[l] if lexicons[l][w] != 0]))\n",
    "\n",
    "    for l1, l2 in itertools.combinations(lexicons.keys(), 2):\n",
    "        ps1, ps2 = lexicons[l1], lexicons[l2]\n",
    "        common_words = set(ps1.keys()) & set(ps2.keys())\n",
    "        print( l1, l2, \"agreement: {:.2f}\".format(\n",
    "            100.0 * sum(1 if ps1[w] == ps2[w] else 0 for w in common_words) / len(common_words)))\n",
    "        common_words = set([word for word in ps1.keys() if ps1[word] != 0]) & \\\n",
    "                       set([word for word in ps2.keys() if ps2[word] != 0])  \n",
    "        print (l1, l2, \"agreement ignoring neutral: {:.2f}\".format(\n",
    "            100.0 * sum(1 if ps1[w] * ps2[w] == 1 else 0 for w in common_words) / len(common_words)))\n",
    "        \n",
    "        if print_disagreements and l1 == 'opinion' and l2 == 'inquirer':\n",
    "            for w in common_words:\n",
    "                if lexicons[l1][w] != lexicons[l2][w]:\n",
    "                    print (w, lexicons[l1][w], lexicons[l2][w])\n",
    "      \n",
    "    \n",
    "## ALL THESE LEXICONS ARE 2-CLASS SENTIMENTS. 1 = POSITIVE; -1 = NEGATIVE\n",
    "finance_lexicons=load_lexicon('finance')\n",
    "bingliu_lexicons=load_lexicon('bingliu')\n",
    "inquirer_lexicons=load_lexicon('inquirer')\n",
    "mpqa_lexicons=load_lexicon('mpqa')\n",
    "twitter_lexicons=load_lexicon('twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compares the different lexicon repositories through the mutual information between them (common words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'horrible': -1,\n",
       " '#eurgh': -1,\n",
       " ':/': -1,\n",
       " ':(': -1,\n",
       " ':)': 1,\n",
       " 'cough': -1,\n",
       " 'whoever': -1,\n",
       " 'ciao': 1,\n",
       " 'relieve': 1,\n",
       " 'ouch': -1,\n",
       " 'foul': -1,\n",
       " 'catch': 1,\n",
       " 'ugh': -1,\n",
       " 'sleep': 1,\n",
       " '#pissedoff': -1,\n",
       " 'hanging': -1,\n",
       " 'go': -1,\n",
       " 'follow': -1,\n",
       " 'revolting': -1,\n",
       " 'chillin': 1,\n",
       " 'children': 1,\n",
       " 'forget': -1,\n",
       " 'spoiled': -1,\n",
       " 'dreadful': -1,\n",
       " 'yummy': 1,\n",
       " 'vile': -1,\n",
       " 'ignorant': -1,\n",
       " '#happy': 1,\n",
       " 'tweet': 1,\n",
       " 'show': 1,\n",
       " 'sweetest': 1,\n",
       " 'young': 1,\n",
       " 'send': 1,\n",
       " 'yummm': 1,\n",
       " 'finally': 1,\n",
       " 'asking': 1,\n",
       " 'wants': 1,\n",
       " 'hola': 1,\n",
       " 'text': 1,\n",
       " 'wooooo': 1,\n",
       " 'smile': 1,\n",
       " 'sorry': -1,\n",
       " 'enjoying': 1,\n",
       " 'ergh': -1,\n",
       " '#forgiveme': -1,\n",
       " 'fly': 1,\n",
       " ':o': 1,\n",
       " '#dying': -1,\n",
       " 'garden': 1,\n",
       " 'song': 1,\n",
       " ':d': -1,\n",
       " '#hurt': -1,\n",
       " 'horror': -1,\n",
       " 'rise': 1,\n",
       " '#rich': 1,\n",
       " 'unfamiliar': -1,\n",
       " 'anyway': 1,\n",
       " ':|': -1,\n",
       " '#justwhatineed': 1,\n",
       " 'fan': 1,\n",
       " 'updates': 1,\n",
       " 'fam': 1,\n",
       " 'telling': -1,\n",
       " '#tramps': -1,\n",
       " ':p': 1,\n",
       " 'w00t': 1,\n",
       " 'cool': 1,\n",
       " 'school': 1,\n",
       " '#nolife': -1,\n",
       " '<33': 1,\n",
       " 'mothers': 1,\n",
       " '#pitiful': -1,\n",
       " 'cause': -1,\n",
       " 'list': -1,\n",
       " 'try': 1,\n",
       " ':]': 1,\n",
       " '#dontyouhate': -1,\n",
       " 'excellence': 1,\n",
       " '#fedup': -1,\n",
       " 'team': 1,\n",
       " '#rude': -1,\n",
       " 'guy': 1,\n",
       " 'heading': 1,\n",
       " '#getagrip': -1,\n",
       " 'enjoy': 1,\n",
       " '#pointless': -1,\n",
       " 'work': -1,\n",
       " 'says': 1,\n",
       " 'villa': 1,\n",
       " 'tired': -1,\n",
       " 'awake': 1,\n",
       " 'tea': 1,\n",
       " '#poorly': -1,\n",
       " 'bacon': 1,\n",
       " 'smfh': -1,\n",
       " '#phenomenal': 1,\n",
       " 'elegant': 1,\n",
       " 'fabulous': 1,\n",
       " 'video': 1,\n",
       " '#superfriends': 1,\n",
       " '#die': -1,\n",
       " 'yayyy': 1,\n",
       " '#spectacular': 1,\n",
       " 'hide': 1,\n",
       " '#losers': -1,\n",
       " '#gross': -1,\n",
       " 'hate': -1,\n",
       " 'boogie': 1,\n",
       " '#relaxing': 1,\n",
       " 'amazzing': 1,\n",
       " 'pleased': 1,\n",
       " 'waiting': 1,\n",
       " 'version': -1,\n",
       " '#gorgeous': 1,\n",
       " '#annoying': -1,\n",
       " 'upset': -1,\n",
       " 'new': 1,\n",
       " '#talent': 1,\n",
       " 'public': -1,\n",
       " 'told': 1,\n",
       " 'stinks': -1,\n",
       " 'full': 1,\n",
       " 'unsure': -1,\n",
       " '#feelinggood': 1,\n",
       " '#encouraging': 1,\n",
       " 'apologise': -1,\n",
       " 'learn': 1,\n",
       " 'shout-out': 1,\n",
       " 'behaviour': -1,\n",
       " 'met': 1,\n",
       " 'busy': 1,\n",
       " 'let': 1,\n",
       " 'making': 1,\n",
       " '#liars': -1,\n",
       " '#upset': -1,\n",
       " 'bullying': -1,\n",
       " '#nogood': -1,\n",
       " 'wait': 1,\n",
       " 'boy': 1,\n",
       " 'great': 1,\n",
       " 'kudos': 1,\n",
       " 'yessss': 1,\n",
       " 'sunnny': 1,\n",
       " '#sosad': -1,\n",
       " 'inspired': 1,\n",
       " '=(': -1,\n",
       " 'pics': 1,\n",
       " 'eh': -1,\n",
       " 'gawjuss': 1,\n",
       " 'horrified': -1,\n",
       " 'social': -1,\n",
       " '#goodsex': 1,\n",
       " 'priceless': 1,\n",
       " '#lost': -1,\n",
       " 'makes': -1,\n",
       " 'headache': -1,\n",
       " 'fantastic': 1,\n",
       " 'shootings': -1,\n",
       " 'love': 1,\n",
       " 'inexactly': -1,\n",
       " 'family': 1,\n",
       " '#feelsick': -1,\n",
       " 'feelings': 1,\n",
       " 'expect': 1,\n",
       " 'marvelous': 1,\n",
       " 'vip': 1,\n",
       " 'wit': 1,\n",
       " '#faith': 1,\n",
       " '#broke': -1,\n",
       " 'imported': -1,\n",
       " 'laughable': 1,\n",
       " 'horrid': -1,\n",
       " '#yuk': -1,\n",
       " 'use': 1,\n",
       " 'promoz': 1,\n",
       " '#unprofessional': -1,\n",
       " 'working': 1,\n",
       " 'lose': -1,\n",
       " 'catchy': 1,\n",
       " 'positive': 1,\n",
       " 'visit': 1,\n",
       " 'story': -1,\n",
       " '#lowlife': -1,\n",
       " 'live': 1,\n",
       " '#neveragain': -1,\n",
       " 'music': 1,\n",
       " 'recommend': 1,\n",
       " '#studentproblems': -1,\n",
       " 'tacky': -1,\n",
       " '#prestige': 1,\n",
       " 'loving': 1,\n",
       " 'sort': 1,\n",
       " '#education': 1,\n",
       " 'sharing': 1,\n",
       " '#glamorous': 1,\n",
       " 'rich': 1,\n",
       " '#dontdoit': -1,\n",
       " 'afford': 1,\n",
       " \"didn't\": -1,\n",
       " 'mad': -1,\n",
       " '#ew': -1,\n",
       " 'haha': 1,\n",
       " 'ooh': 1,\n",
       " 'refrain': -1,\n",
       " 'chocolate': 1,\n",
       " 'phone': 1,\n",
       " 'train': -1,\n",
       " \"ain't\": -1,\n",
       " 'rip': -1,\n",
       " 'adverts': -1,\n",
       " '#poise': 1,\n",
       " 'baby': 1,\n",
       " 'ok': 1,\n",
       " 'striving': 1,\n",
       " 'home-based': 1,\n",
       " 'glad': 1,\n",
       " '#trash': -1,\n",
       " 'customer': 1,\n",
       " 'smh': -1,\n",
       " '#feelingsorryformyself': -1,\n",
       " 'join': 1,\n",
       " 'room': 1,\n",
       " 'hour': -1,\n",
       " 'car': 1,\n",
       " '#norespect': -1,\n",
       " 'oh': 1,\n",
       " 'sucks': -1,\n",
       " 'tnx': 1,\n",
       " 'cheating': -1,\n",
       " '#shocking': -1,\n",
       " '#majestic': 1,\n",
       " '#makesmesick': -1,\n",
       " '#skincare': 1,\n",
       " 'birffday': 1,\n",
       " 'following': 1,\n",
       " 'meet': 1,\n",
       " 'disgust': -1,\n",
       " '#jobless': -1,\n",
       " '#yuck': -1,\n",
       " '#cry': -1,\n",
       " 'beautiful': 1,\n",
       " 'excuses': -1,\n",
       " 'sinister': -1,\n",
       " 'embarrassing': -1,\n",
       " '#pleasestop': -1,\n",
       " 'give': -1,\n",
       " '#ignorant': -1,\n",
       " 'awesome': 1,\n",
       " 'share': 1,\n",
       " 'fuck': -1,\n",
       " 'accept': 1,\n",
       " 'boycott': -1,\n",
       " '#ripoff': -1,\n",
       " 'disbelieve': -1,\n",
       " 'thrilled': 1,\n",
       " 'want': 1,\n",
       " 'mmmmmm': 1,\n",
       " 'offense': -1,\n",
       " '#helpme': -1,\n",
       " 'r.i.p.': -1,\n",
       " \"#fabulous'nis\": 1,\n",
       " 'absolute': 1,\n",
       " 'needs': -1,\n",
       " 'answer': 1,\n",
       " 'end': -1,\n",
       " 'awkward': -1,\n",
       " '#gtfo': -1,\n",
       " 'watching': 1,\n",
       " '#nails': 1,\n",
       " '#growup': -1,\n",
       " '#notfunny': -1,\n",
       " 'thanking': 1,\n",
       " 'immune': 1,\n",
       " 'amazing': 1,\n",
       " '#notgoodenough': -1,\n",
       " 'disgusting': -1,\n",
       " 'freshly': 1,\n",
       " 'endorsement': 1,\n",
       " 'interview': 1,\n",
       " '#justno': -1,\n",
       " '#angry': -1,\n",
       " '#scary': -1,\n",
       " 'badly': -1,\n",
       " '#happygirl': 1,\n",
       " 'okay': 1,\n",
       " 'beauty': 1,\n",
       " 'bday': 1,\n",
       " 'watch': 1,\n",
       " 'disgusted': -1,\n",
       " 'pathetic': -1,\n",
       " 'thankyou': 1,\n",
       " 'mmm': 1,\n",
       " 'birthday': 1,\n",
       " 'unsurprising': -1,\n",
       " 'coming': -1,\n",
       " 'hahahaha': 1,\n",
       " 'waste': -1,\n",
       " 'spelling': 1,\n",
       " '#talented': 1,\n",
       " 'unselfish': 1,\n",
       " 'grow': 1,\n",
       " 'man': 1,\n",
       " 'short': 1,\n",
       " '#immature': -1,\n",
       " 'remember': 1,\n",
       " '#selfish': -1,\n",
       " 'read': 1,\n",
       " '#littlethings': 1,\n",
       " '#stillhoping': 1,\n",
       " 'lost': -1,\n",
       " 'anytime': 1,\n",
       " 'think': 1,\n",
       " 'cuteeee': 1,\n",
       " 'honestly': 1,\n",
       " 'pay': -1,\n",
       " '#inspirational': 1,\n",
       " 'pleasure': 1,\n",
       " '#inspiring': 1,\n",
       " 'playing': 1,\n",
       " 'heehee': 1,\n",
       " 'apreciate': 1,\n",
       " '#whoops': 1,\n",
       " 'cute': 1,\n",
       " '#sorrynotsorry': -1,\n",
       " 'help': 1,\n",
       " '#nasty': -1,\n",
       " \"don't\": -1,\n",
       " '#cruel': -1,\n",
       " 'indeed': 1,\n",
       " '#gag': -1,\n",
       " '#breathtaking': 1,\n",
       " 'satisfied': 1,\n",
       " 'years': 1,\n",
       " '#fantastic': 1,\n",
       " 'course': 1,\n",
       " \"won't\": -1,\n",
       " 'looks': 1,\n",
       " 'hell': -1,\n",
       " 'smiling': 1,\n",
       " 'neurotic': -1,\n",
       " '#cold': -1,\n",
       " '#leadership': 1,\n",
       " '#sweet': 1,\n",
       " 'perfect': 1,\n",
       " 'delay': -1,\n",
       " '#nomoney': -1,\n",
       " '#socomfy': 1,\n",
       " 'disconnect': -1,\n",
       " 'thank': 1,\n",
       " '#graceful': 1,\n",
       " 'interesting': 1,\n",
       " 'howdy': 1,\n",
       " 'mortar': 1,\n",
       " ';d': -1,\n",
       " 'hot': 1,\n",
       " 'faves': 1,\n",
       " 'actually': 1,\n",
       " 'better': 1,\n",
       " 'grade': 1,\n",
       " 'blatant': -1,\n",
       " '#boreoff': -1,\n",
       " 'ruining': -1,\n",
       " 'thanx': 1,\n",
       " '#thingsyoushouldntdo': -1,\n",
       " 'tweets': 1,\n",
       " '#tasteless': -1,\n",
       " ':s': -1,\n",
       " 'spreading': -1,\n",
       " \"haven't\": -1,\n",
       " 'hahah': 1,\n",
       " 'dad': 1,\n",
       " '#loser': -1,\n",
       " 'coffee': 1,\n",
       " 'good': 1,\n",
       " 'sucking': -1,\n",
       " 'later': -1,\n",
       " 'food': 1,\n",
       " 'timestamp': -1,\n",
       " 'safe': 1,\n",
       " 'introduce': 1,\n",
       " 'break': -1,\n",
       " 'band': 1,\n",
       " 'shameful': -1,\n",
       " '#based': 1,\n",
       " '#smiles': 1,\n",
       " 'not': -1,\n",
       " 'tragedy': -1,\n",
       " '#pig': -1,\n",
       " 'realize': 1,\n",
       " 'bank': 1,\n",
       " '#sorethroat': -1,\n",
       " '#juststop': -1,\n",
       " 'grammar': 1,\n",
       " 'name': 1,\n",
       " 'pregnant': 1,\n",
       " '#horror': -1,\n",
       " 'eww': -1,\n",
       " 'times': -1,\n",
       " 'ewwww': -1,\n",
       " 'refereeing': -1,\n",
       " '#disrespectful': -1,\n",
       " 'truth': 1,\n",
       " 'rock': 1,\n",
       " 'disapprove': -1,\n",
       " 'acoustic': 1,\n",
       " 'went': 1,\n",
       " 'shareholder': -1,\n",
       " 'luck': 1,\n",
       " 'prohibit': -1,\n",
       " 'smexy': 1,\n",
       " '#ungrateful': -1,\n",
       " 'house': 1,\n",
       " 'hard': -1,\n",
       " 'yeah': 1,\n",
       " '#needy': -1,\n",
       " '#sohappy': 1,\n",
       " '#brains': 1,\n",
       " '#relaxation': 1,\n",
       " '#passion': 1,\n",
       " 'year': 1,\n",
       " 'th*nks': 1,\n",
       " '#getalife': -1,\n",
       " 'girl': 1,\n",
       " 'morning': 1,\n",
       " 'apprecaite': 1,\n",
       " '#outstanding': 1,\n",
       " '#horrible': -1,\n",
       " 'leave': -1,\n",
       " 'huh': -1,\n",
       " 'mouths': 1,\n",
       " '#mad': -1,\n",
       " 'disgusts': -1,\n",
       " 'miss': 1,\n",
       " 'ha': 1,\n",
       " '#notwell': -1,\n",
       " '#partytime': 1,\n",
       " '#indigenous': 1,\n",
       " '#dumb': -1,\n",
       " 'looking': 1,\n",
       " 'mess': -1,\n",
       " 'seriously': 1,\n",
       " 'stuff': -1,\n",
       " '#stupid': -1,\n",
       " 'nite': 1,\n",
       " 'hilarious': 1,\n",
       " 'got': 1,\n",
       " 'fondly': 1,\n",
       " 'ass': -1,\n",
       " 'shut': -1,\n",
       " '#confident': 1,\n",
       " 'shit': -1,\n",
       " '#boring': -1,\n",
       " '#sweetheart': 1,\n",
       " 'alrighty': 1,\n",
       " 'bitches': -1,\n",
       " 'guess': 1,\n",
       " 'free': 1,\n",
       " 'pleasant': 1,\n",
       " '#filth': -1,\n",
       " '#ashamed': -1,\n",
       " 'giveaway': 1,\n",
       " '#bethatway': -1,\n",
       " 'put': 1,\n",
       " 'teach': 1,\n",
       " 'wanted': 1,\n",
       " 'yummyy': 1,\n",
       " 'care': 1,\n",
       " 'scary': -1,\n",
       " 'dammit': -1,\n",
       " '#wasteoftime': -1,\n",
       " 'tweeting': 1,\n",
       " 'shocking': -1,\n",
       " '#fuckoff': -1,\n",
       " 'yup': 1,\n",
       " 'bummer': -1,\n",
       " 'ask': -1,\n",
       " 'days': 1,\n",
       " 'keep': 1,\n",
       " 'thing': 1,\n",
       " 'smartass': -1,\n",
       " 'abuse': -1,\n",
       " 'yum': 1,\n",
       " 'hopeless': -1,\n",
       " 'woooohoooo': 1,\n",
       " 'misses': -1,\n",
       " '#skin': -1,\n",
       " 'blush': 1,\n",
       " 'cellulite': -1,\n",
       " 'stimulate': 1,\n",
       " 'fuckin': -1,\n",
       " '#optimistic': 1,\n",
       " 'gag': -1,\n",
       " 'scum': -1,\n",
       " 'constantly': 1,\n",
       " 'dont': -1,\n",
       " 'unfollowed': -1,\n",
       " 'feel': 1,\n",
       " 'question': 1,\n",
       " 'embarrassed': -1,\n",
       " 'wash': 1,\n",
       " 'featured': 1,\n",
       " 'done': 1,\n",
       " 'lol': 1,\n",
       " 'hang': -1,\n",
       " '#notmyfault': -1,\n",
       " 'vote': 1,\n",
       " 'art': 1,\n",
       " 'message': 1,\n",
       " 'hehe': 1,\n",
       " 'nonrandom': 1,\n",
       " \"doesn't\": -1,\n",
       " 'little': 1,\n",
       " 'long': 1,\n",
       " 'sadly': -1,\n",
       " 'coooool': 1,\n",
       " 'start': 1,\n",
       " 'guys': 1,\n",
       " 'top': 1,\n",
       " 'girls': 1,\n",
       " '#dirty': -1,\n",
       " '#cheat': -1,\n",
       " 'needed': 1,\n",
       " 'fml': -1,\n",
       " 'wonderful': 1,\n",
       " '#broken': -1,\n",
       " 'rockin': 1,\n",
       " 'kewl': 1,\n",
       " '#letdown': -1,\n",
       " 'turnovers': -1,\n",
       " '#lonely': -1,\n",
       " 'friend': 1,\n",
       " '#giveaway': 1,\n",
       " 'lot': 1,\n",
       " 'relationship': 1,\n",
       " '#rantover': -1,\n",
       " 'hun': 1,\n",
       " 'nobody': -1,\n",
       " 'park': 1,\n",
       " '#hater': -1,\n",
       " 'depressing': -1,\n",
       " 'ridiculous': -1,\n",
       " 'refund': 1,\n",
       " 'rain': -1,\n",
       " 'probably': 1,\n",
       " 'sign': 1,\n",
       " 'appreciated': 1,\n",
       " 'lovee': 1,\n",
       " 'loved': 1,\n",
       " '#love': 1,\n",
       " '#inspire': 1,\n",
       " 'lmao': 1,\n",
       " 'ew': -1,\n",
       " '#disturbing': -1,\n",
       " 'yayyyy': 1,\n",
       " '#unique': 1,\n",
       " 'gotta': 1,\n",
       " \"mother's\": 1,\n",
       " '#inspired': 1,\n",
       " 'see': 1,\n",
       " 'bed': 1,\n",
       " 'project': 1,\n",
       " 'tummy': 1,\n",
       " '#flu': -1,\n",
       " 'loves': 1,\n",
       " '#sophisticated': 1,\n",
       " 'feeling': 1,\n",
       " 'bet': 1,\n",
       " '#fab': 1,\n",
       " 'outta': -1,\n",
       " '#slacking': -1,\n",
       " 'appreciate': 1,\n",
       " 'gotcha': -1,\n",
       " 'head': -1,\n",
       " '#depressed': -1,\n",
       " 'sad': -1,\n",
       " 'tonight': -1,\n",
       " '#cough': -1,\n",
       " 'say': -1,\n",
       " 'dislike': -1,\n",
       " '#fat': -1,\n",
       " 'defending': 1,\n",
       " 'disturbing': -1,\n",
       " '#style': 1,\n",
       " 'seen': 1,\n",
       " '#focused': 1,\n",
       " 'saw': 1,\n",
       " 'clearly': 1,\n",
       " '#tramp': -1,\n",
       " 'sell': -1,\n",
       " 'ballerina': 1,\n",
       " 'bastard': -1,\n",
       " '#oops': -1,\n",
       " '#disgusted': -1,\n",
       " 'congrats': 1,\n",
       " 'nighty': 1,\n",
       " 'self': 1,\n",
       " 'unwrap': -1,\n",
       " 'rotten': -1,\n",
       " 'unhook': 1,\n",
       " 'elegance': 1,\n",
       " 'terrific': 1,\n",
       " 'recommended': 1,\n",
       " 'breeze': 1,\n",
       " 'unnecessarily': -1,\n",
       " 'amusing': 1,\n",
       " '#cringe': -1,\n",
       " '#innovation': 1,\n",
       " '#inaperfectworld': 1,\n",
       " ';-)': 1,\n",
       " 'gutted': -1,\n",
       " 'play': 1,\n",
       " 'sure': 1,\n",
       " '#kind': 1,\n",
       " 'quote': 1,\n",
       " 'ramblings': -1,\n",
       " '#wasteofspace': -1,\n",
       " '#motivated': 1,\n",
       " 'mouth': -1,\n",
       " 'plan': 1,\n",
       " '#vomit': -1,\n",
       " 'nothing': -1,\n",
       " '#noticemepls': -1,\n",
       " 'class': 1,\n",
       " 'woooo': 1,\n",
       " 'death': -1,\n",
       " '#smelly': -1,\n",
       " '#mess': -1,\n",
       " 'expert': 1,\n",
       " 'scariest': -1,\n",
       " 'unmade': -1,\n",
       " 'sometimes': 1,\n",
       " 'teehee': 1,\n",
       " 'lie': -1,\n",
       " 'face': 1,\n",
       " '#petpeeve': -1,\n",
       " 'points': 1,\n",
       " 'sunset': 1,\n",
       " 'talk': 1,\n",
       " '#awesome': 1,\n",
       " 'link': 1,\n",
       " 'entertaining': 1,\n",
       " 'wine': 1,\n",
       " '#horrendous': -1,\n",
       " '#relaxed': 1,\n",
       " 'saying': 1,\n",
       " 'despise': -1,\n",
       " \"g'morning\": 1,\n",
       " 'shot': -1,\n",
       " '#sortitout': 1,\n",
       " '#needcuddles': 1,\n",
       " 'cheat': -1,\n",
       " 'lmaoo': 1,\n",
       " '#heartless': -1,\n",
       " '#jan1st': 1,\n",
       " '#unhappy': -1,\n",
       " 'wasted': -1,\n",
       " '#idiot': -1,\n",
       " 'd:': -1,\n",
       " '#babealert': 1,\n",
       " 'rough': -1,\n",
       " 'fine': 1,\n",
       " 'enjoyed': 1,\n",
       " 'feedback': 1,\n",
       " 'aint': -1,\n",
       " 'inspiring': 1,\n",
       " 'woot': 1,\n",
       " '#coward': -1,\n",
       " '#wimp': -1,\n",
       " 'explain': -1,\n",
       " 'dick': -1,\n",
       " '#sick': -1,\n",
       " 'fans': 1,\n",
       " 'unhappy': -1,\n",
       " 'failed': -1,\n",
       " 'unable': -1,\n",
       " 'black': -1,\n",
       " 'pretty': 1,\n",
       " 'photo': -1,\n",
       " '#middleclass': 1,\n",
       " 'ex': -1,\n",
       " 'hope': 1,\n",
       " 'wearing': -1,\n",
       " '#wtf': -1,\n",
       " 'gone': -1,\n",
       " 'goooodnight': 1,\n",
       " 'get': 1,\n",
       " 'goodluck': 1,\n",
       " 'impersonal': -1,\n",
       " 'banned': -1,\n",
       " 'endorsements': 1,\n",
       " 'sunny': 1,\n",
       " 'unfair': -1,\n",
       " '#disgrace': -1,\n",
       " 'media': -1,\n",
       " 'perfection': 1,\n",
       " 'wedding': 1,\n",
       " '#creepy': -1,\n",
       " 'nasty': -1,\n",
       " '#everyonelikesitbutme': -1,\n",
       " '#happytweet': 1,\n",
       " 'nearly': 1,\n",
       " 'unfortunately': -1,\n",
       " '#insidious': -1,\n",
       " 'rude': -1,\n",
       " '#no': -1,\n",
       " 'nicely': 1,\n",
       " 'cry': -1,\n",
       " 'teams': -1,\n",
       " 'remove': 1,\n",
       " 'tuned': 1,\n",
       " \":'(\": -1,\n",
       " \":')\": 1,\n",
       " 'bad': -1,\n",
       " 're-tweets': 1,\n",
       " 'fuzzball': -1,\n",
       " '#grumpy': -1,\n",
       " 'recommendations': 1,\n",
       " 'bought': 1,\n",
       " 'cringe': -1,\n",
       " 'bother': -1,\n",
       " 'thanks': 1,\n",
       " '#theworst': -1,\n",
       " 'asked': 1,\n",
       " 'inventive': 1,\n",
       " 'yeyy': 1,\n",
       " '#hate': -1,\n",
       " '#killyourself': -1,\n",
       " 'govt': -1,\n",
       " 'catchya': 1,\n",
       " '#ffoe': -1,\n",
       " '#peaceful': 1,\n",
       " 'beg': -1,\n",
       " 'unmoved': -1,\n",
       " 'goodnight': 1,\n",
       " 'defense': -1,\n",
       " 'dumb': -1,\n",
       " '#notgood': -1,\n",
       " 'desperate': -1,\n",
       " 'horrendous': -1,\n",
       " 'apologies': 1,\n",
       " 'close': -1,\n",
       " 'cold': -1,\n",
       " 'best': 1,\n",
       " 'wonder': 1,\n",
       " '#fuckup': -1,\n",
       " 'said': -1,\n",
       " '#celebrity': 1,\n",
       " '#bored': -1,\n",
       " 'luv': 1,\n",
       " '#scumbag': -1,\n",
       " 'lots': 1,\n",
       " 'movie': 1,\n",
       " 'wow': 1,\n",
       " '#annoyed': -1,\n",
       " 'please': 1,\n",
       " 'superb': 1,\n",
       " '#classless': -1,\n",
       " 'woo': 1,\n",
       " 'won': 1,\n",
       " 'cooperation': 1,\n",
       " 'refreshed': 1,\n",
       " '#incredible': 1,\n",
       " '#gotit': 1,\n",
       " 'reading': 1,\n",
       " 'disrespect': -1,\n",
       " 'email': -1,\n",
       " 'compliment': 1,\n",
       " 'watchin': 1,\n",
       " '#idiots': -1,\n",
       " 'bummed': -1,\n",
       " 'home-cooked': 1,\n",
       " 'never': -1,\n",
       " 'omg': 1,\n",
       " 'thankyouuu': 1,\n",
       " 'missing': -1,\n",
       " 'dividends': 1,\n",
       " 'attention': 1,\n",
       " 'yumm': 1,\n",
       " '#liar': -1,\n",
       " 'guitar': 1,\n",
       " 'embarrassment': -1,\n",
       " 'job': 1,\n",
       " 'sacked': -1,\n",
       " 'awards': 1,\n",
       " 'wear': 1,\n",
       " 'hurts': -1,\n",
       " 'fat': -1,\n",
       " '#useless': -1,\n",
       " 'come': 1,\n",
       " 'sleazy': -1,\n",
       " 'hours': -1,\n",
       " 'last': -1,\n",
       " 'post': 1,\n",
       " 'useless': -1,\n",
       " '#horrid': -1,\n",
       " 'joke': 1,\n",
       " 'ill': -1,\n",
       " 'botch': -1,\n",
       " '#canthelpit': -1,\n",
       " 'comment': -1,\n",
       " '#help': -1,\n",
       " '#notsorry': -1,\n",
       " 'assure': 1,\n",
       " '#ugh': -1,\n",
       " 'smilin': 1,\n",
       " 'reply': 1,\n",
       " 'approve': 1,\n",
       " 'thnx': 1,\n",
       " \"can't\": -1,\n",
       " 'yaay': 1,\n",
       " 'liked': 1,\n",
       " 'gooood': 1,\n",
       " 'reject': -1,\n",
       " 'point': 1,\n",
       " '#needajob': -1,\n",
       " 'sweet': 1,\n",
       " '#cheap': -1,\n",
       " 'whatever': -1,\n",
       " 'cd': 1,\n",
       " '#itsnotokayto': -1,\n",
       " 'subscribe': 1,\n",
       " 'laugh': 1,\n",
       " 'respect': 1,\n",
       " 'writing': 1,\n",
       " 'gratitude': 1,\n",
       " 'poem': 1,\n",
       " 'worthless': -1,\n",
       " 'yay': 1,\n",
       " '#skint': -1,\n",
       " \"g'nite\": 1,\n",
       " 'dazzling': 1,\n",
       " '#myweakness': -1,\n",
       " 'breath': 1,\n",
       " 'whoo': 1,\n",
       " '#selfmadesavagez': -1,\n",
       " 'judging': -1,\n",
       " 'lovely': 1,\n",
       " '#crying': -1,\n",
       " 'woke': 1,\n",
       " 'excitedddd': 1,\n",
       " 'life': 1,\n",
       " 'chai': 1,\n",
       " 'shopping': 1,\n",
       " 'innocent': 1,\n",
       " 'relaxing': 1,\n",
       " '#whatajoke': -1,\n",
       " '#slut': -1,\n",
       " 'sunshine': 1,\n",
       " '#tragic': -1,\n",
       " 'photos': 1,\n",
       " 'finished': -1,\n",
       " 'kids': 1,\n",
       " 'lives': 1,\n",
       " '-.-': 1,\n",
       " '#embarrassing': -1,\n",
       " 'child': 1,\n",
       " 'brightening': 1,\n",
       " '#inspiration': 1,\n",
       " 'fucking': -1,\n",
       " '#needalife': -1,\n",
       " '#hellno': -1,\n",
       " '#dreamy': 1,\n",
       " '#hateit': -1,\n",
       " '#spoiled': -1,\n",
       " 'gracias': 1,\n",
       " 'surrender': -1,\n",
       " 'look': 1,\n",
       " '#smashing': 1,\n",
       " 'worried': -1,\n",
       " 'budget': -1,\n",
       " 'heya': 1,\n",
       " '#nojob': -1,\n",
       " 'ugly': -1,\n",
       " '#disappointed': -1,\n",
       " '#sickening': -1,\n",
       " '#lovely': 1,\n",
       " 'hahaha': 1,\n",
       " 'fun': 1,\n",
       " 'rhymed': 1,\n",
       " '#partyon': 1,\n",
       " 'precede': 1,\n",
       " 'coolest': 1,\n",
       " 'real': 1,\n",
       " 'congratulation': 1,\n",
       " '<3': 1,\n",
       " 'find': 1,\n",
       " '#heartbroken': -1,\n",
       " '#eww': -1,\n",
       " 'site': -1,\n",
       " 'encouragement': 1,\n",
       " '#fake': -1,\n",
       " 'need': 1,\n",
       " 'cant': 1,\n",
       " 'evening': 1,\n",
       " 'late': -1,\n",
       " 'ready': 1,\n",
       " 'victims': -1,\n",
       " '#spew': -1,\n",
       " '#glorious': 1,\n",
       " 'breakfast': 1,\n",
       " '#needmoney': -1,\n",
       " '#shame': -1,\n",
       " 'grown': 1,\n",
       " 'funny': 1,\n",
       " '#cosmetics': 1,\n",
       " 'ignored': -1,\n",
       " 'yo': -1,\n",
       " 'things': 1,\n",
       " 'make': 1,\n",
       " 'ashamed': -1,\n",
       " 'stunk': -1,\n",
       " 'amount': 1,\n",
       " '#hardwork': 1,\n",
       " 'yahooo': 1,\n",
       " 'followers': 1,\n",
       " '#makeup': 1,\n",
       " '#stop': -1,\n",
       " 'texting': -1,\n",
       " 'party': 1,\n",
       " 'argh': -1,\n",
       " 'day': -1,\n",
       " 'killed': -1,\n",
       " 'week': 1,\n",
       " 'used': -1,\n",
       " '#stressed': -1,\n",
       " 'damn': -1,\n",
       " '#depressing': -1,\n",
       " 'unready': -1,\n",
       " '#happiness': 1,\n",
       " 'taxes': -1,\n",
       " 'weee': 1,\n",
       " 'cheers': 1,\n",
       " 'heartbroken': -1,\n",
       " 'moment': 1,\n",
       " '#angel': 1,\n",
       " 'awful': -1,\n",
       " '#thebest': 1,\n",
       " '#refreshing': 1,\n",
       " 'totally': 1,\n",
       " 'pleasantly': 1,\n",
       " 'broke': -1,\n",
       " '#sucks': -1,\n",
       " 'awesomeness': 1,\n",
       " 'unattractive': -1,\n",
       " '#needsleep': -1,\n",
       " 'thx': 1,\n",
       " '#whore': -1,\n",
       " 'whole': 1,\n",
       " 'well': 1,\n",
       " 'fever': -1,\n",
       " 'obviously': 1,\n",
       " 'thought': 1,\n",
       " 'person': 1,\n",
       " 'cheer': 1,\n",
       " 'hearty': 1,\n",
       " '#glamour': 1,\n",
       " 'deserve': 1,\n",
       " '#entertainment': 1,\n",
       " 'fail': -1,\n",
       " '#luxury': 1,\n",
       " 'loveeee': 1,\n",
       " 'left': -1,\n",
       " 'summer': 1,\n",
       " 'morons': -1,\n",
       " 'disloyal': -1,\n",
       " 'listening': 1,\n",
       " 'scarred': -1,\n",
       " '#failure': -1,\n",
       " 'ya': 1,\n",
       " 'money': 1,\n",
       " 'unfollow': -1,\n",
       " 'half': -1,\n",
       " 'muahahah': 1,\n",
       " 'yey': 1,\n",
       " 'quotes': 1,\n",
       " '#bravo': 1,\n",
       " 'kill': -1,\n",
       " 'greetings': 1,\n",
       " 'human': 1,\n",
       " 'using': -1,\n",
       " 'yep': 1,\n",
       " 'yes': 1,\n",
       " '#getoverit': -1,\n",
       " '#hideous': -1,\n",
       " 'thinking': 1,\n",
       " 'yaaay': 1,\n",
       " 'surprises': 1,\n",
       " '#lovelife': 1,\n",
       " '#cretin': -1,\n",
       " 'shhh': -1,\n",
       " 'blog': 1,\n",
       " 'depressed': -1,\n",
       " 'add': 1,\n",
       " 'book': 1,\n",
       " '#stunning': 1,\n",
       " 'dreams': 1,\n",
       " 'easy': 1,\n",
       " 'excited': 1,\n",
       " 'hates': -1,\n",
       " 'take': -1,\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inquirer 8640 3457\n",
      "mpqa 6886 6462\n",
      "bingliu 6785 6785\n",
      "inquirer mpqa agreement: 82.47\n",
      "inquirer mpqa agreement ignoring neutral: 98.50\n",
      "inquirer bingliu agreement: 84.39\n",
      "inquirer bingliu agreement ignoring neutral: 98.74\n",
      "mpqa bingliu agreement: 99.19\n",
      "mpqa bingliu agreement ignoring neutral: 99.44\n"
     ]
    }
   ],
   "source": [
    "## FIGURE OUT WHAT DOES THE COMPARE_LEXICONS DOES.\n",
    "compare_lexicons()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Induction : the idea is to generate the lexicons provided the corpus. This method makes sure that the lexicon are sensitive to the context they are drawn from. They may prove useful if we would like to assess them in a simiar context. For instance, financial lexicons will reflect better sentiments than using general lexicons such as SentiWordNet. Three ways purposed for induction \n",
    "\n",
    "- SENTPROP\n",
    "- DENSIFIER\n",
    "- Sentiment140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist(embeds, positive_seeds, negative_seeds, **kwargs):\n",
    "    polarities = {}\n",
    "    sim_mat = similarity_matrix(embeds, **kwargs)\n",
    "    for i, w in enumerate(embeds.iw):\n",
    "        if w not in positive_seeds and w not in negative_seeds:\n",
    "            pol = sum(sim_mat[embeds.wi[p_seed], i] for p_seed in positive_seeds)\n",
    "            pol -= sum(sim_mat[embeds.wi[n_seed], i] for n_seed in negative_seeds)\n",
    "            polarities[w] = pol\n",
    "    return polarities\n",
    "\n",
    "\n",
    "def pmi(count_embeds, positive_seeds, negative_seeds, smooth=0.01, **kwargs):\n",
    "    \"\"\"\n",
    "    Learns polarity scores using PMI with seed words.\n",
    "    Adapted from Turney, P. and M. Littman. \"Measuring Praise and Criticism: Inference of semantic orientation from assocition\".\n",
    "    ACM Trans. Inf. Sys., 2003. 21(4) 315-346.\n",
    "\n",
    "    counts is explicit embedding containing raw co-occurrence counts\n",
    "    \"\"\"\n",
    "    w_index = count_embeds.wi\n",
    "    c_index = count_embeds.ci\n",
    "    counts = count_embeds.m\n",
    "    polarities = {}\n",
    "    for w in count_embeds.iw:\n",
    "        if w not in positive_seeds and w not in negative_seeds:\n",
    "            pol = sum(np.log(counts[w_index[w], c_index[seed]] + smooth) \n",
    "                    - np.log(counts[w_index[seed],:].sum()) for seed in positive_seeds)\n",
    "            pol -= sum(np.log(counts[w_index[w], c_index[seed]] + smooth) \n",
    "                    - np.log(counts[w_index[seed],:].sum())for seed in negative_seeds)\n",
    "            polarities[w] = pol\n",
    "    return polarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data =  /home/ubuntu/workspace/nlpclass-1187-g-Mad_Titans/sa/embeddings_socialsent/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import polarity_induction_methods\n",
    "\n",
    "### THIS IS THE FUNCTION FOR INDUCING LEXICONS GIVEN THE SEEDS, EMBEDDINGS AND THE METHOD.\n",
    "def run_method(positive_seeds, negative_seeds, embeddings, transform_embeddings=False, post_densify=False,\n",
    "        method=polarity_induction_methods.densify, **kwargs):\n",
    "    \n",
    "    \n",
    "    if transform_embeddings:\n",
    "        print (\"Transforming embeddings...\")\n",
    "        embeddings = embedding_transformer.apply_embedding_transformation(embeddings, positive_seeds, negative_seeds, n_dim=50)\n",
    "    \n",
    "    \n",
    "    ## using densify method\n",
    "    if post_densify:\n",
    "        polarities = method(embeddings, positive_seeds, negative_seeds, **kwargs)\n",
    "        top_pos = [word for word in \n",
    "                sorted(polarities, key = lambda w : -polarities[w])[:150]]\n",
    "        top_neg = [word for word in \n",
    "                sorted(polarities, key = lambda w : polarities[w])[:150]]\n",
    "        top_pos.extend(positive_seeds)\n",
    "        top_neg.extend(negative_seeds)\n",
    "        return polarity_induction_methods.densify(embeddings, top_pos, top_neg)\n",
    "    \n",
    "    \n",
    "    positive_seeds = [s for s in positive_seeds if s in embeddings]\n",
    "    negative_seeds = [s for s in negative_seeds if s in embeddings]\n",
    "    \n",
    "    \n",
    "    return method(embeddings, positive_seeds, negative_seeds, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seeds\n",
    "from representations.representation_factory import create_representation\n",
    "import constants\n",
    "\n",
    "def evaluate_methods():\n",
    "    \"\"\"\n",
    "    Evaluates different methods on standard English.\n",
    "    \"\"\"\n",
    "    print (\"Getting evalution words..\")\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    ## inquirer is ternrary -1,0,1\n",
    "    lexicon = load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "    \n",
    "    ## kuperman is continus -5.0 to 5\n",
    "    kuperman = load_lexicon(\"kuperman\", remove_neutral=False)\n",
    "    eval_words = set(lexicon.keys())\n",
    "\n",
    "    qwn = load_lexicon(\"qwn-scores\")\n",
    "    for word in lexicon:\n",
    "        if not word in qwn:\n",
    "            qwn[word] = 0\n",
    "\n",
    "    positive_seeds, negative_seeds = seeds.hist_seeds()\n",
    "    \n",
    "    common_embed = create_representation(\"GIGA\", constants.GLOVE_EMBEDDINGS,eval_words.union(positive_seeds).union(negative_seeds))\n",
    "    \n",
    "    \n",
    "    embed_words = set(common_embed.iw)\n",
    "    \n",
    "    \n",
    "    eval_words = eval_words.intersection(embed_words)\n",
    "\n",
    "    eval_words = [word for word in eval_words \n",
    "            if not word in positive_seeds \n",
    "            and not word in negative_seeds]\n",
    "    \n",
    "    \n",
    "    print (\"Evaluating with \", len(eval_words), \"out of\", len(lexicon))\n",
    "    print (\"SentProp:\")\n",
    "    \n",
    "    \n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "            common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "            method=polarity_induction_methods.label_propagate_probabilistic,beta=0.99, nn=10)\n",
    "    \n",
    "    \n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "    util.write_pickle(polarities, \"tmp/gi-cc-walk-pols.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting evalution words..\n",
      "Evaluating with  8528 out of 8640\n",
      "SentProp:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DEFAULT_ARGUMENTS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-58a26d49cf5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_methods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-22e7b4c15858>\u001b[0m in \u001b[0;36mevaluate_methods\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     polarities = run_method(positive_seeds, negative_seeds, \n\u001b[1;32m     44\u001b[0m             \u001b[0mcommon_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_subembed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_seeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_seeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             method=polarity_induction_methods.label_propagate_probabilistic,beta=0.99, nn=10, **DEFAULT_ARGUMENTS)\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DEFAULT_ARGUMENTS' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seeds\n",
    "from representations.representation_factory import create_representation\n",
    "import constants\n",
    "\n",
    "\n",
    "lexicon = load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "eval_words = set(lexicon.keys())\n",
    "positive_seeds, negative_seeds = seeds.hist_seeds()\n",
    "eval_words_with_seeds=eval_words.union(positive_seeds).union(negative_seeds)\n",
    "\n",
    "\n",
    "common_embed = create_representation(\"GIGA\", constants.GLOVE_EMBEDDINGS,eval_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating with  8528 out of 8640\n",
      "SentProp:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'util' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-8ef90393f04e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mcommon_embed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_subembed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_seeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_seeds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         method=polarity_induction_methods.label_propagate_probabilistic,beta=0.99, nn=10)\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tmp/gi-cc-walk-pols.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'util' is not defined"
     ]
    }
   ],
   "source": [
    "embed_words = set(common_embed.iw)\n",
    "\n",
    "\n",
    "eval_words = set(eval_words).intersection(embed_words)\n",
    "\n",
    "eval_words = [word for word in eval_words  if not word in positive_seeds   and not word in negative_seeds]\n",
    "\n",
    "\n",
    "print (\"Evaluating with \", len(eval_words), \"out of\", len(lexicon))\n",
    "print (\"SentProp:\")\n",
    "\n",
    "\n",
    "polarities = run_method(positive_seeds, negative_seeds, \n",
    "        common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "        method=polarity_induction_methods.label_propagate_probabilistic,beta=0.99, nn=10)\n",
    "\n",
    "\n",
    "util.write_pickle(polarities, \"tmp/gi-cc-walk-pols.pkl\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname=\"tmp/gi-cc-walk-pols.pkl\"\n",
    "\n",
    "def write_pickle(o, fname):\n",
    "    with open(fname, 'w') as f:\n",
    "        cPickle.dump(polarities, f, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting evalution words..\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'accurateness'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-670a0c9eb600>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mqwn\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau_lexicon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkuperman\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tmp/gi-cc-walk-pols.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/nlpclass-1187-g-Mad_Titans/sa/evaluate_methods.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(polarities, lexicon, eval_words, tau_lexicon, tern)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_words\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtau_lexicon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m     \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_prec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauc\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0mpolarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolarities\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/nlpclass-1187-g-Mad_Titans/sa/evaluate_methods.py\u001b[0m in \u001b[0;36mbinary_metrics\u001b[0;34m(polarities, lexicon, eval_words, print_predictions, top_perc)\u001b[0m\n\u001b[1;32m    536\u001b[0m         polarities = {word:polarities[word] for word in \n\u001b[1;32m    537\u001b[0m                 sorted(eval_words, key = lambda w : abs(polarities[w]-0.5), reverse=True)[:int(top_perc*len(polarities))]}\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0mpolarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_words\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolarities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/nlpclass-1187-g-Mad_Titans/sa/evaluate_methods.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    536\u001b[0m         polarities = {word:polarities[word] for word in \n\u001b[1;32m    537\u001b[0m                 sorted(eval_words, key = lambda w : abs(polarities[w]-0.5), reverse=True)[:int(top_perc*len(polarities))]}\n\u001b[0;32m--> 538\u001b[0;31m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m         \u001b[0mpolarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval_words\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolarities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'accurateness'"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from historical import vocab\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, f1_score\n",
    "from scipy.stats import kendalltau\n",
    "from representations.representation_factory import create_representation\n",
    "from evaluate_methods import evaluate\n",
    "\n",
    "print (\"Getting evalution words..\")\n",
    "np.random.seed(0)\n",
    "\n",
    "## inquirer is ternrary -1,0,1\n",
    "lexicon = load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "\n",
    "## kuperman is continus -5.0 to 5\n",
    "kuperman = load_lexicon(\"kuperman\", remove_neutral=False)\n",
    "eval_words = set(lexicon.keys())\n",
    "\n",
    "qwn = load_lexicon(\"qwn-scores\")\n",
    "for word in lexicon:\n",
    "    if not word in qwn:\n",
    "        qwn[word] = 0\n",
    "            \n",
    "evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "util.write_pickle(polarities, \"tmp/gi-cc-walk-pols.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'accurateness'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-e7743ac2d3f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#list(filter(lambda x:x=='accurateness',eval_words))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpolarities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accurateness'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'accurateness'"
     ]
    }
   ],
   "source": [
    "#list(filter(lambda x:x=='accurateness',eval_words))\n",
    "polarities['accurateness']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from socialsent import constants\n",
    "from socialsent import util\n",
    "from socialsent import polarity_induction_methods\n",
    "from socialsent import seeds\n",
    "from socialsent import lexicons\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import embedding_transformer\n",
    "\n",
    "from operator import itemgetter\n",
    "from socialsent.historical import vocab\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, confusion_matrix, f1_score\n",
    "from scipy.stats import kendalltau\n",
    "from socialsent.representations.representation_factory import create_representation\n",
    "\n",
    "DEFAULT_ARGUMENTS = dict(\n",
    "        # for iterative graph algorithms\n",
    "        similarity_power=1,\n",
    "        arccos=True,\n",
    "        max_iter=50,\n",
    "        epsilon=1e-6,\n",
    "        sym=True,\n",
    "\n",
    "        # for learning embeddings transformation\n",
    "        n_epochs=50,\n",
    "        force_orthogonal=False,\n",
    "        batch_size=100,\n",
    "        cosine=False,\n",
    "\n",
    "        ## bootstrap\n",
    "        num_boots=1,\n",
    "        n_procs=1,\n",
    ")\n",
    "\n",
    "def evaluate_methods():\n",
    "    \"\"\"\n",
    "    Evaluates different methods on standard English.\n",
    "    \"\"\"\n",
    "    print (\"Getting evalution words..\")\n",
    "    np.random.seed(0)\n",
    "    lexicon = lexicons.load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "    kuperman = lexicons.load_lexicon(\"kuperman\", remove_neutral=False)\n",
    "    eval_words = set(lexicon.keys())\n",
    "\n",
    "    # load in WordNet lexicon and pad with zeros for missing words\n",
    "    # (since these are implicitly zero for this method)\n",
    "    qwn = lexicons.load_lexicon(\"qwn-scores\")\n",
    "    for word in lexicon:\n",
    "        if not word in qwn:\n",
    "            qwn[word] = 0\n",
    "\n",
    "    positive_seeds, negative_seeds = seeds.hist_seeds()\n",
    "\n",
    "    common_embed = create_representation(\"GIGA\", constants.GOOGLE_EMBEDDINGS, \n",
    "            eval_words.union(positive_seeds).union(negative_seeds))\n",
    "    embed_words = set(common_embed.iw)\n",
    "    eval_words = eval_words.intersection(embed_words)\n",
    "\n",
    "    eval_words = [word for word in eval_words \n",
    "            if not word in positive_seeds \n",
    "            and not word in negative_seeds]\n",
    "    print (\"Evaluating with \", len(eval_words), \"out of\", len(lexicon))\n",
    "\n",
    "#    \n",
    "#    print \"WordNet:\"\n",
    "#    evaluate(qwn, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "#\n",
    "#    print \"Densifier:\"\n",
    "#    polarities = run_method(positive_seeds, negative_seeds, \n",
    "#            common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "#            method=polarity_induction_methods.bootstrap, score_method=polarity_induction_methods.densify,\n",
    "#            **DEFAULT_ARGUMENTS)\n",
    "#    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "\n",
    "    print( \"SentProp:\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "            common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "            method=polarity_induction_methods.label_propagate_probabilistic,\n",
    "            #method=polarity_induction_methods.bootstrap, \n",
    "            beta=0.99, nn=10,\n",
    "\n",
    "            **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "    util.write_pickle(polarities, \"tmp/gi-cc-walk-pols.pkl\")\n",
    "\n",
    "def hyperparam_eval():\n",
    "    print (\"Getting evaluation words and embeddings\")\n",
    "    lexicon = lexicons.load_lexicon(\"bingliu\", remove_neutral=False)\n",
    "    eval_words = set(lexicon.keys())\n",
    "\n",
    "    positive_seeds, negative_seeds = seeds.hist_seeds()\n",
    "\n",
    "    common_embed = create_representation(\"GIGA\", constants.COMMON_EMBEDDINGS, \n",
    "            eval_words.union(positive_seeds).union(negative_seeds))\n",
    "    common_words = set(common_embed.iw)\n",
    "    eval_words = eval_words.intersection(common_words)\n",
    "\n",
    "    hist_embed = create_representation(\"SVD\", constants.SVD_EMBEDDINGS + \"1990\")\n",
    "    hist_words = set(hist_embed.iw)\n",
    "    eval_words = eval_words.intersection(hist_words)\n",
    "\n",
    "    eval_words = [word for word in eval_words\n",
    "            if not word in positive_seeds \n",
    "            and not word in negative_seeds] \n",
    "\n",
    "    print (\"SentProp...\")\n",
    "    for nn in [5, 10, 25, 50]:\n",
    "        for beta in [0.8, 0.9, 0.95, 0.99]:\n",
    "          print( \"Common\")\n",
    "          polarities = run_method(positive_seeds, negative_seeds, \n",
    "                    common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "                    method=polarity_induction_methods.random_walk, \n",
    "                    nn=nn, beta=beta,\n",
    "                    **DEFAULT_ARGUMENTS)\n",
    "          evaluate(polarities, lexicon, eval_words)\n",
    "          print( \"Hist\")\n",
    "          polarities = run_method(positive_seeds, negative_seeds, \n",
    "                    hist_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "                    method=polarity_induction_methods.random_walk, \n",
    "                    nn=nn, beta=beta,\n",
    "                    **DEFAULT_ARGUMENTS)\n",
    "          evaluate(polarities, lexicon, eval_words)\n",
    "\n",
    "    print (\"Densify...\")\n",
    "    for lr in [0.001, 0.01, 0.1, 0.5]:\n",
    "        for reg in [0.001, 0.01, 0.1, 0.5]:\n",
    "          print( \"LR : \", lr, \"Reg: \", reg)\n",
    "          print (\"Common\")\n",
    "          polarities = run_method(positive_seeds, negative_seeds, \n",
    "                    common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "                    method=polarity_induction_methods.densify, \n",
    "                    lr=lr, regularization_strength=reg,\n",
    "                    **DEFAULT_ARGUMENTS)\n",
    "          evaluate(polarities, lexicon, eval_words, tern=False)\n",
    "          print (\"Hist\")\n",
    "          polarities = run_method(positive_seeds, negative_seeds, \n",
    "                    hist_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "                    method=polarity_induction_methods.densify, \n",
    "                    lr=lr, regularization_strength=reg,\n",
    "                    **DEFAULT_ARGUMENTS)\n",
    "          evaluate(polarities, lexicon, eval_words, tern=False)\n",
    "\n",
    "\n",
    "def evaluate_overlap_methods():\n",
    "    \"\"\"\n",
    "    Evaluate different methods on standard English,\n",
    "    but restrict to words that are present in the 1990s portion of historical data.\n",
    "    \"\"\"\n",
    "    print (\"Getting evalution words and embeddings..\")\n",
    "    np.random.seed(0)\n",
    "    lexicon = lexicons.load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "    kuperman = lexicons.load_lexicon(\"kuperman\", remove_neutral=False)\n",
    "    eval_words = set(lexicon.keys())\n",
    "\n",
    "    # load in WordNet lexicon and pad with zeros for missing words\n",
    "    # (since these are implicitly zero for this method)\n",
    "    qwn = lexicons.load_lexicon(\"qwn-scores\")\n",
    "    for word in lexicon:\n",
    "        if not word in qwn:\n",
    "            qwn[word] = 0\n",
    "\n",
    "    positive_seeds, negative_seeds = seeds.hist_seeds()\n",
    "\n",
    "#    common_embed = create_representation(\"GIGA\", constants.COMMON_EMBEDDINGS, \n",
    "#            eval_words.union(positive_seeds).union(negative_seeds))\n",
    "#    common_words = set(common_embed.iw)\n",
    "#    eval_words = eval_words.intersection(common_words)\n",
    "\n",
    "    hist_embed = create_representation(\"SVD\", constants.COHA_EMBEDDINGS + \"2000\")\n",
    "    hist_counts = create_representation(\"Explicit\", constants.COHA_COUNTS + \"2000\", normalize=False)\n",
    "    hist_words = set(hist_embed.iw)\n",
    "    eval_words = eval_words.intersection(hist_words)\n",
    "\n",
    "    eval_words = [word for word in eval_words\n",
    "            if not word in positive_seeds \n",
    "            and not word in negative_seeds] \n",
    "\n",
    "    hist_counts = hist_counts.get_subembed(set(eval_words).union(positive_seeds).union(negative_seeds), \n",
    "            restrict_context=False)\n",
    "\n",
    "    print( \"Evaluating with \", len(eval_words), \"out of\", len(lexicon))\n",
    "\n",
    "    print (\"PMI\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds,\n",
    "            hist_counts,\n",
    "            method=polarity_induction_methods.bootstrap,\n",
    "            score_method=polarity_induction_methods.pmi,\n",
    "            **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "\n",
    "    print\n",
    "    evaluate(qwn, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "\n",
    "    print( \"SentProp with 1990s Fic embeddings\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        hist_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "                        method=polarity_induction_methods.bootstrap,\n",
    "                        score_method=polarity_induction_methods.random_walk, \n",
    "                        nn=25, beta=0.9,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "    print\n",
    "    \n",
    "    print( \"Densifier with 1990s Fic embeddings\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        hist_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "                        method=polarity_induction_methods.bootstrap,\n",
    "                        score_method=polarity_induction_methods.densify,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "    \n",
    "\n",
    "    print (\"Velikovich with 1990s Fic embeddings\")\n",
    "    hist_counts.normalize()\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        hist_counts,\n",
    "                        method=polarity_induction_methods.bootstrap,\n",
    "                        score_method=polarity_induction_methods.graph_propagate,\n",
    "                        T=3,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "    \n",
    "\n",
    "#    print \"SentProp with CC\"\n",
    "#    polarities = run_method( positive_seeds, negative_seeds, \n",
    "#                        common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "#                        method=polarity_induction_methods.bootstrap,\n",
    "#                        score_method=polarity_induction_methods.random_walk,\n",
    "#                        beta=0.99, nn=10,\n",
    "#                        **DEFAULT_ARGUMENTS)\n",
    "#    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "#\n",
    "#    print \"Densifier with CC\"\n",
    "#    polarities = run_method( positive_seeds, negative_seeds, \n",
    "#                        common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "#                        method=polarity_induction_methods.bootstrap,\n",
    "#                        score_method=polarity_induction_methods.densify,\n",
    "#                        **DEFAULT_ARGUMENTS)\n",
    "#    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_adj_methods():\n",
    "    \"\"\"\n",
    "    Evaluate different methods on standard English,\n",
    "    but restrict to words that are present in the 1990s portion of historical data.\n",
    "    \"\"\"\n",
    "    print (\"Getting evalution words and embeddings..\")\n",
    "    np.random.seed(0)\n",
    "    lexicon = lexicons.load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "    kuperman = lexicons.load_lexicon(\"kuperman\", remove_neutral=False)\n",
    "    eval_words = set(lexicon.keys())\n",
    "    adjs = vocab.pos_words(\"1990\", \"ADJ\")\n",
    "\n",
    "    # load in WordNet lexicon and pad with zeros for missing words\n",
    "    # (since these are implicitly zero for this method)\n",
    "    qwn = lexicons.load_lexicon(\"qwn-scores\")\n",
    "    for word in lexicon:\n",
    "        if not word in qwn:\n",
    "            qwn[word] = 0\n",
    "\n",
    "    positive_seeds, negative_seeds = seeds.adj_seeds()\n",
    "\n",
    "    common_embed = create_representation(\"GIGA\", constants.COMMON_EMBEDDINGS, \n",
    "            eval_words.union(positive_seeds).union(negative_seeds))\n",
    "    common_words = set(common_embed.iw)\n",
    "    eval_words = eval_words.intersection(common_words)\n",
    "\n",
    "    hist_embed = create_representation(\"SVD\", constants.COHA_EMBEDDINGS + \"2000\")\n",
    "    hist_counts = create_representation(\"Explicit\", constants.COUNTS + \"1990\", normalize=False)\n",
    "    hist_words = set(hist_embed.iw)\n",
    "    eval_words = eval_words.intersection(hist_words)\n",
    "\n",
    "    embed_words = [word for word in adjs if word in hist_words and word in common_words]\n",
    "    eval_words = [word for word in eval_words if word in embed_words\n",
    "            and not word in positive_seeds \n",
    "            and not word in negative_seeds] \n",
    "    \n",
    "    hist_counts = hist_counts.get_subembed(set(eval_words).union(positive_seeds).union(negative_seeds), \n",
    "            restrict_context=False)\n",
    "\n",
    "    print (\"Evaluating with \", len(eval_words), \"out of\", len(lexicon))\n",
    "    print (\"Embeddings with \", len(embed_words))\n",
    "\n",
    "    print (\"PMI\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds,\n",
    "            hist_counts,\n",
    "            method=polarity_induction_methods.bootstrap,\n",
    "            score_method=polarity_induction_methods.pmi,\n",
    "            boot_size=6,\n",
    "            **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "\n",
    "    \n",
    "    evaluate(qwn, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "\n",
    "    print (\"Dist with 1990s Fic embeddings\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        hist_embed.get_subembed(set(embed_words).union(negative_seeds).union(positive_seeds)),\n",
    "                        method=polarity_induction_methods.dist, \n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "    \n",
    "\n",
    "    print( \"Densifier with 1990s Fic embeddings\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        hist_embed.get_subembed(set(embed_words).union(negative_seeds).union(positive_seeds)),\n",
    "                        method=polarity_induction_methods.bootstrap, \n",
    "                        score_method=polarity_induction_methods.densify, \n",
    "                        boot_size=6,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "    \n",
    "\n",
    "    print( \"SentProp with 1990s Fic embeddings\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        hist_embed.get_subembed(set(embed_words).union(negative_seeds).union(positive_seeds)),\n",
    "                        method=polarity_induction_methods.bootstrap, \n",
    "                        nn=25, beta=0.9,\n",
    "                        boot_size=6,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "    \n",
    "\n",
    "    print (\"Velikovich with 1990s Fic embeddings\")\n",
    "    hist_counts.normalize()\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        hist_counts,\n",
    "                        method=polarity_induction_methods.bootstrap, \n",
    "                        score_method=polarity_induction_methods.graph_propagate,\n",
    "                        T=3,\n",
    "                        boot_size=6,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "    \n",
    "\n",
    "    print (\"SentProp with CC\")\n",
    "    polarities = run_method( positive_seeds, negative_seeds, \n",
    "                        common_embed.get_subembed(set(embed_words).union(negative_seeds).union(positive_seeds)),\n",
    "                        method=polarity_induction_methods.bootstrap, \n",
    "                        score_method=polarity_induction_methods.random_walk,\n",
    "                        beta=0.99, nn=10,\n",
    "                        boot_size=6,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "\n",
    "    print (\"Densifier with CC\")\n",
    "    polarities = run_method( positive_seeds, negative_seeds, \n",
    "                        common_embed.get_subembed(set(embed_words).union(negative_seeds).union(positive_seeds)),\n",
    "                        method=polarity_induction_methods.bootstrap, \n",
    "                        score_method=polarity_induction_methods.densify,\n",
    "                        boot_size=6,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "\n",
    "\n",
    "def evaluate_finance_methods():\n",
    "    np.random.seed(0)\n",
    "    print (\"Getting evalution words and embeddings..\")\n",
    "    gi = lexicons.load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "    lexicon = lexicons.load_lexicon(\"finance\", remove_neutral=True)\n",
    "\n",
    "    ### padding in neutrals from GI lexicon\n",
    "    gi_neut = [word for word in gi if gi[word] == 0]\n",
    "    gi_neut = np.random.choice(gi_neut, int( (float(len(gi_neut))/(len(gi)-len(gi_neut)) * len(lexicon))))\n",
    "    for word in gi_neut:\n",
    "        lexicon[word] = 0\n",
    "    positive_seeds, negative_seeds = seeds.finance_seeds()\n",
    "    stock_embed = create_representation(\"SVD\", constants.STOCK_EMBEDDINGS)\n",
    "    stock_counts = create_representation(\"Explicit\", constants.STOCK_COUNTS)\n",
    "    common_embed = create_representation(\"GIGA\", constants.COMMON_EMBEDDINGS, set(lexicon.keys()).union(positive_seeds).union(negative_seeds))\n",
    "\n",
    "    stock_words = set(stock_embed.iw)\n",
    "    common_words = set(common_embed)\n",
    "    eval_words = [word for word in lexicon if word in stock_words and\n",
    "            word in common_words and\n",
    "            not word in positive_seeds and  \n",
    "            not word in negative_seeds]\n",
    "\n",
    "    stock_counts = stock_counts.get_subembed(set(eval_words).union(positive_seeds).union(negative_seeds), restrict_context=False)\n",
    "\n",
    "    print (\"Evaluating with \", len(eval_words), \"out of\", len(lexicon))\n",
    "\n",
    "    print (\"Velikovich with 1990s Fic embeddings\")\n",
    "    stock_counts.normalize()\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        stock_counts,\n",
    "                        method=polarity_induction_methods.bootstrap, \n",
    "                        score_method=polarity_induction_methods.graph_propagate,\n",
    "                        T=3,\n",
    "                        boot_size=6,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=None)\n",
    "\n",
    "\n",
    "    print (\"PMI\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds,\n",
    "            stock_counts,\n",
    "            method=polarity_induction_methods.bootstrap, \n",
    "            score_method=polarity_induction_methods.pmi,\n",
    "            **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words)\n",
    "    print\n",
    "\n",
    "    print( \"SentProp with stock embeddings\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        stock_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "                        method=polarity_induction_methods.bootstrap, \n",
    "                        beta=0.9, nn=25,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words)\n",
    "\n",
    "    print (\"Densifier with stock embeddings\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        stock_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "                        method=polarity_induction_methods.bootstrap, \n",
    "                        score_method=polarity_induction_methods.densify, \n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words)\n",
    "\n",
    "\n",
    "def evaluate_twitter_methods():\n",
    "    np.random.seed(0)\n",
    "\n",
    "    print (\"Getting evalution words and embeddings..\")\n",
    "    gi = lexicons.load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "    lexicon = lexicons.load_lexicon(\"twitter\", remove_neutral=True)\n",
    "    scores = lexicons.load_lexicon(\"twitter-scores\", remove_neutral=True)\n",
    "    sent140 = lexicons.load_lexicon(\"140-scores\", remove_neutral=False)\n",
    "\n",
    "    # padding lexicon with neutral from GI\n",
    "    gi_neut = [word for word in gi if gi[word] == 0]\n",
    "    gi_neut = np.random.choice(gi_neut, int( (float(len(gi_neut))/(len(gi)-len(gi_neut)) * len(lexicon))))\n",
    "    for word in gi_neut:\n",
    "        lexicon[word] = 0\n",
    "\n",
    "    positive_seeds, negative_seeds = seeds.twitter_seeds()\n",
    "    embed = create_representation(\"GIGA\", constants.TWITTER_EMBEDDINGS, set(lexicon.keys()).union(positive_seeds).union(negative_seeds))\n",
    "    print len((set(positive_seeds).union(negative_seeds)).intersection(embed.iw))\n",
    "    embed_words = set(embed.iw)\n",
    "    s140_words = set(sent140.keys())\n",
    "    eval_words = [word for word in lexicon if word in s140_words and\n",
    "            not word in positive_seeds \n",
    "            and not word in negative_seeds\n",
    "            and word in embed_words] \n",
    "\n",
    "    print (\"Evaluating with \", len(eval_words), \"out of\", len(lexicon))\n",
    "\n",
    "    print( \"Sentiment 140\")\n",
    "    evaluate(sent140, lexicon, eval_words, tau_lexicon=scores)\n",
    "    print\n",
    "\n",
    "    print( \"SentProp\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        embed,\n",
    "                        method=polarity_induction_methods.bootstrap, \n",
    "                        score_method=polarity_induction_methods.densify,\n",
    "                        lr=0.01, regularization_strength=0.5,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    util.write_pickle(polarities, \"twitter-test.pkl\")\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=scores)\n",
    "\n",
    "    print (\"SentProp\")\n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "                        embed,\n",
    "                        method=polarity_induction_methods.bootstrap, \n",
    "                        score_method=polarity_induction_methods.random_walk,\n",
    "                        beta=0.9, nn=25,\n",
    "                        **DEFAULT_ARGUMENTS)\n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=scores)\n",
    "\n",
    "\n",
    "def run_method(positive_seeds, negative_seeds, embeddings, transform_embeddings=False, post_densify=False,\n",
    "        method=polarity_induction_methods.densify, **kwargs):\n",
    "    if transform_embeddings:\n",
    "        print (\"Transforming embeddings...\")\n",
    "        embeddings = embedding_transformer.apply_embedding_transformation(embeddings, positive_seeds, negative_seeds, n_dim=50)\n",
    "    if post_densify:\n",
    "        polarities = method(embeddings, positive_seeds, negative_seeds, **kwargs)\n",
    "        top_pos = [word for word in \n",
    "                sorted(polarities, key = lambda w : -polarities[w])[:150]]\n",
    "        top_neg = [word for word in \n",
    "                sorted(polarities, key = lambda w : polarities[w])[:150]]\n",
    "        top_pos.extend(positive_seeds)\n",
    "        top_neg.extend(negative_seeds)\n",
    "        return polarity_induction_methods.densify(embeddings, top_pos, top_neg)\n",
    "    positive_seeds = [s for s in positive_seeds if s in embeddings]\n",
    "    negative_seeds = [s for s in negative_seeds if s in embeddings]\n",
    "    return method(embeddings, positive_seeds, negative_seeds, **kwargs)\n",
    "\n",
    "\n",
    "def print_polarities(polarities, lexicon):\n",
    "    for w, p in sorted(polarities.items(), key=itemgetter(1), reverse=True):\n",
    "        print (util.GREEN if lexicon[w] == 1 else util.RED) + \\\n",
    "              \"{:}: {:0.5f}\".format(w, p) + util.ENDC\n",
    "\n",
    "def evaluate(polarities, lexicon, eval_words, tau_lexicon=None, tern=True):\n",
    "    acc, auc, avg_prec = binary_metrics(polarities, lexicon, eval_words)\n",
    "    if auc < 0.5:\n",
    "        polarities = {word:-1*polarities[word] for word in polarities}\n",
    "        acc, auc, avg_prec = binary_metrics(polarities, lexicon, eval_words)\n",
    "    print (\"Binary metrics:\")\n",
    "    print( \"==============\")\n",
    "    print (\"Accuracy with optimal threshold: {:.4f}\".format(acc))\n",
    "    print (\"ROC AUC Score: {:.4f}\".format(auc))\n",
    "    print (\"Average Precision Score: {:.4f}\".format(avg_prec))\n",
    "    \n",
    "    if not tern:\n",
    "        return \n",
    "    tau, cmn_f1, maj_f1, conf_mat = ternary_metrics(polarities, lexicon, eval_words, tau_lexicon=tau_lexicon)\n",
    "    print (\"Ternary metrics:\")\n",
    "    print( \"==============\")\n",
    "    print (\"Majority macro F1 baseline {:.4f}\".format(maj_f1))\n",
    "    print (\"Macro F1 with cmn threshold: {:.4f}\".format(cmn_f1))\n",
    "    if tau:\n",
    "        print (\"Kendall Tau {:.4f}\".format(tau))\n",
    "    print (\"Confusion matrix: \")\n",
    "    print conf_mat\n",
    "    print( \"Neg :\", float(conf_mat[0,0]) / np.sum(conf_mat[0,:]))\n",
    "    print (\"Neut :\", float(conf_mat[1,1]) / np.sum(conf_mat[1,:]))\n",
    "    print (\"Pos :\", float(conf_mat[2,2]) / np.sum(conf_mat[2,:]))\n",
    "    print\n",
    "    if tau:\n",
    "        print( \"Latex table line: {:2.1f} & {:2.1f} & {:.2f}\\\\\\\\\".format(100*auc, 100*cmn_f1, tau))\n",
    "    else:\n",
    "        print (\"Latex table line: {:2.1f} & {:2.1f}\\\\\\\\\".format(100*auc, 100*cmn_f1))\n",
    "\n",
    "\n",
    "def binary_metrics(polarities, lexicon, eval_words, print_predictions=False, top_perc=None):\n",
    "    eval_words = [word for word in eval_words if lexicon[word] != 0]\n",
    "    y_prob, y_true = [], []\n",
    "    if top_perc:\n",
    "        polarities = {word:polarities[word] for word in \n",
    "                sorted(eval_words, key = lambda w : abs(polarities[w]-0.5), reverse=True)[:int(top_perc*len(polarities))]}\n",
    "    else:\n",
    "        polarities = {word:polarities[word] for word in eval_words}\n",
    "    for w in polarities:\n",
    "        y_prob.append(polarities[w])\n",
    "        y_true.append(1 + lexicon[w] / 2)\n",
    "\n",
    "    n = len(y_true)\n",
    "    ordered_labels = [y_true[i] for i in sorted(range(n), key=lambda i: y_prob[i])]\n",
    "    positive = sum(ordered_labels)\n",
    "    cumsum = np.cumsum(ordered_labels)\n",
    "    best_accuracy = max([(1 + i - cumsum[i] + positive - cumsum[i]) / float(n) for i in range(n)])\n",
    "\n",
    "    return best_accuracy, roc_auc_score(y_true, y_prob), average_precision_score(y_true, y_prob)\n",
    "\n",
    "def ternary_metrics(polarities, lexicon, eval_words, tau_lexicon=None):\n",
    "    if not tau_lexicon == None:\n",
    "        kendall_words = list(set(eval_words).intersection(tau_lexicon))\n",
    "    y_prob, y_true = [], []\n",
    "    polarities = {word:polarities[word] for word in eval_words}\n",
    "    for w in polarities:\n",
    "        y_prob.append(polarities[w])\n",
    "        y_true.append(lexicon[w])\n",
    "    y_prob = np.array(y_prob)\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = 2*(y_prob - np.min(y_prob)) / (np.max(y_prob) - np.min(y_prob)) - 1\n",
    "    neg_prop = np.sum(np.array(lexicon.values()) == -1) / float(len(lexicon))\n",
    "    pos_prop = np.sum(np.array(lexicon.values()) == 1) / float(len(lexicon))\n",
    "    sorted_probs = sorted(y_prob)\n",
    "    neg_thresh = sorted_probs[int(np.round(neg_prop*len(sorted_probs)))]\n",
    "    pos_thresh = sorted_probs[-int(np.round(pos_prop*len(sorted_probs)))]\n",
    "    cmn_labels = [1 if val >= pos_thresh else -1 if val <= neg_thresh else 0 for val in y_prob]\n",
    "    if not tau_lexicon == None:\n",
    "        tau = kendalltau(*zip(*[(polarities[word], tau_lexicon[word]) for word in kendall_words]))[0]\n",
    "    else:\n",
    "        tau = None\n",
    "    maj_f1 = f1_score(y_true, np.repeat(sp.stats.mode(y_true)[0][0], len(y_true)), average=\"macro\")\n",
    "    cmn_f1 = f1_score(y_true, cmn_labels, average=\"macro\")\n",
    "    label_func = lambda entry : 1 if entry > pos_thresh else -1 if entry < neg_thresh else 0\n",
    "    conf_mat = confusion_matrix(y_true, [label_func(entry) for entry in y_prob])\n",
    "    return tau, cmn_f1, maj_f1, conf_mat\n",
    "\n",
    "def optimal_tern_acc(polarities, lexicon, eval_words, threshes=np.arange(0.95, 0.0, -0.01)):\n",
    "    \"\"\"\n",
    "    Performs grid search to determine optimal ternary accuracy.\n",
    "    \"\"\"\n",
    "    y_prob, y_true = [], []\n",
    "    polarities = {word:polarities[word] for word in eval_words}\n",
    "    for w in polarities:\n",
    "        y_prob.append(polarities[w])\n",
    "        y_true.append(lexicon[w])\n",
    "    y_prob = np.array(y_prob)\n",
    "    y_true = np.array(y_true)\n",
    "    y_prob = 2*(y_prob - np.min(y_prob)) / (np.max(y_prob) - np.min(y_prob)) - 1\n",
    "    f1s = np.zeros((len(threshes)**2,))\n",
    "    for i, pos_thresh in enumerate(threshes):\n",
    "        for k, neg_thresh in enumerate(threshes):\n",
    "            labels = []\n",
    "            for j in range(len(y_prob)):\n",
    "                if y_prob[j] > pos_thresh:\n",
    "                    labels.append(1)\n",
    "                elif y_prob[j] < -1*neg_thresh:\n",
    "                    labels.append(-1)\n",
    "                else:\n",
    "                    labels.append(0)\n",
    "            f1s[i*len(threshes)+k] = f1_score(y_true, labels, average=\"macro\")\n",
    "    print (\"(Oracle) majority baseline {:.4f}\".format(\n",
    "            f1_score(y_true, np.repeat(sp.stats.mode(y_true)[0][0], len(y_true)), average=\"macro\")))\n",
    "    print (\"Accuracy with optimal threshold: {:.4f}\".format(np.max(f1s)))\n",
    "    best_iter = int(np.argmax(f1s))\n",
    "    pos_thresh = threshes[best_iter / len(threshes)]\n",
    "    neg_thresh = -1*threshes[best_iter % len(threshes)]\n",
    "    print (\"Optimal positive threshold: {:.4f}\".format(pos_thresh))\n",
    "    print (\"Optimal negative threshold: {:.4f}\".format(neg_thresh))\n",
    "    print( \"Confusion matrix: \")\n",
    "    label_func = lambda entry : 1 if entry > pos_thresh else -1 if entry < neg_thresh else 0\n",
    "    conf_mat = confusion_matrix(y_true, [label_func(entry) for entry in y_prob])\n",
    "    print( conf_mat)\n",
    "    print( \"Neg :\", float(conf_mat[0,0]) / np.sum(conf_mat[0,:]))\n",
    "    print (\"Neut :\", float(conf_mat[1,1]) / np.sum(conf_mat[1,:]))\n",
    "    print (\"Pos :\", float(conf_mat[2,2]) / np.sum(conf_mat[2,:]))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    random.seed(0)\n",
    "    if sys.argv[1] == \"twitter\":\n",
    "        evaluate_twitter_methods()\n",
    "    elif sys.argv[1] == \"finance\":\n",
    "        evaluate_finance_methods()\n",
    "    elif sys.argv[1] == \"overlap\":\n",
    "        evaluate_overlap_methods()\n",
    "    elif sys.argv[1] == \"adj\":\n",
    "        evaluate_adj_methods()\n",
    "    elif sys.argv[1] == \"hyper\":\n",
    "        hyperparam_eval()\n",
    "    else:\n",
    "        evaluate_methods()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
