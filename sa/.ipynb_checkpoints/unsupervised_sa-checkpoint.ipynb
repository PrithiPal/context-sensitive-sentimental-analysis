{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from normalization import normalize_accented_characters, html_parser, strip_html\n",
    "from utils import display_evaluation_metrics, display_confusion_matrix, display_classification_report\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sentiwordnet')\n",
    "import dynet as dy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_movie_dataset(train_start,train_end,test_start,test_end) : \n",
    "\n",
    "    dataset = pd.read_csv(r'datasets/movie_reviews.csv')\n",
    "    print('dataset size : ',dataset.shape[0])\n",
    "\n",
    "    train_data = dataset[train_start:train_end]\n",
    "    test_data = dataset[test_start:test_end]\n",
    "    \n",
    "    print('Train_X : ',train_data.shape[0])\n",
    "    print('Test_X  : ',test_data.shape[0])\n",
    "\n",
    "    test_reviews = np.array(test_data['review'])\n",
    "    test_sentiments = np.array(test_data['sentiment'])\n",
    "\n",
    "    return train_data,test_reviews,test_sentiments\n",
    "\n",
    "def prepare_labeled_data(train_start,train_end,test_start,test_end) : \n",
    "    \n",
    "    labeled_data=open(\"datasets/labeledTrainData.tsv\",\"r\")\n",
    "    data=labeled_data.readlines()\n",
    "    data=[d.split(\"\\t\") for d in data]\n",
    "    sa_data=pd.DataFrame(data,columns=['ind','sentiment','review'])\n",
    "    sa_data=sa_data[['sentiment','review']]\n",
    "    \n",
    "    print('dataset size : ',sa_data.shape[0])\n",
    "\n",
    "    train_data = sa_data[train_start:train_end]\n",
    "    test_data = sa_data[test_start:test_end]\n",
    "    \n",
    "    print('Train_X : ',train_data.shape[0])\n",
    "    print('Test_X  : ',test_data.shape[0])\n",
    "\n",
    "    test_reviews = np.array(test_data['review'])\n",
    "    test_sentiments = np.array(test_data['sentiment'])\n",
    "\n",
    "    return train_data,test_reviews,test_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  50000\n",
      "Train_X :  999\n",
      "Test_X  :  1000\n"
     ]
    }
   ],
   "source": [
    "train_x,test_x,test_y=prepare_movie_dataset(1,1000,1000,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for unsupervised Lexicon sentiment tagging\n",
    "\n",
    "#### compare against the sentence tagging (already provided in the dataset )\n",
    "\n",
    "[add markdown #11 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BORROWED FROM THE AR_SARKAR METRIC\n",
    "def analyze_sentiment_sentiwordnet_lexicon(review,verbose=False):\n",
    "    \n",
    "    \n",
    "    #review = normalize_accented_characters(review)\n",
    "    #review = review.decode('utf-8')\n",
    "    review = html_parser.unescape(review)\n",
    "    review = strip_html(review)\n",
    "    \n",
    "    text_tokens = nltk.word_tokenize(review)\n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and swn.senti_synsets(word, 'n'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'VB' in tag and swn.senti_synsets(word, 'v'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'JJ' in tag and swn.senti_synsets(word, 'a'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'RB' in tag and swn.senti_synsets(word, 'r'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        \n",
    "        if ss_set:\n",
    "            \n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    \n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        \n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score,\n",
    "                                         norm_pos_score, norm_neg_score,\n",
    "                                         norm_final_score]],\n",
    "                                         columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment', 'Objectivity',\n",
    "                                                                       'Positive', 'Negative', 'Overall']], \n",
    "                                                              labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print (sentiment_frame)   \n",
    "    return final_sentiment\n",
    "            \n",
    "                                                               \n",
    "def evaluate_lexicons(TRUE_LABELS,PREDICTED_LABELS,POS_CLASS,NEG_CLASS) : \n",
    "\n",
    "    print ('Performance metrics:')\n",
    "    display_evaluation_metrics(true_labels=TRUE_LABELS,\n",
    "                               predicted_labels=PREDICTED_LABELS,\n",
    "                               positive_class=str(POS_CLASS))  \n",
    "    print ('\\nConfusion Matrix:'             )              \n",
    "    display_confusion_matrix(true_labels=TRUE_LABELS,\n",
    "                             predicted_labels=PREDICTED_LABELS,\n",
    "                             classes=[str(POS_CLASS),str(NEG_CLASS)])\n",
    "    print ('\\nClassification report:' )                        \n",
    "    display_classification_report(true_labels=TRUE_LABELS,\n",
    "                                  predicted_labels=PREDICTED_LABELS,\n",
    "                                  classes=[str(POS_CLASS),str(NEG_CLASS)])\n",
    "    return\n",
    "\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basline lexicon evaluation\n",
    "\n",
    "#### movie dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  50000\n",
      "Train_X :  1000\n",
      "Test_X  :  1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-139cf7a8ba38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_movie_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentiwordnet_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_sentiwordnet_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mevaluate_lexicons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiwordnet_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-139cf7a8ba38>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_movie_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentiwordnet_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_sentiwordnet_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mevaluate_lexicons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiwordnet_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-6512750e98c6>\u001b[0m in \u001b[0;36manalyze_sentiment_sentiwordnet_lexicon\u001b[0;34m(review, verbose)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtagged_text\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mss_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m'NN'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtag\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mswn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenti_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mss_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mswn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenti_synsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mss_set\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/nltk/corpus/reader/sentiwordnet.py\u001b[0m in \u001b[0;36msenti_synsets\u001b[0;34m(self, string, pos)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0msynset_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msynset_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0msentis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msenti_synset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0msentis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msentis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/nltk/corpus/reader/sentiwordnet.py\u001b[0m in \u001b[0;36msenti_synset\u001b[0;34m(self, *vals)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mSentiSynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0msynset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m's'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/nltk/corpus/reader/wordnet.py\u001b[0m in \u001b[0;36msynset\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1293\u001b[0m         \u001b[0;31m# get the offset for this synset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lemma_pos_offset_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msynset_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'no lemma %r with part of speech %r'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x,test_x,test_y=prepare_movie_dataset(0,1000,1000,2000)\n",
    "sentiwordnet_predictions = [analyze_sentiment_sentiwordnet_lexicon(review) for review in test_x]\n",
    "evaluate_lexicons(test_y.tolist(),sentiwordnet_predictions,'positive','negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labeled dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,test_y=prepare_labeled_data(0,1000,1000,2000)\n",
    "sentiwordnet_predictions = [analyze_sentiment_sentiwordnet_lexicon(review) for review in test_x]\n",
    "binary_predicted=['1' if p=='positive' else '0' for p in sentiwordnet_predictions ]\n",
    "evaluate_lexicons(test_y.tolist(),binary_predicted,'1','0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple network for learning (do afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss before step is: 1.0331029891967773\n",
      "the loss after step is: 0.7983773350715637\n"
     ]
    }
   ],
   "source": [
    "## SIMPLE NETWORK WITH THE sigma(V*tanh(WX+B)) ## for the XOR problem\n",
    "# create a parameter collection and add the parameters.\n",
    "m = dy.ParameterCollection()\n",
    "W = m.add_parameters((8,2))\n",
    "V = m.add_parameters((1,8))\n",
    "b = m.add_parameters((8))\n",
    "\n",
    "dy.renew_cg() # new computation graph. not strictly needed here, but good practice.\n",
    "b.value() ## bias values\n",
    "x=dy.vecInput(2) ## 2 sized inputs \n",
    "output=dy.logistic(V*(dy.tanh(W*x)+b)) ## output node\n",
    "\n",
    "y = dy.scalarInput(0) ## objective function\n",
    "loss = dy.binary_log_loss(output,y) ## loss function\n",
    "## trainer with the initialized parameters m \n",
    "trainer=dy.SimpleSGDTrainer(m)\n",
    "x.set([1,0])\n",
    "y.set(1)\n",
    "loss_value = loss.value() # this performs a forward through the network.\n",
    "print(\"the loss before step is:\",loss_value)\n",
    "\n",
    "loss.backward()  # compute the gradients\n",
    "trainer.update()\n",
    "\n",
    "loss_value = loss.value(recalculate=True) \n",
    "print(\"the loss after step is:\",loss_value)\n",
    "pc = dy.ParameterCollection()\n",
    "NUM_LAYERS=2\n",
    "INPUT_DIM=50\n",
    "HIDDEN_DIM=10\n",
    "builder = dy.LSTMBuilder(NUM_LAYERS, INPUT_DIM, HIDDEN_DIM, pc)\n",
    "s0 = builder.initial_state()\n",
    "x1 = dy.vecInput(INPUT_DIM)\n",
    "s1=s0.add_input(x1)\n",
    "y1 = s1.output()\n",
    "s2=s1.add_input(x1) # we can add another input\n",
    "y2=s2.output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING 2.1 SECTION OF GOOGLE PAPER FOR LEXICON EXPANSION [this is for the lexicon expansion ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORDS=list(wn.words())\n",
    "SCORES=defaultdict()\n",
    "df=pd.DataFrame(ALL_WORDS,columns=['word'])\n",
    "\n",
    "swn.all_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the lexicons from stanford paper \"Incuding Domain-Specific Sentiment Lexicons from Unalabeled Copora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socialsent_util\n",
    "def load_lexicon(name, remove_neutral=True):\n",
    "    lexicon = socialsent_util.load_json(\"./lexicons_socialsent/\"+ name + '.json')\n",
    "    return {w: p for w, p in lexicon.items() if p != 0} if remove_neutral else lexicon\n",
    "\n",
    "def compare_lexicons(print_disagreements=False):\n",
    "    lexicons = {\n",
    "        \"inquirer\": load_lexicon(\"inquirer\", False),\n",
    "        \"mpqa\": load_lexicon(\"mpqa\", False),\n",
    "        \"bingliu\": load_lexicon(\"bingliu\", False),\n",
    "    }\n",
    "\n",
    "    for l in lexicons:\n",
    "        print( l, len(lexicons[l]), len([w for w in lexicons[l] if lexicons[l][w] != 0]))\n",
    "\n",
    "    for l1, l2 in itertools.combinations(lexicons.keys(), 2):\n",
    "        ps1, ps2 = lexicons[l1], lexicons[l2]\n",
    "        common_words = set(ps1.keys()) & set(ps2.keys())\n",
    "        print( l1, l2, \"agreement: {:.2f}\".format(\n",
    "            100.0 * sum(1 if ps1[w] == ps2[w] else 0 for w in common_words) / len(common_words)))\n",
    "        common_words = set([word for word in ps1.keys() if ps1[word] != 0]) & \\\n",
    "                       set([word for word in ps2.keys() if ps2[word] != 0])  \n",
    "        print (l1, l2, \"agreement ignoring neutral: {:.2f}\".format(\n",
    "            100.0 * sum(1 if ps1[w] * ps2[w] == 1 else 0 for w in common_words) / len(common_words)))\n",
    "        \n",
    "        if print_disagreements and l1 == 'opinion' and l2 == 'inquirer':\n",
    "            for w in common_words:\n",
    "                if lexicons[l1][w] != lexicons[l2][w]:\n",
    "                    print (w, lexicons[l1][w], lexicons[l2][w])\n",
    "      \n",
    "    \n",
    "## ALL THESE LEXICONS ARE 2-CLASS SENTIMENTS. 1 = POSITIVE; -1 = NEGATIVE\n",
    "finance_lexicons=load_lexicon('finance')\n",
    "bingliu_lexicons=load_lexicon('bingliu')\n",
    "inquirer_lexicons=load_lexicon('inquirer')\n",
    "mpqa_lexicons=load_lexicon('mpqa')\n",
    "twitter_lexicons=load_lexicon('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZEEYANG_LEXICONS='lexicons_zeeyang'\n",
    "def read_zeeyang_lexicons(fname) : \n",
    "    \n",
    "    polarities=defaultdict()\n",
    "    for line in open(fname,'r') : \n",
    "        token=line.split(\" \")[0]\n",
    "        score=line.split(\" \")[1]\n",
    "        polarities[token]=score\n",
    "        \n",
    "    return polarities\n",
    "\n",
    "## THESE LEXICONS HAVE CONTINOUS SCORES (BETWEEN -1 AND 1 )\n",
    "senti140_lexicons=read_zeeyang_lexicons(ZEEYANG_LEXICONS+\"/sentiment140.lex\")\n",
    "sentiwn_lexicons=read_zeeyang_lexicons(ZEEYANG_LEXICONS+\"/sentiwordnet.lex\")\n",
    "sst_lexicons=read_zeeyang_lexicons(ZEEYANG_LEXICONS+\"/stanford.tree.lexicon\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compares the different lexicon repositories through the mutual information between them (common words)\n",
    "\n",
    "### The comparison is done through looking for words in two lexicon dictionaries L1 AND L2, and how many words are common in them which have the same scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inquirer 8640 3457\n",
      "mpqa 6886 6462\n",
      "bingliu 6785 6785\n",
      "inquirer mpqa agreement: 82.47\n",
      "inquirer mpqa agreement ignoring neutral: 98.50\n",
      "inquirer bingliu agreement: 84.39\n",
      "inquirer bingliu agreement ignoring neutral: 98.74\n",
      "mpqa bingliu agreement: 99.19\n",
      "mpqa bingliu agreement ignoring neutral: 99.44\n"
     ]
    }
   ],
   "source": [
    "## COMPARING THE BINARY LEXICONS \n",
    "compare_lexicons()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Induction : the idea is to generate the lexicons provided the corpus. This method makes sure that the lexicon are sensitive to the context they are drawn from. They may prove useful if we would like to assess them in a simiar context. For instance, financial lexicons will reflect better sentiments than using general lexicons such as SentiWordNet. Three ways purposed for induction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLARITY INDUCTION METHOD : This is used for re-scoring of the lexicons(tokens) by taking information from the word-embeddings (domain-specific), positive and the negative seed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data =  /home/ubuntu/workspace/nlpclass-1187-g-Mad_Titans/sa/embeddings_socialsent/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import polarity_induction_methods\n",
    "\n",
    "### THIS IS THE FUNCTION FOR INDUCING LEXICONS GIVEN THE SEEDS, EMBEDDINGS AND THE METHOD.\n",
    "def run_method(positive_seeds, negative_seeds, embeddings, transform_embeddings=False, post_densify=False,\n",
    "        method=polarity_induction_methods.densify, **kwargs):\n",
    "    \n",
    "    print(\"THE INTERNAL RUN_METHOD IS RUNNING...\")\n",
    "    \n",
    "    if transform_embeddings:\n",
    "        print (\"Transforming embeddings...\")\n",
    "        embeddings = embedding_transformer.apply_embedding_transformation(embeddings, positive_seeds, negative_seeds, n_dim=50)\n",
    "    \n",
    "    print(\"AFTER EMBEDDING TRANSFORM \",embeddings)\n",
    "    \n",
    "    ## using densify method\n",
    "    if post_densify:\n",
    "        polarities = method(embeddings, positive_seeds, negative_seeds, **kwargs)\n",
    "        top_pos = [word for word in \n",
    "                sorted(polarities, key = lambda w : -polarities[w])[:150]]\n",
    "        top_neg = [word for word in \n",
    "                sorted(polarities, key = lambda w : polarities[w])[:150]]\n",
    "        top_pos.extend(positive_seeds)\n",
    "        top_neg.extend(negative_seeds)\n",
    "        return polarity_induction_methods.densify(embeddings, top_pos, top_neg)\n",
    "    \n",
    "    \n",
    "    positive_seeds = [s for s in positive_seeds if s in embeddings]\n",
    "    negative_seeds = [s for s in negative_seeds if s in embeddings]\n",
    "    \n",
    "    \n",
    "    return method(embeddings, positive_seeds, negative_seeds, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEXICON INDUCTION ON STANDARD ENGLISH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_lexicon_polarities(parent_lexicon,positive_seeds,negative_seeds,technique) : \n",
    "    \n",
    "    ## CHOOSE SEEDS : \n",
    "\n",
    "    POSITIVE_SEEDS = [\"good\", \"lovely\", \"excellent\", \"fortunate\", \"pleasant\", \"delightful\", \"perfect\", \"loved\", \"love\", \"happy\"] \n",
    "    NEGATIVE_SEEDS = [\"bad\", \"horrible\", \"poor\",  \"unfortunate\", \"unpleasant\", \"disgusting\", \"evil\", \"hated\", \"hate\", \"unhappy\"]\n",
    "\n",
    "    ## LOAD THE WORD-EMBEDDINGS : \n",
    "\n",
    "    eval_words = set(parent_lexicon.keys())\n",
    "\n",
    "    EMBEDDING_TYPE = constants.GLOVE_EMBEDDINGS\n",
    "    EMBEDDING = create_representation(\"GIGA\", constants.GLOVE_EMBEDDINGS,eval_words.union(POSITIVE_SEEDS).union(NEGATIVE_SEEDS))\n",
    "\n",
    "    embed_words = set(EMBEDDING.iw)\n",
    "    eval_words = eval_words.intersection(EMBEDDING)\n",
    "    eval_words = [word for word in eval_words  if not word in POSITIVE_SEEDS and not word in NEGATIVE_SEEDS]\n",
    "\n",
    "    ## TRAIN THE BEST ALGORITHM : SENTPROP and get polarities re-scored\n",
    "    \n",
    "    polarities=defaultdict()\n",
    "    if technique=='label_propagate_prob' : \n",
    "        \n",
    "        polarities = run_method(POSITIVE_SEEDS, NEGATIVE_SEEDS, \n",
    "                    EMBEDDING.get_subembed(set(eval_words).union(NEGATIVE_SEEDS).union(POSITIVE_SEEDS)),\n",
    "                    method=polarity_induction_methods.label_propagate_probabilistic,beta=0.99, nn=10)\n",
    "    \n",
    "    elif technique == 'pmi' : \n",
    "        \n",
    "        EMBEDDING = create_representation(\"Explicit\", constants.GLOVE_EMBEDDINGS)\n",
    "        hist_counts = EMBEDDING.get_subembed(set(eval_words).union(positive_seeds).union(negative_seeds),restrict_context=False)\n",
    "        \n",
    "        \n",
    "        print(dir(EMBEDDING))\n",
    "        \n",
    "        polarities = run_method(positive_seeds, negative_seeds,\n",
    "                hist_counts,\n",
    "                method=polarity_induction_methods.bootstrap,\n",
    "                score_method=polarity_induction_methods.pmi)\n",
    "    \n",
    "    return polarities,eval_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from representations.representation_factory import create_representation\n",
    "import constants\n",
    "from evaluate_methods import run_method\n",
    "import polarity_induction_methods\n",
    "\n",
    "\n",
    "## TRAINING THE LABEL-PROPAGATION FOR THE RE-SCORING OF POLARITIES FROM PRE-DETERMINED LEXICONS (MADE FROM WORD EMBEDDINGS)\n",
    "\n",
    "INQUIRER = load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "\n",
    "FINANCE_LEXICONS=load_lexicon('finance')\n",
    "TWITTER_LEXICONS=load_lexicon('twitter')\n",
    "\n",
    "THREE_WAY_LEXICON = kuperman = load_lexicon(\"kuperman\", remove_neutral=False)\n",
    "\n",
    "POSITIVE_FINANCE = [\"successful\", \"excellent\", \"profit\", \"beneficial\", \"improving\", \"improved\", \"success\", \"gains\", \"positive\"]\n",
    "NEGATIVE_FINANCE = [\"negligent\", \"loss\", \"volatile\", \"wrong\", \"losses\", \"damages\", \"bad\", \"litigation\", \"failure\", \"down\", \"negative\"]\n",
    "\n",
    "\n",
    "POSITIVE_SE = [\"good\", \"lovely\", \"excellent\", \"fortunate\", \"pleasant\", \"delightful\", \"perfect\", \"loved\", \"love\", \"happy\"] \n",
    "NEGATIVE_SE = [\"bad\", \"horrible\", \"poor\",  \"unfortunate\", \"unpleasant\", \"disgusting\", \"evil\", \"hated\", \"hate\", \"unhappy\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training financial and standard-english lexicons with glove embeddings using probabilistic label propagation method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_polarities,finance_eval=calculate_new_lexicon_polarities(FINANCE_LEXICONS,POSITIVE_FINANCE,NEGATIVE_FINANCE,'label_propagate_prob')\n",
    "standard_english_polarities,se_eval=calculate_new_lexicon_polarities(INQUIRER,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'senticalculate_new_lexicon_polarities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5f7377722a87>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msenti_polarities\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msenti_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msenticalculate_new_lexicon_polarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSENTI_LEXICONS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mPOSITIVE_SE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNEGATIVE_SE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'label_propagate_prob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'senticalculate_new_lexicon_polarities' is not defined"
     ]
    }
   ],
   "source": [
    "ss=swn.all_senti_synsets()\n",
    "SENTI_LEXICONS=defaultdict()\n",
    "\n",
    "for s in ss : \n",
    "    \n",
    "    lemmas = s.synset.lemma_names()\n",
    "    positive=s.pos_score()\n",
    "    \n",
    "    negative=s.neg_score()\n",
    "    \n",
    "    for l in lemmas : \n",
    "        net_sentiment=positive-negative\n",
    "        ## CONVERTING TO BINARY SENTIMENTS \n",
    "        if net_sentiment>0 : \n",
    "            SENTI_LEXICONS[l]=1\n",
    "        elif net_sentiment<0:\n",
    "            SENTI_LEXICONS[l]=-1\n",
    "            \n",
    "            \n",
    "\n",
    "senti_polarities,senti_eval=calculate_new_lexicon_polarities(SENTI_LEXICONS,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING THE EFFECTIVENESS OF THE NEW LEXICON POLARITIES.\n",
    "\n",
    "#### Calculates the ROC auc scores with the new polarities comparing to the earlier lexicon binary classification (1 = positive and 0 = negative).\n",
    "#### Interpretation of the score. \n",
    "#### Higher the score, it means that new polarities (continous sentiment scores) confirms with the binary sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASKS IN THE DOC : \n",
    "\n",
    "## 1. EXPLAIN ABOUT THE LEXICON INDUCING : LABEL PROPAGATION ALGORITHM\n",
    "## 2. GIVES SOME UNDERLYING MATHEMATICS FROM THE RESEARCH PAPER REGARDING HOW SCORES ARE COMPUTED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method_performance(polarities,INITIAL_LEXICON_LIB,domain,eval_words) : \n",
    "    \n",
    "    ## EVALUATING THE EFFECTIVENESS OF THE NEW LEXICON POLARITIES.\n",
    "    from evaluate_methods import binary_metrics,ternary_metrics\n",
    "\n",
    "    acc, auc, avg_prec = binary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words)\n",
    "    if auc < 0.5:\n",
    "        polarities = {word:-1*polarities[word] for word in polarities}\n",
    "        acc, auc, avg_prec = binary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words)\n",
    "\n",
    "    print(\"============== DOMAIN : {} ==============\".format(domain))\n",
    "    print (\"Binary metrics:\")\n",
    "    print( \"==============\")\n",
    "    print (\"Accuracy with optimal threshold: {:.4f}\".format(acc))\n",
    "    print (\"ROC AUC Score: {:.4f}\".format(auc))\n",
    "    print (\"Average Precision Score: {:.4f}\".format(avg_prec))\n",
    "\n",
    "\n",
    "    tau, cmn_f1, maj_f1, conf_mat = ternary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words, tau_lexicon=THREE_WAY_LEXICON)\n",
    "    print (\"Ternary metrics:\")\n",
    "    print( \"==============\")\n",
    "    print (\"Majority macro F1 baseline {:.4f}\".format(maj_f1))\n",
    "    print (\"Macro F1 with cmn threshold: {:.4f}\".format(cmn_f1))\n",
    "    if tau:\n",
    "        print (\"Kendall Tau {:.4f}\".format(tau))\n",
    "    print (\"Confusion matrix: \")\n",
    "    print (conf_mat)\n",
    "    print( \"Neg :\", float(conf_mat[0,0]) / np.sum(conf_mat[0,:]))\n",
    "    print (\"Neut :\", float(conf_mat[1,1]) / np.sum(conf_mat[1,:]))\n",
    "    print (\"Pos :\", float(conf_mat[2,2]) / np.sum(conf_mat[2,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== DOMAIN : FINANCE ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.7899\n",
      "ROC AUC Score: 0.9576\n",
      "Average Precision Score: 0.8367\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.4642\n",
      "Macro F1 with cmn threshold: 0.1180\n",
      "Kendall Tau 0.3746\n",
      "Confusion matrix: \n",
      "[[   0    1 2246]\n",
      " [   0    0    0]\n",
      " [   0    0  347]]\n",
      "Neg : 0.0\n",
      "Neut : nan\n",
      "Pos : 1.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_method_performance(finance_polarities,FINANCE_LEXICONS,'FINANCE',finance_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== DOMAIN :  STANDARD ENGLISH  ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.1485\n",
      "ROC AUC Score: 0.7891\n",
      "Average Precision Score: 0.7536\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.2497\n",
      "Macro F1 with cmn threshold: 0.1024\n",
      "Kendall Tau 0.3353\n",
      "Confusion matrix: \n",
      "[[   0    1 1874]\n",
      " [   0    0 5106]\n",
      " [   0    0 1547]]\n",
      "Neg : 0.0\n",
      "Neut : 0.0\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "evaluate_method_performance(standard_english_polarities,INQUIRER,' STANDARD ENGLISH ',se_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE : On the Movie Reviews using senti-wordnet lexicons\n",
    "\n",
    "-  Accuracy: 0.6\n",
    "-  Precision: 0.56\n",
    "-  Recall: 0.93\n",
    "-  F1 Score: 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is to compute the effectiveness of binary sentiment scores provided a lexicon library.\n",
    "### This can be used to see which lexicon libraries help achieving the closest sentiment scores.\n",
    "### Thus a supervised algorithm and evaluation is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS : \n",
    "## review = single sentence \n",
    "## lexicon_dict = dict of the lexicon with key as word and value as the polarity\n",
    "\n",
    "def analyze_sentiment_domain(review,lexicon_dict,verbose=False):\n",
    "    \n",
    "    \n",
    "    #review = normalize_accented_characters(review)\n",
    "    #review = review.decode('utf-8')\n",
    "    review = html_parser.unescape(review)\n",
    "    review = strip_html(review)\n",
    "    \n",
    "    text_tokens = nltk.word_tokenize(review)\n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "\n",
    "    ## postitve polarity counts as positive and negative polarities counts as negative\n",
    "    \n",
    "    \n",
    "    for token in text_tokens : \n",
    "        \n",
    "        if token in lexicon_dict : \n",
    "            \n",
    "            if lexicon_dict[token]>0 : \n",
    "                pos_score+=1\n",
    "            elif lexicon_dict[token]<0:\n",
    "                neg_score+=1\n",
    "\n",
    "        token_count+=1\n",
    "            \n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment,\n",
    "                                         norm_pos_score, norm_neg_score,\n",
    "                                         norm_final_score]],\n",
    "                                         columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment',\n",
    "                                                                       'Positive', 'Negative', 'Overall']], \n",
    "                                                              labels=[[0,0,0,0],[0,1,2,3]]))\n",
    "        print (sentiment_frame)   \n",
    "    return final_sentiment\n",
    "            \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  50000\n",
      "Train_X :  1000\n",
      "Test_X  :  1000\n"
     ]
    }
   ],
   "source": [
    "train_x,test_x,test_y=prepare_movie_dataset(0,1000,1000,2000)\n",
    "\n",
    "sentiwordnet_predictions = [analyze_sentiment_domain(review,polarities) for review in test_x]\n",
    "#evaluate_lexicons(test_y.tolist(),sentiwordnet_predictions,'positive','negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO : \n",
    "\n",
    "## 1. COMPUTE THESE SCORES AGAIN WITH THE NEW DOMAIN SPECIFIC (GLOVE BASED) EMBEDDINGS TO SEE SENTIMENTAL SCORE CHANGE.\n",
    "## 2. GET THE TWITTER DATASET  AND DO AGAIN THE SAME THING. (TWITTER EMBEDDINGS/GLOVE EMBEDDINGS/SENTIWORDNET + TWITTER DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR TWO DATASETS : \n",
    "### FOR THREE EMBEDDINGS : CUSTOM-MADE / GLOVE (1B) / ACTUAL-DOMAIN ONES.\n",
    "### GET THESE SCORES (2*3 MATRIX OF SCORES)\n",
    "### GET THE NEW POLARITIES AND USE THEM TO CALCULATE SENTIMENTAL SCORES(SENTENCE BASED).\n",
    "### ALSO GET THE BINARY_METRICS FOR THE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOW THE TIME TO THE EVLUATION ON THE SENTI-WORDNET. THE AGGREGATE POLARITY WILL BE POSITIVE-NEGATIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=swn.all_senti_synsets()\n",
    "SENTI_LEXICONS=defaultdict()\n",
    "\n",
    "for s in ss : \n",
    "    \n",
    "    lemmas = s.synset.lemma_names()\n",
    "    positive=s.pos_score()\n",
    "    \n",
    "    negative=s.neg_score()\n",
    "    \n",
    "    for l in lemmas : \n",
    "        net_sentiment=positive-negative\n",
    "        ## CONVERTING TO BINARY SENTIMENTS \n",
    "        if net_sentiment>0 : \n",
    "            SENTI_LEXICONS[l]=1\n",
    "        elif net_sentiment<0:\n",
    "            SENTI_LEXICONS[l]=-1\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None,\n",
       "            {'able': 1,\n",
       "             'unable': -1,\n",
       "             'dissilient': 1,\n",
       "             'parturient': 1,\n",
       "             'full-length': 1,\n",
       "             'uncut': -1,\n",
       "             'absolute': -1,\n",
       "             'direct': 1,\n",
       "             'infinite': -1,\n",
       "             'living': -1,\n",
       "             'relative': -1,\n",
       "             'comparative': -1,\n",
       "             'absorbefacient': 1,\n",
       "             'sorbefacient': 1,\n",
       "             'assimilating': -1,\n",
       "             'assimilative': -1,\n",
       "             'assimilatory': -1,\n",
       "             'receptive': 1,\n",
       "             'nonabsorbent': -1,\n",
       "             'nonabsorptive': -1,\n",
       "             'repellent': -1,\n",
       "             'resistant': -1,\n",
       "             'chemisorptive': -1,\n",
       "             'chemosorptive': -1,\n",
       "             'nonadsorbent': -1,\n",
       "             'nonadsorptive': -1,\n",
       "             'absorbable': 1,\n",
       "             'adsorbable': 1,\n",
       "             'adsorbate': 1,\n",
       "             'abstinent': -1,\n",
       "             'abstentious': -1,\n",
       "             'ascetic': 1,\n",
       "             'ascetical': 1,\n",
       "             'austere': -1,\n",
       "             'spartan': 1,\n",
       "             'crapulent': -1,\n",
       "             'crapulous': -1,\n",
       "             'hoggish': -1,\n",
       "             'piggish': -1,\n",
       "             'piggy': -1,\n",
       "             'porcine': 1,\n",
       "             'swinish': -1,\n",
       "             'overgreedy': -1,\n",
       "             'too-greedy': -1,\n",
       "             'conceptional': 1,\n",
       "             'ideational': 1,\n",
       "             'notional': -1,\n",
       "             'conceptual': 1,\n",
       "             'concrete': 1,\n",
       "             'abundant': -1,\n",
       "             'abounding': -1,\n",
       "             'galore': 1,\n",
       "             'ample': 1,\n",
       "             'copious': 1,\n",
       "             'plenteous': 1,\n",
       "             'plentiful': 1,\n",
       "             'rich': 1,\n",
       "             'easy': 1,\n",
       "             'exuberant': 1,\n",
       "             'lush': -1,\n",
       "             'luxuriant': 1,\n",
       "             'profuse': -1,\n",
       "             'riotous': -1,\n",
       "             'thick': -1,\n",
       "             'long': 1,\n",
       "             'overabundant': -1,\n",
       "             'plethoric': -1,\n",
       "             'rife': -1,\n",
       "             'rank': 1,\n",
       "             'superabundant': -1,\n",
       "             'torrential': 1,\n",
       "             'verdant': -1,\n",
       "             'scarce': 1,\n",
       "             'rare': -1,\n",
       "             'tight': 1,\n",
       "             'abused': 1,\n",
       "             'ill-treated': -1,\n",
       "             'maltreated': -1,\n",
       "             'mistreated': -1,\n",
       "             'battered': -1,\n",
       "             'unabused': 1,\n",
       "             'acceptable': 1,\n",
       "             'bankable': 1,\n",
       "             'unexceptionable': -1,\n",
       "             'unimpeachable': -1,\n",
       "             'unobjectionable': -1,\n",
       "             'unacceptable': 1,\n",
       "             'exceptionable': -1,\n",
       "             'objectionable': -1,\n",
       "             'accessible': 1,\n",
       "             'approachable': 1,\n",
       "             'reachable': 1,\n",
       "             'come-at-able': 1,\n",
       "             'get-at-able': 1,\n",
       "             'getatable': 1,\n",
       "             'inaccessible': -1,\n",
       "             'unaccessible': 1,\n",
       "             'outback': 1,\n",
       "             'remote': 1,\n",
       "             'pathless': -1,\n",
       "             'roadless': -1,\n",
       "             'trackless': -1,\n",
       "             'untracked': -1,\n",
       "             'untrod': -1,\n",
       "             'untrodden': -1,\n",
       "             'un-come-at-able': -1,\n",
       "             'un-get-at-able': -1,\n",
       "             'ungetatable': -1,\n",
       "             'accommodating': 1,\n",
       "             'accommodative': 1,\n",
       "             'complaisant': 1,\n",
       "             'obliging': 1,\n",
       "             'disobliging': -1,\n",
       "             'uncooperative': -1,\n",
       "             'accurate': 1,\n",
       "             'close': 1,\n",
       "             'faithful': 1,\n",
       "             'dead-on': 1,\n",
       "             'high-fidelity': 1,\n",
       "             'hi-fi': 1,\n",
       "             'surgical': 1,\n",
       "             'true': 1,\n",
       "             'dead_on_target': 1,\n",
       "             'veracious': 1,\n",
       "             'right': -1,\n",
       "             'inaccurate': -1,\n",
       "             'faulty': 1,\n",
       "             'incorrect': -1,\n",
       "             'wrong': -1,\n",
       "             'unfaithful': 1,\n",
       "             'wide': 1,\n",
       "             'wide_of_the_mark': -1,\n",
       "             'accustomed': 1,\n",
       "             'used_to': -1,\n",
       "             'wont_to': -1,\n",
       "             'unaccustomed': -1,\n",
       "             'new': 1,\n",
       "             'unused': 1,\n",
       "             'acidic': -1,\n",
       "             'acid': -1,\n",
       "             'acid-forming': -1,\n",
       "             'alkalescent': -1,\n",
       "             'alcalescent': -1,\n",
       "             'basic': 1,\n",
       "             'saltlike': 1,\n",
       "             'amphoteric': 1,\n",
       "             'amphiprotic': 1,\n",
       "             'acid-loving': -1,\n",
       "             'alkaline-loving': -1,\n",
       "             'acknowledged': 1,\n",
       "             'accepted': 1,\n",
       "             'recognized': 1,\n",
       "             'recognised': 1,\n",
       "             'self-confessed': -1,\n",
       "             'assumptive': -1,\n",
       "             'declarable': 1,\n",
       "             'putative': 1,\n",
       "             'unacknowledged': -1,\n",
       "             'unappreciated': -1,\n",
       "             'unsung': -1,\n",
       "             'unvalued': -1,\n",
       "             'unavowed': 1,\n",
       "             'secret': -1,\n",
       "             'unconfessed': -1,\n",
       "             'unrecognized': -1,\n",
       "             'unrecognised': -1,\n",
       "             'accumulative': 1,\n",
       "             'rapacious': -1,\n",
       "             'ravening': -1,\n",
       "             'voracious': -1,\n",
       "             'sordid': -1,\n",
       "             'unacquisitive': -1,\n",
       "             'active': 1,\n",
       "             'acrobatic': 1,\n",
       "             'athletic': 1,\n",
       "             'gymnastic': 1,\n",
       "             'hot': -1,\n",
       "             'hyperactive': 1,\n",
       "             'overactive': 1,\n",
       "             'on_the_go': 1,\n",
       "             'sporty': 1,\n",
       "             'inactive': -1,\n",
       "             'desk-bound': 1,\n",
       "             'deskbound': 1,\n",
       "             'abeyant': -1,\n",
       "             'dormant': -1,\n",
       "             'hypoactive': -1,\n",
       "             'underactive': -1,\n",
       "             'inert': 1,\n",
       "             'sluggish': -1,\n",
       "             'soggy': -1,\n",
       "             'torpid': 1,\n",
       "             'sedentary': 1,\n",
       "             'activated': -1,\n",
       "             'off': -1,\n",
       "             'retired': -1,\n",
       "             'brisk': 1,\n",
       "             'bustling': 1,\n",
       "             'busy': 1,\n",
       "             'going': -1,\n",
       "             'springy': -1,\n",
       "             'dead': 1,\n",
       "             'dull': -1,\n",
       "             'slow': -1,\n",
       "             'idle': -1,\n",
       "             'progressive': 1,\n",
       "             'latent': -1,\n",
       "             'quiescent': -1,\n",
       "             'passive': 1,\n",
       "             'hands-off': -1,\n",
       "             'resistless': -1,\n",
       "             'supine': -1,\n",
       "             'unresisting': -1,\n",
       "             'eruptive': -1,\n",
       "             'alive': 1,\n",
       "             'live': 1,\n",
       "             'counteractive': -1,\n",
       "             'quiet': -1,\n",
       "             'actual': 1,\n",
       "             'existent': 1,\n",
       "             'effective': 1,\n",
       "             'potential': -1,\n",
       "             'possible': 1,\n",
       "             'acute': 1,\n",
       "             'chronic': -1,\n",
       "             'degenerative': -1,\n",
       "             'virulent': -1,\n",
       "             'highly_infective': -1,\n",
       "             'deadly': -1,\n",
       "             'avirulent': -1,\n",
       "             'adaptive': 1,\n",
       "             'adaptative': 1,\n",
       "             'reconciling': 1,\n",
       "             'adjustive': 1,\n",
       "             'maladaptive': 1,\n",
       "             'maladjustive': 1,\n",
       "             'unaddicted': -1,\n",
       "             'clean': 1,\n",
       "             'addictive': -1,\n",
       "             'habit-forming': -1,\n",
       "             'nonaddictive': -1,\n",
       "             'additive': -1,\n",
       "             'addable': 1,\n",
       "             'addible': 1,\n",
       "             'subtractive': 1,\n",
       "             'ablative': 1,\n",
       "             'reductive': -1,\n",
       "             'addressed': 1,\n",
       "             'unaddressed': -1,\n",
       "             'adequate_to': 1,\n",
       "             'capable': -1,\n",
       "             'equal_to': 1,\n",
       "             'up_to': 1,\n",
       "             'competent': 1,\n",
       "             'inadequate': -1,\n",
       "             'unequal': -1,\n",
       "             'deficient': -1,\n",
       "             'lacking': -1,\n",
       "             'wanting': -1,\n",
       "             'incapable': -1,\n",
       "             'incompetent': -1,\n",
       "             'unequal_to': -1,\n",
       "             'short-handed': -1,\n",
       "             'short-staffed': -1,\n",
       "             'undermanned': -1,\n",
       "             'understaffed': -1,\n",
       "             'adhesive': -1,\n",
       "             'bondable': 1,\n",
       "             'cohesive': -1,\n",
       "             'gummed': -1,\n",
       "             'gummy': -1,\n",
       "             'self-sealing': 1,\n",
       "             'stick-on': -1,\n",
       "             'nonadhesive': -1,\n",
       "             'nonglutinous': -1,\n",
       "             'nonviscid': -1,\n",
       "             'nonresinous': -1,\n",
       "             'non-resinous': -1,\n",
       "             'nonresiny': -1,\n",
       "             'non-resiny': -1,\n",
       "             'ungummed': -1,\n",
       "             'substantive': 1,\n",
       "             'essential': -1,\n",
       "             'adoptable': 1,\n",
       "             'adorned': 1,\n",
       "             'decorated': 1,\n",
       "             'beady': -1,\n",
       "             'gemmed': -1,\n",
       "             'jeweled': -1,\n",
       "             'jewelled': -1,\n",
       "             'sequined': -1,\n",
       "             'spangled': -1,\n",
       "             'spangly': -1,\n",
       "             'bedaubed': -1,\n",
       "             'bespectacled': -1,\n",
       "             'monocled': -1,\n",
       "             'spectacled': -1,\n",
       "             'buttony': -1,\n",
       "             'crested': -1,\n",
       "             'plumed': -1,\n",
       "             'feathery': -1,\n",
       "             'feathered': -1,\n",
       "             'plumy': -1,\n",
       "             'frilled': -1,\n",
       "             'frilly': -1,\n",
       "             'ruffled': -1,\n",
       "             'fringed': 1,\n",
       "             'gilt-edged': 1,\n",
       "             'inflamed': -1,\n",
       "             'inlaid': -1,\n",
       "             'mounted': -1,\n",
       "             'paneled': -1,\n",
       "             'wainscoted': -1,\n",
       "             'studded': -1,\n",
       "             'tapestried': -1,\n",
       "             'tasseled': -1,\n",
       "             'tasselled': -1,\n",
       "             'tricked-out': -1,\n",
       "             'tufted': -1,\n",
       "             'unadorned': -1,\n",
       "             'undecorated': -1,\n",
       "             'plain': -1,\n",
       "             'bare': -1,\n",
       "             'spare': -1,\n",
       "             'unembellished': -1,\n",
       "             'unornamented': -1,\n",
       "             'adroit': 1,\n",
       "             'neat': 1,\n",
       "             'clever': 1,\n",
       "             'cunning': 1,\n",
       "             'ingenious': 1,\n",
       "             'handy': 1,\n",
       "             'quick-witted': 1,\n",
       "             'maladroit': -1,\n",
       "             'bumbling': -1,\n",
       "             'bungling': -1,\n",
       "             'butterfingered': -1,\n",
       "             'ham-fisted': -1,\n",
       "             'ham-handed': -1,\n",
       "             'handless': -1,\n",
       "             'heavy-handed': -1,\n",
       "             'left-handed': -1,\n",
       "             'inept': -1,\n",
       "             'tactless': -1,\n",
       "             'uncoordinated': 1,\n",
       "             'unmechanical': -1,\n",
       "             'nonmechanical': -1,\n",
       "             'advantageous': 1,\n",
       "             'beneficial': 1,\n",
       "             'good': 1,\n",
       "             'plus': 1,\n",
       "             'positive': 1,\n",
       "             'discriminatory': 1,\n",
       "             'preferential': -1,\n",
       "             'disadvantageous': 1,\n",
       "             'minus': -1,\n",
       "             'negative': -1,\n",
       "             'adventurous': 1,\n",
       "             'adventuresome': 1,\n",
       "             'audacious': 1,\n",
       "             'daring': -1,\n",
       "             'venturesome': 1,\n",
       "             'venturous': 1,\n",
       "             'swaggering': 1,\n",
       "             'swashbuckling': 1,\n",
       "             'unadventurous': -1,\n",
       "             'advisable': 1,\n",
       "             'better': 1,\n",
       "             'best': 1,\n",
       "             'well': 1,\n",
       "             'inadvisable': -1,\n",
       "             'unadvisable': -1,\n",
       "             'well-advised': 1,\n",
       "             'advised': 1,\n",
       "             'ill-advised': -1,\n",
       "             'unadvised': 1,\n",
       "             'anaerobic': -1,\n",
       "             'anaerobiotic': -1,\n",
       "             'aerobic': 1,\n",
       "             'aesthetic': 1,\n",
       "             'esthetic': 1,\n",
       "             'aesthetical': 1,\n",
       "             'esthetical': 1,\n",
       "             'artistic': 1,\n",
       "             'painterly': 1,\n",
       "             'sensuous': 1,\n",
       "             'inartistic': -1,\n",
       "             'unartistic': -1,\n",
       "             'affected': -1,\n",
       "             'smitten': 1,\n",
       "             'stricken': -1,\n",
       "             'struck': -1,\n",
       "             'stage-struck': 1,\n",
       "             'subject': 1,\n",
       "             'taken': 1,\n",
       "             'wonder-struck': -1,\n",
       "             'immune': 1,\n",
       "             'superior': 1,\n",
       "             'unimpressed': -1,\n",
       "             'uninfluenced': -1,\n",
       "             'unswayed': -1,\n",
       "             'untouched': -1,\n",
       "             'artificial': -1,\n",
       "             'contrived': 1,\n",
       "             'hokey': 1,\n",
       "             'stilted': -1,\n",
       "             'constrained': -1,\n",
       "             'forced': 1,\n",
       "             'strained': 1,\n",
       "             'elocutionary': -1,\n",
       "             'mannered': -1,\n",
       "             'plummy': 1,\n",
       "             'unaffected': -1,\n",
       "             'lifelike': 1,\n",
       "             'natural': 1,\n",
       "             'unmannered': -1,\n",
       "             'affirmative': 1,\n",
       "             'affirmatory': 1,\n",
       "             'assentient': 1,\n",
       "             'dissentient': -1,\n",
       "             'dissenting': -1,\n",
       "             'dissident': 1,\n",
       "             'acceptive': 1,\n",
       "             'accepting': 1,\n",
       "             'rejective': -1,\n",
       "             'dismissive': 1,\n",
       "             'afraid': -1,\n",
       "             'acrophobic': -1,\n",
       "             'afeard': -1,\n",
       "             'afeared': -1,\n",
       "             'aghast': 1,\n",
       "             'appalled': 1,\n",
       "             'dismayed': 1,\n",
       "             'shocked': 1,\n",
       "             'agoraphobic': -1,\n",
       "             'alarmed': -1,\n",
       "             'algophobic': -1,\n",
       "             'apprehensive': 1,\n",
       "             'hangdog': 1,\n",
       "             'claustrophobic': -1,\n",
       "             'fearful': -1,\n",
       "             'frightened': -1,\n",
       "             'scared': -1,\n",
       "             'horrified': -1,\n",
       "             'horror-stricken': -1,\n",
       "             'horror-struck': -1,\n",
       "             'hunted': 1,\n",
       "             'hydrophobic': -1,\n",
       "             'aquaphobic': -1,\n",
       "             'mysophobic': -1,\n",
       "             'numb': -1,\n",
       "             'shitless': -1,\n",
       "             'terror-stricken': -1,\n",
       "             'terror-struck': -1,\n",
       "             'triskaidekaphobic': -1,\n",
       "             'unnerved': -1,\n",
       "             'white-lipped': 1,\n",
       "             'xenophobic': -1,\n",
       "             'unafraid': -1,\n",
       "             'fearless': 1,\n",
       "             'unapprehensive': -1,\n",
       "             'unblinking': 1,\n",
       "             'unflinching': -1,\n",
       "             'unintimidated': -1,\n",
       "             'unshrinking': -1,\n",
       "             'unfrightened': 1,\n",
       "             'aggressive': -1,\n",
       "             'battleful': 1,\n",
       "             'bellicose': 1,\n",
       "             'combative': 1,\n",
       "             'competitive': 1,\n",
       "             'militant': 1,\n",
       "             'in-your-face': 1,\n",
       "             'obstreperous': -1,\n",
       "             'pugnacious': -1,\n",
       "             'rough': -1,\n",
       "             'scrappy': 1,\n",
       "             'truculent': -1,\n",
       "             'unaggressive': -1,\n",
       "             'nonaggressive': -1,\n",
       "             'low-pressure': -1,\n",
       "             'agitated': -1,\n",
       "             'aroused': 1,\n",
       "             'emotional': 1,\n",
       "             'excited': 1,\n",
       "             'worked_up': -1,\n",
       "             'distraught': -1,\n",
       "             'overwrought': -1,\n",
       "             'jolted': -1,\n",
       "             'shaken': -1,\n",
       "             'frantic': 1,\n",
       "             'frenetic': -1,\n",
       "             'phrenetic': -1,\n",
       "             'frenzied': -1,\n",
       "             'hysterical': -1,\n",
       "             'psychedelic': -1,\n",
       "             'unagitated': -1,\n",
       "             'churning': 1,\n",
       "             'churned-up': 1,\n",
       "             'nonturbulent': -1,\n",
       "             'unstirred': 1,\n",
       "             'agreeable': 1,\n",
       "             'disagreeable': -1,\n",
       "             'annoying': -1,\n",
       "             'bothersome': -1,\n",
       "             'galling': -1,\n",
       "             'irritating': -1,\n",
       "             'nettlesome': -1,\n",
       "             'pesky': -1,\n",
       "             'pestering': -1,\n",
       "             'pestiferous': 1,\n",
       "             'plaguy': -1,\n",
       "             'plaguey': -1,\n",
       "             'teasing': -1,\n",
       "             'vexatious': -1,\n",
       "             'vexing': -1,\n",
       "             'harsh': -1,\n",
       "             'abrasive': -1,\n",
       "             'nerve-racking': -1,\n",
       "             'nerve-wracking': -1,\n",
       "             'stressful': -1,\n",
       "             'trying': -1,\n",
       "             'unsweet': -1,\n",
       "             'alert': -1,\n",
       "             'watchful': -1,\n",
       "             'argus-eyed': 1,\n",
       "             'open-eyed': 1,\n",
       "             'vigilant': 1,\n",
       "             'wakeful': 1,\n",
       "             'fly': -1,\n",
       "             'heads-up': 1,\n",
       "             'wide-awake': -1,\n",
       "             'lidless': -1,\n",
       "             'sleepless': -1,\n",
       "             'unalert': -1,\n",
       "             'unwatchful': -1,\n",
       "             'unvigilant': -1,\n",
       "             'assignable': 1,\n",
       "             'conveyable': 1,\n",
       "             'negotiable': 1,\n",
       "             'transferable': 1,\n",
       "             'transferrable': 1,\n",
       "             'inalienable': -1,\n",
       "             'unalienable': -1,\n",
       "             'infrangible': -1,\n",
       "             'inviolable': 1,\n",
       "             'non-negotiable': 1,\n",
       "             'nontransferable': -1,\n",
       "             'unassignable': -1,\n",
       "             'untransferable': -1,\n",
       "             'liveborn': 1,\n",
       "             'viable': 1,\n",
       "             'asleep': -1,\n",
       "             'at_peace': -1,\n",
       "             'at_rest': -1,\n",
       "             'deceased': -1,\n",
       "             'departed': -1,\n",
       "             'gone': -1,\n",
       "             'bloodless': -1,\n",
       "             'exsanguine': -1,\n",
       "             'exsanguinous': -1,\n",
       "             'breathless': -1,\n",
       "             'inanimate': -1,\n",
       "             'pulseless': -1,\n",
       "             'cold': -1,\n",
       "             'deathlike': -1,\n",
       "             'deathly': -1,\n",
       "             'doomed': -1,\n",
       "             'fallen': -1,\n",
       "             'late': -1,\n",
       "             'lifeless': 1,\n",
       "             'exanimate': -1,\n",
       "             'nonviable': -1,\n",
       "             'stillborn': -1,\n",
       "             'stone-dead': -1,\n",
       "             'eccrine': 1,\n",
       "             'subartesian': -1,\n",
       "             'extinct': -1,\n",
       "             'out': -1,\n",
       "             'analphabetic': -1,\n",
       "             'precocial': 1,\n",
       "             'altruistic': 1,\n",
       "             'selfless': 1,\n",
       "             'egoistic': 1,\n",
       "             'egoistical': 1,\n",
       "             'egocentric': 1,\n",
       "             'self-centered': 1,\n",
       "             'self-centred': 1,\n",
       "             'self-absorbed': 1,\n",
       "             'self-involved': 1,\n",
       "             'ambiguous': -1,\n",
       "             'double-barreled': 1,\n",
       "             'double-barrelled': 1,\n",
       "             'enigmatic': -1,\n",
       "             'oracular': 1,\n",
       "             'multivalent': 1,\n",
       "             'multi-valued': 1,\n",
       "             'unambiguous': -1,\n",
       "             'monosemous': 1,\n",
       "             'ambitious': -1,\n",
       "             'pushful': 1,\n",
       "             'pushy': 1,\n",
       "             'aspirant': 1,\n",
       "             'aspiring': 1,\n",
       "             'wishful': 1,\n",
       "             'compulsive': -1,\n",
       "             'determined': -1,\n",
       "             'driven': 1,\n",
       "             'manque': 1,\n",
       "             'would-be': 1,\n",
       "             'overambitious': 1,\n",
       "             'unambitious': 1,\n",
       "             'ambitionless': 1,\n",
       "             'shiftless': -1,\n",
       "             'ametropic': -1,\n",
       "             'generous': -1,\n",
       "             'meager': -1,\n",
       "             'meagre': -1,\n",
       "             'meagerly': -1,\n",
       "             'stingy': -1,\n",
       "             'scrimpy': -1,\n",
       "             'scanty': -1,\n",
       "             'exiguous': -1,\n",
       "             'hand-to-mouth': -1,\n",
       "             'hardscrabble': -1,\n",
       "             'anabolic': 1,\n",
       "             'catabolic': 1,\n",
       "             'katabolic': 1,\n",
       "             'anastigmatic': -1,\n",
       "             'stigmatic': -1,\n",
       "             'analytic': 1,\n",
       "             'analytical': 1,\n",
       "             'uninflected': -1,\n",
       "             'derivational': -1,\n",
       "             'aggravated': -1,\n",
       "             'provoked': -1,\n",
       "             'black': -1,\n",
       "             'choleric': 1,\n",
       "             'irascible': -1,\n",
       "             'huffy': -1,\n",
       "             'mad': 1,\n",
       "             'sore': -1,\n",
       "             'indignant': -1,\n",
       "             'incensed': -1,\n",
       "             'outraged': -1,\n",
       "             'umbrageous': -1,\n",
       "             'irate': -1,\n",
       "             'ireful': -1,\n",
       "             'livid': -1,\n",
       "             'smoldering': 1,\n",
       "             'smouldering': 1,\n",
       "             'wrathful': -1,\n",
       "             'wroth': -1,\n",
       "             'wrothful': -1,\n",
       "             'unangry': -1,\n",
       "             'resentful': -1,\n",
       "             'acrimonious': 1,\n",
       "             'bitter': -1,\n",
       "             'rancorous': 1,\n",
       "             'unresentful': -1,\n",
       "             'unbitter': -1,\n",
       "             'sentient': 1,\n",
       "             'animate': 1,\n",
       "             'sensate': -1,\n",
       "             'insentient': -1,\n",
       "             'insensate': -1,\n",
       "             'unfeeling': -1,\n",
       "             'nonliving': -1,\n",
       "             'non-living': -1,\n",
       "             'animated': 1,\n",
       "             'enlivened': 1,\n",
       "             'spirited': 1,\n",
       "             'full_of_life': 1,\n",
       "             'lively': 1,\n",
       "             'vital': 1,\n",
       "             'reanimated': 1,\n",
       "             'revived': 1,\n",
       "             'unanimated': -1,\n",
       "             'wan': -1,\n",
       "             'perked_up': 1,\n",
       "             'unenlivened': -1,\n",
       "             'anonymous': -1,\n",
       "             'anon.': -1,\n",
       "             'postmortem': -1,\n",
       "             'postmortal': -1,\n",
       "             'anticipatory': 1,\n",
       "             'prevenient': 1,\n",
       "             'preexistent': -1,\n",
       "             'pre-existent': -1,\n",
       "             'preexisting': -1,\n",
       "             'pre-existing': -1,\n",
       "             'decurved': -1,\n",
       "             'preceding': -1,\n",
       "             'above': 1,\n",
       "             'above-mentioned': -1,\n",
       "             'above-named': -1,\n",
       "             'back-to-back': -1,\n",
       "             'consecutive': 1,\n",
       "             'precedented': 1,\n",
       "             'unexampled': -1,\n",
       "             'nonprehensile': -1,\n",
       "             'prenatal': -1,\n",
       "             'antenatal': -1,\n",
       "             'antepartum': -1,\n",
       "             'postnatal': -1,\n",
       "             'postpartum': -1,\n",
       "             'retrograde': -1,\n",
       "             'anterograde': -1,\n",
       "             'appealable': 1,\n",
       "             'unappealable': -1,\n",
       "             'unappendaged': -1,\n",
       "             'appetizing': 1,\n",
       "             'appetising': 1,\n",
       "             'mouth-watering': 1,\n",
       "             'savory': 1,\n",
       "             'savoury': 1,\n",
       "             'unappetizing': -1,\n",
       "             'unappetising': -1,\n",
       "             'unapproachable': -1,\n",
       "             'offish': -1,\n",
       "             'standoffish': -1,\n",
       "             'pat': 1,\n",
       "             'inappropriate': -1,\n",
       "             'unbefitting': -1,\n",
       "             'improper': -1,\n",
       "             'delinquent': -1,\n",
       "             'overdue': -1,\n",
       "             'undue': -1,\n",
       "             'due': 1,\n",
       "             'apropos': 1,\n",
       "             'apposite': 1,\n",
       "             'apt': -1,\n",
       "             'pertinent': 1,\n",
       "             'malapropos': -1,\n",
       "             'inapposite': -1,\n",
       "             'out_of_place': -1,\n",
       "             'a_priori': -1,\n",
       "             'a_posteriori': 1,\n",
       "             'apteral': -1,\n",
       "             'amphiprostylar': 1,\n",
       "             'amphiprostyle': 1,\n",
       "             'amphistylar': 1,\n",
       "             'porticoed': 1,\n",
       "             'prostyle': 1,\n",
       "             'pseudoprostyle': 1,\n",
       "             'nonarbitrable': -1,\n",
       "             'columned': 1,\n",
       "             'columnar': 1,\n",
       "             'colonnaded': 1,\n",
       "             'pillared': 1,\n",
       "             'noncolumned': -1,\n",
       "             'uncolumned': -1,\n",
       "             'astylar': -1,\n",
       "             'unpillared': -1,\n",
       "             'nonarboreal': -1,\n",
       "             'arenaceous': -1,\n",
       "             'sandy': -1,\n",
       "             'sandlike': -1,\n",
       "             'argillaceous': -1,\n",
       "             'clayey': -1,\n",
       "             'armed': -1,\n",
       "             'unarmed': -1,\n",
       "             'barehanded': 1,\n",
       "             'defenseless': -1,\n",
       "             'defenceless': -1,\n",
       "             'armored': 1,\n",
       "             'armoured': -1,\n",
       "             'armor-clad': -1,\n",
       "             'armour-clad': -1,\n",
       "             'armor-plated': -1,\n",
       "             'armour-plated': -1,\n",
       "             'steel-plated': -1,\n",
       "             'bony-plated': -1,\n",
       "             'bulletproof': -1,\n",
       "             'mail-clad': -1,\n",
       "             'mailed': -1,\n",
       "             'barbed': 1,\n",
       "             'barbellate': -1,\n",
       "             'briary': -1,\n",
       "             'briery': -1,\n",
       "             'bristled': -1,\n",
       "             'bristly': -1,\n",
       "             'burred': -1,\n",
       "             'burry': -1,\n",
       "             'prickly': -1,\n",
       "             'setose': -1,\n",
       "             'setaceous': -1,\n",
       "             'spiny': 1,\n",
       "             'thorny': -1,\n",
       "             'brushlike': -1,\n",
       "             'clawed': -1,\n",
       "             'taloned': -1,\n",
       "             'thornless': -1,\n",
       "             'spineless': -1,\n",
       "             'armlike': -1,\n",
       "             'bone-covered': -1,\n",
       "             'unarmored': -1,\n",
       "             'unarmoured': -1,\n",
       "             'scaleless': -1,\n",
       "             'artful': -1,\n",
       "             'crafty': 1,\n",
       "             'dodgy': -1,\n",
       "             'foxy': 1,\n",
       "             'guileful': 1,\n",
       "             'knavish': 1,\n",
       "             'slick': 1,\n",
       "             'sly': 1,\n",
       "             'tricksy': 1,\n",
       "             'tricky': -1,\n",
       "             'wily': 1,\n",
       "             'cute': 1,\n",
       "             'precious': -1,\n",
       "             'designing': 1,\n",
       "             'scheming': 1,\n",
       "             'deep': 1,\n",
       "             'elusive': 1,\n",
       "             'manipulative': 1,\n",
       "             'pawky': -1,\n",
       "             'artless': 1,\n",
       "             'careless': -1,\n",
       "             'articulate': 1,\n",
       "             'eloquent': 1,\n",
       "             'facile': -1,\n",
       "             'fluent': -1,\n",
       "             'silver': 1,\n",
       "             'silver-tongued': 1,\n",
       "             'smooth-spoken': 1,\n",
       "             'speech-endowed': 1,\n",
       "             'inarticulate': -1,\n",
       "             'unarticulate': -1,\n",
       "             'aphasic': -1,\n",
       "             'aphonic': -1,\n",
       "             'voiceless': -1,\n",
       "             'incoherent': -1,\n",
       "             'tongue-tied': -1,\n",
       "             'mute': -1,\n",
       "             'tongueless': -1,\n",
       "             'unspoken': -1,\n",
       "             'wordless': -1,\n",
       "             'speechless': -1,\n",
       "             'dumb': -1,\n",
       "             'unarticulated': -1,\n",
       "             'speaking': 1,\n",
       "             'tongued': -1,\n",
       "             'nonspeaking': -1,\n",
       "             'walk-on': -1,\n",
       "             'articulated': -1,\n",
       "             'unjointed': -1,\n",
       "             'ashamed': -1,\n",
       "             'discredited': -1,\n",
       "             'disgraced': -1,\n",
       "             'dishonored': -1,\n",
       "             'shamed': 1,\n",
       "             'embarrassed': -1,\n",
       "             'humiliated': -1,\n",
       "             'mortified': -1,\n",
       "             'guilty': -1,\n",
       "             'shamefaced': 1,\n",
       "             'sheepish': -1,\n",
       "             'barefaced': -1,\n",
       "             'bodacious': -1,\n",
       "             'bald-faced': 1,\n",
       "             'brassy': -1,\n",
       "             'brazen': 1,\n",
       "             'brazen-faced': 1,\n",
       "             'insolent': -1,\n",
       "             'shameless': -1,\n",
       "             'unblushing': -1,\n",
       "             'unabashed': -1,\n",
       "             'unembarrassed': -1,\n",
       "             'assertive': 1,\n",
       "             'self-asserting': 1,\n",
       "             'self-assertive': -1,\n",
       "             'cocky': 1,\n",
       "             'emphatic': 1,\n",
       "             'forceful': -1,\n",
       "             'unassertive': 1,\n",
       "             'nonassertive': -1,\n",
       "             'reticent': 1,\n",
       "             'self-effacing': -1,\n",
       "             'retiring': -1,\n",
       "             'associative': -1,\n",
       "             'associatory': -1,\n",
       "             'associable': 1,\n",
       "             'nonassociative': -1,\n",
       "             'bespoken': 1,\n",
       "             'betrothed': 1,\n",
       "             'unattached': -1,\n",
       "             'uncommitted': -1,\n",
       "             'unengaged': -1,\n",
       "             'unpledged': -1,\n",
       "             'unpromised': -1,\n",
       "             'affixed': 1,\n",
       "             'unaffixed': -1,\n",
       "             'loose': 1,\n",
       "             'sessile': -1,\n",
       "             'stalkless': -1,\n",
       "             'pedunculate': -1,\n",
       "             'stalked': -1,\n",
       "             'free-swimming': -1,\n",
       "             'freestanding': -1,\n",
       "             'separate': -1,\n",
       "             'unstuck': 1,\n",
       "             'attachable': 1,\n",
       "             'bindable': 1,\n",
       "             'clip-on': 1,\n",
       "             'detachable': 1,\n",
       "             'clastic': 1,\n",
       "             'wary': -1,\n",
       "             'on_guard': 1,\n",
       "             \"on_one's_guard\": 1,\n",
       "             \"upon_one's_guard\": 1,\n",
       "             'on_your_guard': 1,\n",
       "             'shy': 1,\n",
       "             'unwary': -1,\n",
       "             'gullible': 1,\n",
       "             'unguarded': -1,\n",
       "             'attentive': 1,\n",
       "             'captive': 1,\n",
       "             'absorbed': 1,\n",
       "             'engrossed': 1,\n",
       "             'enwrapped': 1,\n",
       "             'intent': 1,\n",
       "             'wrapped': -1,\n",
       "             'advertent': 1,\n",
       "             'heedful': 1,\n",
       "             'observant': 1,\n",
       "             'oversolicitous': -1,\n",
       "             'solicitous': -1,\n",
       "             'inattentive': -1,\n",
       "             'absent': -1,\n",
       "             'absentminded': 1,\n",
       "             'abstracted': 1,\n",
       "             'scatty': -1,\n",
       "             'distracted': -1,\n",
       "             'distrait': -1,\n",
       "             'drowsy': -1,\n",
       "             'oscitant': -1,\n",
       "             'yawning': -1,\n",
       "             'forgetful': -1,\n",
       "             'oblivious': -1,\n",
       "             'attractive': 1,\n",
       "             'bewitching': 1,\n",
       "             'captivating': 1,\n",
       "             'enchanting': 1,\n",
       "             'enthralling': 1,\n",
       "             'entrancing': 1,\n",
       "             'fascinating': 1,\n",
       "             'charismatic': 1,\n",
       "             'magnetic': 1,\n",
       "             'dinky': -1,\n",
       "             'engaging': 1,\n",
       "             'piquant': -1,\n",
       "             'fetching': 1,\n",
       "             'taking': 1,\n",
       "             'winning': -1,\n",
       "             'glossy': 1,\n",
       "             'showy': 1,\n",
       "             'hypnotic': 1,\n",
       "             'mesmeric': 1,\n",
       "             'mesmerizing': 1,\n",
       "             'spellbinding': 1,\n",
       "             'irresistible': -1,\n",
       "             'personable': 1,\n",
       "             'photogenic': 1,\n",
       "             'prepossessing': 1,\n",
       "             'winsome': 1,\n",
       "             'unattractive': -1,\n",
       "             'homely': 1,\n",
       "             'subfusc': -1,\n",
       "             'unprepossessing': -1,\n",
       "             'unpresentable': -1,\n",
       "             'repulsive': -1,\n",
       "             'appealing': -1,\n",
       "             'attention-getting': -1,\n",
       "             'catchy': 1,\n",
       "             'unappealing': -1,\n",
       "             'off-putting': -1,\n",
       "             'attributable': 1,\n",
       "             'ascribable': -1,\n",
       "             'imputable': -1,\n",
       "             'referable': -1,\n",
       "             'unattributable': -1,\n",
       "             'unascribable': -1,\n",
       "             'nonpregnant': -1,\n",
       "             'audible': 1,\n",
       "             'hearable': 1,\n",
       "             'inaudible': -1,\n",
       "             'unhearable': -1,\n",
       "             'breathed': 1,\n",
       "             'infrasonic': 1,\n",
       "             'silent': -1,\n",
       "             'unsounded': 1,\n",
       "             'supersonic': 1,\n",
       "             'ultrasonic': 1,\n",
       "             'unheard': 1,\n",
       "             'auspicious': 1,\n",
       "             'bright': 1,\n",
       "             'hopeful': 1,\n",
       "             'promising': 1,\n",
       "             ...})"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POS_SCORES[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE WHOLE USE OF THE METHODS IS THAT TO GET INSIGHT INTO THE CONTEXT-SENSITIVE INFORMATION.\n",
    "\n",
    "\n",
    "### HOW? \n",
    "\n",
    "### 1. FIND THE LEXICONS IF BINARY \n",
    "### 2. WORD EMBEDDINGS TRAINED ON THE CONTEXT-MATERIAL.\n",
    "\n",
    "### LABEL-PROPAGATE ALGORITHM TO MAKE LEXICONS FROM BINARY TO THE CONTINOUS SENTIMENT SCORES.\n",
    "\n",
    "### USE SUM (P+V)/T OR NEURAL NETWORK TO OBTAIN THE SCORE FOR THE WHOLE SENTENCE SENTIMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117659"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
