{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from normalization import normalize_accented_characters, html_parser, strip_html\n",
    "from utils import display_evaluation_metrics, display_confusion_matrix, display_classification_report\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sentiwordnet')\n",
    "import dynet as dy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_movie_dataset(train_start,train_end,test_start,test_end) : \n",
    "\n",
    "    dataset = pd.read_csv(r'datasets/movie_reviews.csv')\n",
    "    print('dataset size : ',dataset.shape[0])\n",
    "\n",
    "    train_data = dataset[train_start:train_end]\n",
    "    test_data = dataset[test_start:test_end]\n",
    "    \n",
    "    print('Train_X : ',train_data.shape[0])\n",
    "    print('Test_X  : ',test_data.shape[0])\n",
    "\n",
    "    test_reviews = np.array(test_data['review'])\n",
    "    test_sentiments = np.array(test_data['sentiment'])\n",
    "\n",
    "    return train_data,test_reviews,test_sentiments\n",
    "\n",
    "def prepare_labeled_data(train_start,train_end,test_start,test_end) : \n",
    "    \n",
    "    labeled_data=open(\"datasets/labeledTrainData.tsv\",\"r\")\n",
    "    data=labeled_data.readlines()\n",
    "    data=[d.split(\"\\t\") for d in data]\n",
    "    sa_data=pd.DataFrame(data,columns=['ind','sentiment','review'])\n",
    "    sa_data=sa_data[['sentiment','review']]\n",
    "    \n",
    "    print('dataset size : ',sa_data.shape[0])\n",
    "\n",
    "    train_data = sa_data[train_start:train_end]\n",
    "    test_data = sa_data[test_start:test_end]\n",
    "    \n",
    "    print('Train_X : ',train_data.shape[0])\n",
    "    print('Test_X  : ',test_data.shape[0])\n",
    "\n",
    "    test_reviews = np.array(test_data['review'])\n",
    "    test_sentiments = np.array(test_data['sentiment'])\n",
    "\n",
    "    return train_data,test_reviews,test_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  50000\n",
      "Train_X :  999\n",
      "Test_X  :  1000\n"
     ]
    }
   ],
   "source": [
    "train_x,test_x,test_y=prepare_movie_dataset(1,1000,1000,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for unsupervised Lexicon sentiment tagging\n",
    "\n",
    "#### compare against the sentence tagging (already provided in the dataset )\n",
    "\n",
    "[add markdown #11 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BORROWED FROM THE AR_SARKAR METRIC\n",
    "def analyze_sentiment_sentiwordnet_lexicon(review,verbose=False):\n",
    "    \n",
    "    \n",
    "    #review = normalize_accented_characters(review)\n",
    "    #review = review.decode('utf-8')\n",
    "    review = html_parser.unescape(review)\n",
    "    review = strip_html(review)\n",
    "    \n",
    "    text_tokens = nltk.word_tokenize(review)\n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and swn.senti_synsets(word, 'n'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'VB' in tag and swn.senti_synsets(word, 'v'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'JJ' in tag and swn.senti_synsets(word, 'a'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'RB' in tag and swn.senti_synsets(word, 'r'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        \n",
    "        if ss_set:\n",
    "            \n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    \n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        \n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score,\n",
    "                                         norm_pos_score, norm_neg_score,\n",
    "                                         norm_final_score]],\n",
    "                                         columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment', 'Objectivity',\n",
    "                                                                       'Positive', 'Negative', 'Overall']], \n",
    "                                                              labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print (sentiment_frame)   \n",
    "    return final_sentiment\n",
    "            \n",
    "                                                               \n",
    "def evaluate_lexicons(TRUE_LABELS,PREDICTED_LABELS,POS_CLASS,NEG_CLASS) : \n",
    "\n",
    "    print ('Performance metrics:')\n",
    "    display_evaluation_metrics(true_labels=TRUE_LABELS,\n",
    "                               predicted_labels=PREDICTED_LABELS,\n",
    "                               positive_class=str(POS_CLASS))  \n",
    "    print ('\\nConfusion Matrix:'             )              \n",
    "    display_confusion_matrix(true_labels=TRUE_LABELS,\n",
    "                             predicted_labels=PREDICTED_LABELS,\n",
    "                             classes=[str(POS_CLASS),str(NEG_CLASS)])\n",
    "    print ('\\nClassification report:' )                        \n",
    "    display_classification_report(true_labels=TRUE_LABELS,\n",
    "                                  predicted_labels=PREDICTED_LABELS,\n",
    "                                  classes=[str(POS_CLASS),str(NEG_CLASS)])\n",
    "    return\n",
    "\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basline lexicon evaluation\n",
    "\n",
    "#### movie dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,test_y=prepare_movie_dataset(0,1000,1000,2000)\n",
    "sentiwordnet_predictions = [analyze_sentiment_sentiwordnet_lexicon(review) for review in test_x]\n",
    "evaluate_lexicons(test_y.tolist(),sentiwordnet_predictions,'positive','negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labeled dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,test_y=prepare_labeled_data(0,1000,1000,2000)\n",
    "sentiwordnet_predictions = [analyze_sentiment_sentiwordnet_lexicon(review) for review in test_x]\n",
    "binary_predicted=['1' if p=='positive' else '0' for p in sentiwordnet_predictions ]\n",
    "evaluate_lexicons(test_y.tolist(),binary_predicted,'1','0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple network for learning (do afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SIMPLE NETWORK WITH THE sigma(V*tanh(WX+B)) ## for the XOR problem\n",
    "# create a parameter collection and add the parameters.\n",
    "m = dy.ParameterCollection()\n",
    "W = m.add_parameters((8,2))\n",
    "V = m.add_parameters((1,8))\n",
    "b = m.add_parameters((8))\n",
    "\n",
    "dy.renew_cg() # new computation graph. not strictly needed here, but good practice.\n",
    "b.value() ## bias values\n",
    "x=dy.vecInput(2) ## 2 sized inputs \n",
    "output=dy.logistic(V*(dy.tanh(W*x)+b)) ## output node\n",
    "\n",
    "y = dy.scalarInput(0) ## objective function\n",
    "loss = dy.binary_log_loss(output,y) ## loss function\n",
    "## trainer with the initialized parameters m \n",
    "trainer=dy.SimpleSGDTrainer(m)\n",
    "x.set([1,0])\n",
    "y.set(1)\n",
    "loss_value = loss.value() # this performs a forward through the network.\n",
    "print(\"the loss before step is:\",loss_value)\n",
    "\n",
    "loss.backward()  # compute the gradients\n",
    "trainer.update()\n",
    "\n",
    "loss_value = loss.value(recalculate=True) \n",
    "print(\"the loss after step is:\",loss_value)\n",
    "pc = dy.ParameterCollection()\n",
    "NUM_LAYERS=2\n",
    "INPUT_DIM=50\n",
    "HIDDEN_DIM=10\n",
    "builder = dy.LSTMBuilder(NUM_LAYERS, INPUT_DIM, HIDDEN_DIM, pc)\n",
    "s0 = builder.initial_state()\n",
    "x1 = dy.vecInput(INPUT_DIM)\n",
    "s1=s0.add_input(x1)\n",
    "y1 = s1.output()\n",
    "s2=s1.add_input(x1) # we can add another input\n",
    "y2=s2.output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING 2.1 SECTION OF GOOGLE PAPER FOR LEXICON EXPANSION [this is for the lexicon expansion ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORDS=list(wn.words())\n",
    "SCORES=defaultdict()\n",
    "df=pd.DataFrame(ALL_WORDS,columns=['word'])\n",
    "\n",
    "swn.all_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the lexicons from stanford paper \"Incuding Domain-Specific Sentiment Lexicons from Unalabeled Copora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socialsent_util\n",
    "def load_lexicon(name, remove_neutral=True):\n",
    "    lexicon = socialsent_util.load_json(\"./lexicons_socialsent/\"+ name + '.json')\n",
    "    return {w: p for w, p in lexicon.items() if p != 0} if remove_neutral else lexicon\n",
    "\n",
    "def compare_lexicons(print_disagreements=False):\n",
    "    lexicons = {\n",
    "        \"inquirer\": load_lexicon(\"inquirer\", False),\n",
    "        \"mpqa\": load_lexicon(\"mpqa\", False),\n",
    "        \"bingliu\": load_lexicon(\"bingliu\", False),\n",
    "    }\n",
    "\n",
    "    for l in lexicons:\n",
    "        print( l, len(lexicons[l]), len([w for w in lexicons[l] if lexicons[l][w] != 0]))\n",
    "\n",
    "    for l1, l2 in itertools.combinations(lexicons.keys(), 2):\n",
    "        ps1, ps2 = lexicons[l1], lexicons[l2]\n",
    "        common_words = set(ps1.keys()) & set(ps2.keys())\n",
    "        print( l1, l2, \"agreement: {:.2f}\".format(\n",
    "            100.0 * sum(1 if ps1[w] == ps2[w] else 0 for w in common_words) / len(common_words)))\n",
    "        common_words = set([word for word in ps1.keys() if ps1[word] != 0]) & \\\n",
    "                       set([word for word in ps2.keys() if ps2[word] != 0])  \n",
    "        print (l1, l2, \"agreement ignoring neutral: {:.2f}\".format(\n",
    "            100.0 * sum(1 if ps1[w] * ps2[w] == 1 else 0 for w in common_words) / len(common_words)))\n",
    "        \n",
    "        if print_disagreements and l1 == 'opinion' and l2 == 'inquirer':\n",
    "            for w in common_words:\n",
    "                if lexicons[l1][w] != lexicons[l2][w]:\n",
    "                    print (w, lexicons[l1][w], lexicons[l2][w])\n",
    "      \n",
    "    \n",
    "## ALL THESE LEXICONS ARE 2-CLASS SENTIMENTS. 1 = POSITIVE; -1 = NEGATIVE\n",
    "finance_lexicons=load_lexicon('finance')\n",
    "bingliu_lexicons=load_lexicon('bingliu')\n",
    "inquirer_lexicons=load_lexicon('inquirer')\n",
    "mpqa_lexicons=load_lexicon('mpqa')\n",
    "twitter_lexicons=load_lexicon('twitter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compares the different lexicon repositories through the mutual information between them (common words)\n",
    "\n",
    "### The comparison is done through looking for words in two lexicon dictionaries L1 AND L2, and how many words are common in them which have the same scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inquirer 8640 3457\n",
      "mpqa 6886 6462\n",
      "bingliu 6785 6785\n",
      "inquirer mpqa agreement: 82.47\n",
      "inquirer mpqa agreement ignoring neutral: 98.50\n",
      "inquirer bingliu agreement: 84.39\n",
      "inquirer bingliu agreement ignoring neutral: 98.74\n",
      "mpqa bingliu agreement: 99.19\n",
      "mpqa bingliu agreement ignoring neutral: 99.44\n"
     ]
    }
   ],
   "source": [
    "## FIGURE OUT WHAT DOES THE COMPARE_LEXICONS DOES.\n",
    "compare_lexicons()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Induction : the idea is to generate the lexicons provided the corpus. This method makes sure that the lexicon are sensitive to the context they are drawn from. They may prove useful if we would like to assess them in a simiar context. For instance, financial lexicons will reflect better sentiments than using general lexicons such as SentiWordNet. Three ways purposed for induction \n",
    "\n",
    "- SENTPROP\n",
    "- DENSIFIER\n",
    "- Sentiment140"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLARITY INDUCTION METHOD : This is used for re-scoring of the lexicons(tokens) by taking information from the word-embeddings (domain-specific), positive and the negative seed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data =  /home/ubuntu/workspace/nlpclass-1187-g-Mad_Titans/sa/embeddings_socialsent/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import polarity_induction_methods\n",
    "\n",
    "### THIS IS THE FUNCTION FOR INDUCING LEXICONS GIVEN THE SEEDS, EMBEDDINGS AND THE METHOD.\n",
    "def run_method(positive_seeds, negative_seeds, embeddings, transform_embeddings=False, post_densify=False,\n",
    "        method=polarity_induction_methods.densify, **kwargs):\n",
    "    \n",
    "    \n",
    "    if transform_embeddings:\n",
    "        print (\"Transforming embeddings...\")\n",
    "        embeddings = embedding_transformer.apply_embedding_transformation(embeddings, positive_seeds, negative_seeds, n_dim=50)\n",
    "    \n",
    "    \n",
    "    ## using densify method\n",
    "    if post_densify:\n",
    "        polarities = method(embeddings, positive_seeds, negative_seeds, **kwargs)\n",
    "        top_pos = [word for word in \n",
    "                sorted(polarities, key = lambda w : -polarities[w])[:150]]\n",
    "        top_neg = [word for word in \n",
    "                sorted(polarities, key = lambda w : polarities[w])[:150]]\n",
    "        top_pos.extend(positive_seeds)\n",
    "        top_neg.extend(negative_seeds)\n",
    "        return polarity_induction_methods.densify(embeddings, top_pos, top_neg)\n",
    "    \n",
    "    \n",
    "    positive_seeds = [s for s in positive_seeds if s in embeddings]\n",
    "    negative_seeds = [s for s in negative_seeds if s in embeddings]\n",
    "    \n",
    "    \n",
    "    return method(embeddings, positive_seeds, negative_seeds, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seeds\n",
    "from representations.representation_factory import create_representation\n",
    "import constants\n",
    "\n",
    "def evaluate_methods():\n",
    "    \"\"\"\n",
    "    Evaluates different methods on standard English.\n",
    "    \"\"\"\n",
    "    print (\"Getting evalution words..\")\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    ## inquirer is ternrary -1,0,1\n",
    "    lexicon = load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "    \n",
    "    ## kuperman is continus -5.0 to 5\n",
    "    kuperman = load_lexicon(\"kuperman\", remove_neutral=False)\n",
    "    eval_words = set(lexicon.keys())\n",
    "\n",
    "    qwn = load_lexicon(\"qwn-scores\")\n",
    "    for word in lexicon:\n",
    "        if not word in qwn:\n",
    "            qwn[word] = 0\n",
    "\n",
    "    positive_seeds, negative_seeds = seeds.hist_seeds()\n",
    "    \n",
    "    common_embed = create_representation(\"GIGA\", constants.GLOVE_EMBEDDINGS,eval_words.union(positive_seeds).union(negative_seeds))\n",
    "    \n",
    "    \n",
    "    embed_words = set(common_embed.iw)\n",
    "    \n",
    "    \n",
    "    eval_words = eval_words.intersection(embed_words)\n",
    "\n",
    "    eval_words = [word for word in eval_words \n",
    "            if not word in positive_seeds \n",
    "            and not word in negative_seeds]\n",
    "    \n",
    "    \n",
    "    print (\"Evaluating with \", len(eval_words), \"out of\", len(lexicon))\n",
    "    print (\"SentProp:\")\n",
    "    \n",
    "    \n",
    "    polarities = run_method(positive_seeds, negative_seeds, \n",
    "            common_embed.get_subembed(set(eval_words).union(negative_seeds).union(positive_seeds)),\n",
    "            method=polarity_induction_methods.label_propagate_probabilistic,beta=0.99, nn=10)\n",
    "    \n",
    "    \n",
    "    evaluate(polarities, lexicon, eval_words, tau_lexicon=kuperman)\n",
    "    socialsent_util.write_pickle(polarities, \"tmp/gi-cc-walk-pols.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is using evaluate_methods() \n",
    "\n",
    "### THIS IS FOR THE STANDARD ENGLISH ACCORDING TO THE PAPER.\n",
    "### it evaluates custom induced \"inquirer\" and \"kuberman\"\n",
    "\n",
    "\n",
    "### THE WORKFLOW IS AS FOLLOWS : \n",
    "\n",
    "\n",
    "### DOMAIN-CORPUS---> WORD_EMBEDDINGS+SEEDS(POSITIVE+NEGATIVE) ---> METHOD(LABEL_PROPAGATION) ---> NEW LEXICON (NEW POLARITY SCORES WHICH IS SENSITIVE TO THE CONTEXT DOMAIN)\n",
    "\n",
    "\n",
    "#### IN THE END, THESE LEXICONS CAN BE USED TO FIND THE SENTIMENT OF THE WHOLE SENTENCE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting evalution words..\n",
      "Evaluating with  8528 out of 8640\n",
      "SentProp:\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.1549\n",
      "ROC AUC Score: 0.8001\n",
      "Average Precision Score: 0.7663\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.2497\n",
      "Macro F1 with cmn threshold: 0.1024\n",
      "Kendall Tau 0.3498\n",
      "Confusion matrix: \n",
      "[[   0    1 1874]\n",
      " [   0    0 5106]\n",
      " [   0    0 1547]]\n",
      "Neg : 0.0\n",
      "Neut : 0.0\n",
      "Pos : 1.0\n",
      "Latex table line: 80.0 & 10.2 & 0.35\\\\\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "from evaluate_methods import evaluate\n",
    "evaluate_methods()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
