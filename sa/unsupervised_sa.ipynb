{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import itertools\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from normalization import normalize_accented_characters, html_parser, strip_html\n",
    "from utils import display_evaluation_metrics, display_confusion_matrix, display_classification_report\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('sentiwordnet')\n",
    "import dynet as dy\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie review dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  50000\n",
      "Train_X :  999\n",
      "Test_X  :  1000\n"
     ]
    }
   ],
   "source": [
    "def prepare_movie_dataset(train_start,train_end,test_start,test_end) : \n",
    "\n",
    "    dataset = pd.read_csv(r'datasets/movie_reviews.csv')\n",
    "    print('dataset size : ',dataset.shape[0])\n",
    "\n",
    "    train_data = dataset[train_start:train_end]\n",
    "    test_data = dataset[test_start:test_end]\n",
    "    \n",
    "    print('Train_X : ',train_data.shape[0])\n",
    "    print('Test_X  : ',test_data.shape[0])\n",
    "\n",
    "    test_reviews = np.array(test_data['review'])\n",
    "    test_sentiments = np.array(test_data['sentiment'])\n",
    "\n",
    "    return train_data,test_reviews,test_sentiments\n",
    "\n",
    "def prepare_labeled_data(train_start,train_end,test_start,test_end) : \n",
    "    \n",
    "    labeled_data=open(\"datasets/labeledTrainData.tsv\",\"r\")\n",
    "    data=labeled_data.readlines()\n",
    "    data=[d.split(\"\\t\") for d in data]\n",
    "    sa_data=pd.DataFrame(data,columns=['ind','sentiment','review'])\n",
    "    sa_data=sa_data[['sentiment','review']]\n",
    "    \n",
    "    print('dataset size : ',sa_data.shape[0])\n",
    "\n",
    "    train_data = sa_data[train_start:train_end]\n",
    "    test_data = sa_data[test_start:test_end]\n",
    "    \n",
    "    print('Train_X : ',train_data.shape[0])\n",
    "    print('Test_X  : ',test_data.shape[0])\n",
    "\n",
    "    test_reviews = np.array(test_data['review'])\n",
    "    test_sentiments = np.array(test_data['sentiment'])\n",
    "\n",
    "    return train_data,test_reviews,test_sentiments\n",
    "\n",
    "train_x,test_x,test_y=prepare_movie_dataset(1,1000,1000,2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation for unsupervised Lexicon sentiment tagging\n",
    "\n",
    "#### compare against the sentence tagging (already provided in the dataset )\n",
    "\n",
    "[add markdown #11 here]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BORROWED FROM THE AR_SARKAR METRIC\n",
    "def analyze_sentiment_sentiwordnet_lexicon(review,verbose=False):\n",
    "    \n",
    "    \n",
    "    #review = normalize_accented_characters(review)\n",
    "    #review = review.decode('utf-8')\n",
    "    review = html_parser.unescape(review)\n",
    "    review = strip_html(review)\n",
    "    \n",
    "    text_tokens = nltk.word_tokenize(review)\n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "\n",
    "    for word, tag in tagged_text:\n",
    "        ss_set = None\n",
    "        if 'NN' in tag and swn.senti_synsets(word, 'n'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'n'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'VB' in tag and swn.senti_synsets(word, 'v'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'v'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'JJ' in tag and swn.senti_synsets(word, 'a'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'a'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        elif 'RB' in tag and swn.senti_synsets(word, 'r'):\n",
    "            ss_set = list(swn.senti_synsets(word, 'r'))\n",
    "            if ss_set : \n",
    "                ss_set=ss_set[0]\n",
    "        \n",
    "        if ss_set:\n",
    "            \n",
    "            pos_score += ss_set.pos_score()\n",
    "            neg_score += ss_set.neg_score()\n",
    "            obj_score += ss_set.obj_score()\n",
    "            token_count += 1\n",
    "    \n",
    "    \n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_obj_score = round(float(obj_score) / token_count, 2)\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        \n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment, norm_obj_score,\n",
    "                                         norm_pos_score, norm_neg_score,\n",
    "                                         norm_final_score]],\n",
    "                                         columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment', 'Objectivity',\n",
    "                                                                       'Positive', 'Negative', 'Overall']], \n",
    "                                                              labels=[[0,0,0,0,0],[0,1,2,3,4]]))\n",
    "        print (sentiment_frame)   \n",
    "    return final_sentiment\n",
    "            \n",
    "                                                               \n",
    "def evaluate_lexicons(TRUE_LABELS,PREDICTED_LABELS,POS_CLASS,NEG_CLASS) : \n",
    "\n",
    "    print ('Performance metrics:')\n",
    "    display_evaluation_metrics(true_labels=TRUE_LABELS,\n",
    "                               predicted_labels=PREDICTED_LABELS,\n",
    "                               positive_class=str(POS_CLASS))  \n",
    "    print ('\\nConfusion Matrix:'             )              \n",
    "    display_confusion_matrix(true_labels=TRUE_LABELS,\n",
    "                             predicted_labels=PREDICTED_LABELS,\n",
    "                             classes=[str(POS_CLASS),str(NEG_CLASS)])\n",
    "    print ('\\nClassification report:' )                        \n",
    "    display_classification_report(true_labels=TRUE_LABELS,\n",
    "                                  predicted_labels=PREDICTED_LABELS,\n",
    "                                  classes=[str(POS_CLASS),str(NEG_CLASS)])\n",
    "    return\n",
    "\n",
    "                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basline lexicon evaluation\n",
    "\n",
    "#### movie dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  50000\n",
      "Train_X :  1000\n",
      "Test_X  :  1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-139cf7a8ba38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_movie_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentiwordnet_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_sentiwordnet_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mevaluate_lexicons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiwordnet_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-139cf7a8ba38>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_movie_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msentiwordnet_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_sentiwordnet_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mevaluate_lexicons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msentiwordnet_predictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'positive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'negative'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-6512750e98c6>\u001b[0m in \u001b[0;36manalyze_sentiment_sentiwordnet_lexicon\u001b[0;34m(review, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mss_set\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mpos_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mss_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mneg_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mss_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mobj_score\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mss_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_x,test_x,test_y=prepare_movie_dataset(0,1000,1000,2000)\n",
    "sentiwordnet_predictions = [analyze_sentiment_sentiwordnet_lexicon(review) for review in test_x]\n",
    "evaluate_lexicons(test_y.tolist(),sentiwordnet_predictions,'positive','negative')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### labeled dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,test_x,test_y=prepare_labeled_data(0,1000,1000,2000)\n",
    "sentiwordnet_predictions = [analyze_sentiment_sentiwordnet_lexicon(review) for review in test_x]\n",
    "binary_predicted=['1' if p=='positive' else '0' for p in sentiwordnet_predictions ]\n",
    "evaluate_lexicons(test_y.tolist(),binary_predicted,'1','0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple network for learning (do afterwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loss before step is: 1.0331029891967773\n",
      "the loss after step is: 0.7983773350715637\n"
     ]
    }
   ],
   "source": [
    "## SIMPLE NETWORK WITH THE sigma(V*tanh(WX+B)) ## for the XOR problem\n",
    "# create a parameter collection and add the parameters.\n",
    "m = dy.ParameterCollection()\n",
    "W = m.add_parameters((8,2))\n",
    "V = m.add_parameters((1,8))\n",
    "b = m.add_parameters((8))\n",
    "\n",
    "dy.renew_cg() # new computation graph. not strictly needed here, but good practice.\n",
    "b.value() ## bias values\n",
    "x=dy.vecInput(2) ## 2 sized inputs \n",
    "output=dy.logistic(V*(dy.tanh(W*x)+b)) ## output node\n",
    "\n",
    "y = dy.scalarInput(0) ## objective function\n",
    "loss = dy.binary_log_loss(output,y) ## loss function\n",
    "## trainer with the initialized parameters m \n",
    "trainer=dy.SimpleSGDTrainer(m)\n",
    "x.set([1,0])\n",
    "y.set(1)\n",
    "loss_value = loss.value() # this performs a forward through the network.\n",
    "print(\"the loss before step is:\",loss_value)\n",
    "\n",
    "loss.backward()  # compute the gradients\n",
    "trainer.update()\n",
    "\n",
    "loss_value = loss.value(recalculate=True) \n",
    "print(\"the loss after step is:\",loss_value)\n",
    "pc = dy.ParameterCollection()\n",
    "NUM_LAYERS=2\n",
    "INPUT_DIM=50\n",
    "HIDDEN_DIM=10\n",
    "builder = dy.LSTMBuilder(NUM_LAYERS, INPUT_DIM, HIDDEN_DIM, pc)\n",
    "s0 = builder.initial_state()\n",
    "x1 = dy.vecInput(INPUT_DIM)\n",
    "s1=s0.add_input(x1)\n",
    "y1 = s1.output()\n",
    "s2=s1.add_input(x1) # we can add another input\n",
    "y2=s2.output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTING 2.1 SECTION OF GOOGLE PAPER FOR LEXICON EXPANSION [this is for the lexicon expansion ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_WORDS=list(wn.words())\n",
    "SCORES=defaultdict()\n",
    "df=pd.DataFrame(ALL_WORDS,columns=['word'])\n",
    "\n",
    "swn.all_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the lexicons from stanford paper \"Incuding Domain-Specific Sentiment Lexicons from Unalabeled Copora\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socialsent_util\n",
    "def load_lexicon(name, remove_neutral=True):\n",
    "    lexicon = socialsent_util.load_json(\"./lexicons_socialsent/\"+ name + '.json')\n",
    "    return {w: p for w, p in lexicon.items() if p != 0} if remove_neutral else lexicon\n",
    "\n",
    "def compare_lexicons_binary(print_disagreements=False):\n",
    "    lexicons = {\n",
    "        \"inquirer\": load_lexicon(\"inquirer\", False),\n",
    "        \"mpqa\": load_lexicon(\"mpqa\", False),\n",
    "        \"bingliu\": load_lexicon(\"bingliu\", False),\n",
    "    }\n",
    "\n",
    "    for l in lexicons:\n",
    "        print( l, len(lexicons[l]), len([w for w in lexicons[l] if lexicons[l][w] != 0]))\n",
    "\n",
    "    for l1, l2 in itertools.combinations(lexicons.keys(), 2):\n",
    "        ps1, ps2 = lexicons[l1], lexicons[l2]\n",
    "        common_words = set(ps1.keys()) & set(ps2.keys())\n",
    "        print( l1, l2, \"agreement: {:.2f}\".format(\n",
    "            100.0 * sum(1 if ps1[w] == ps2[w] else 0 for w in common_words) / len(common_words)))\n",
    "        common_words = set([word for word in ps1.keys() if ps1[word] != 0]) & \\\n",
    "                       set([word for word in ps2.keys() if ps2[word] != 0])  \n",
    "        print (l1, l2, \"agreement ignoring neutral: {:.2f}\".format(\n",
    "            100.0 * sum(1 if ps1[w] * ps2[w] == 1 else 0 for w in common_words) / len(common_words)))\n",
    "        \n",
    "        if print_disagreements and l1 == 'opinion' and l2 == 'inquirer':\n",
    "            for w in common_words:\n",
    "                if lexicons[l1][w] != lexicons[l2][w]:\n",
    "                    print (w, lexicons[l1][w], lexicons[l2][w])\n",
    "      \n",
    "    \n",
    "## ALL THESE LEXICONS ARE 2-CLASS SENTIMENTS. 1 = POSITIVE; -1 = NEGATIVE\n",
    "finance_lexicons=load_lexicon('finance')\n",
    "bingliu_lexicons=load_lexicon('bingliu')\n",
    "inquirer_lexicons=load_lexicon('inquirer')\n",
    "mpqa_lexicons=load_lexicon('mpqa')\n",
    "twitter_lexicons=load_lexicon('twitter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ZEEYANG_LEXICONS='lexicons_zeeyang'\n",
    "def read_zeeyang_lexicons(fname) : \n",
    "    \n",
    "    polarities=defaultdict()\n",
    "    for line in open(fname,'r') : \n",
    "        token=line.split(\" \")[0]\n",
    "        score=line.split(\" \")[1]\n",
    "        polarities[token]=score\n",
    "        \n",
    "    return polarities\n",
    "\n",
    "## THESE LEXICONS HAVE CONTINOUS SCORES (BETWEEN -1 AND 1 )\n",
    "senti140_lexicons=read_zeeyang_lexicons(ZEEYANG_LEXICONS+\"/sentiment140.lex\")\n",
    "sentiwn_lexicons=read_zeeyang_lexicons(ZEEYANG_LEXICONS+\"/sentiwordnet.lex\")\n",
    "sst_lexicons=read_zeeyang_lexicons(ZEEYANG_LEXICONS+\"/stanford.tree.lexicon\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polarity values of imported lexicons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POLARTTY VALUES OF IMPORTED LEXICONS\n",
      "Finance\n",
      "[-1  1]\n",
      "Bingliu\n",
      "[-1  1]\n",
      "Inquirer\n",
      "[-1  1]\n",
      "Twitter\n",
      "[-1  1]\n",
      "Senti140\n",
      "['-1.250000\\n' '-0.798000\\n' '0.049000\\n' ... '-1.033000\\n' '-1.876000\\n'\n",
      " '-2.000000\\n']\n",
      "SentiWordNet\n",
      "['2.250000\\n' '0.750000\\n' '1.750000\\n' '3.250000\\n' '1.000000\\n'\n",
      " '0.250000\\n' '1.250000\\n' '2.500000\\n' '0.500000\\n' '1.500000\\n'\n",
      " '3.000000\\n' '2.750000\\n' '3.500000\\n' '3.750000\\n' '4.000000\\n'\n",
      " '0.000000\\n']\n",
      "SST\n",
      "['2\\n' '3\\n' '1\\n' '4\\n' '0\\n']\n",
      "========== Finance ========== \n",
      "2709\n",
      "========== Bingliu ========== \n",
      "6785\n",
      "========== Inquirer ========== \n",
      "3457\n",
      "========== Twitter ========== \n",
      "1277\n",
      "========== Senti140 ========== \n",
      "62468\n",
      "========== Sentiwordnet ========== \n",
      "32980\n",
      "========== SST ========== \n",
      "19465\n"
     ]
    }
   ],
   "source": [
    "print(\"POLARTTY VALUES OF IMPORTED LEXICONS\")\n",
    "print(\"Finance\")\n",
    "print(pd.DataFrame(list(finance_lexicons.values()),columns=['score'])['score'].unique())\n",
    "print(\"Bingliu\")\n",
    "print(pd.DataFrame(list(bingliu_lexicons.values()),columns=['score'])['score'].unique())\n",
    "print(\"Inquirer\")\n",
    "print(pd.DataFrame(list(inquirer_lexicons.values()),columns=['score'])['score'].unique())\n",
    "print(\"Twitter\")\n",
    "print(pd.DataFrame(list(twitter_lexicons.values()),columns=['score'])['score'].unique())\n",
    "\n",
    "print(\"Senti140\")\n",
    "print(pd.DataFrame(list(senti140_lexicons.values()),columns=['score'])['score'].unique())\n",
    "print(\"SentiWordNet\")\n",
    "print(pd.DataFrame(list(sentiwn_lexicons.values()),columns=['score'])['score'].unique())\n",
    "print(\"SST\")\n",
    "print(pd.DataFrame(list(sst_lexicons.values()),columns=['score'])['score'].unique())\n",
    "\n",
    "\n",
    "LEXICON_LIST=[finance_lexicons,bingliu_lexicons,inquirer_lexicons,twitter_lexicons,senti140_lexicons,sentiwn_lexicons,sst_lexicons]\n",
    "LEXICON_LABELS=['Finance','Bingliu','Inquirer','Twitter','Senti140','Sentiwordnet','SST']\n",
    "for i,l in enumerate(LEXICON_LIST) : \n",
    "    \n",
    "    \n",
    "    print(\"========== {} ========== \".format(LEXICON_LABELS[i]))\n",
    "    print(len(LEXICON_LIST[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compares the different lexicon repositories through the mutual information between them (common words)\n",
    "\n",
    "### The comparison is done through looking for words in two lexicon dictionaries L1 AND L2, and how many words are common in them which have the same scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inquirer 8640 3457\n",
      "mpqa 6886 6462\n",
      "bingliu 6785 6785\n",
      "inquirer mpqa agreement: 82.47\n",
      "inquirer mpqa agreement ignoring neutral: 98.50\n",
      "inquirer bingliu agreement: 84.39\n",
      "inquirer bingliu agreement ignoring neutral: 98.74\n",
      "mpqa bingliu agreement: 99.19\n",
      "mpqa bingliu agreement ignoring neutral: 99.44\n"
     ]
    }
   ],
   "source": [
    "## COMPARING THE BINARY LEXICONS \n",
    "compare_lexicons_binary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicon Induction : the idea is to generate the lexicons provided the corpus. This method makes sure that the lexicon are sensitive to the context they are drawn from. They may prove useful if we would like to assess them in a simiar context. For instance, financial lexicons will reflect better sentiments than using general lexicons such as SentiWordNet. Three ways purposed for induction \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POLARITY INDUCTION METHOD : This is used for re-scoring of the lexicons(tokens) by taking information from the word-embeddings (domain-specific), positive and the negative seed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polarity_induction_methods\n",
    "\n",
    "### THIS IS THE FUNCTION FOR INDUCING LEXICONS GIVEN THE SEEDS, EMBEDDINGS AND THE METHOD.\n",
    "def run_method(positive_seeds, negative_seeds, embeddings, transform_embeddings=False, post_densify=False,\n",
    "        method=polarity_induction_methods.densify, **kwargs):\n",
    "    \n",
    "    print(\"THE INTERNAL RUN_METHOD IS RUNNING...\")\n",
    "    \n",
    "    if transform_embeddings:\n",
    "        print (\"Transforming embeddings...\")\n",
    "        embeddings = embedding_transformer.apply_embedding_transformation(embeddings, positive_seeds, negative_seeds, n_dim=50)\n",
    "    \n",
    "    print(\"AFTER EMBEDDING TRANSFORM \",embeddings)\n",
    "    \n",
    "    ## using densify method\n",
    "    if post_densify:\n",
    "        polarities = method(embeddings, positive_seeds, negative_seeds, **kwargs)\n",
    "        top_pos = [word for word in \n",
    "                sorted(polarities, key = lambda w : -polarities[w])[:150]]\n",
    "        top_neg = [word for word in \n",
    "                sorted(polarities, key = lambda w : polarities[w])[:150]]\n",
    "        top_pos.extend(positive_seeds)\n",
    "        top_neg.extend(negative_seeds)\n",
    "        return polarity_induction_methods.densify(embeddings, top_pos, top_neg)\n",
    "    \n",
    "    \n",
    "    positive_seeds = [s for s in positive_seeds if s in embeddings]\n",
    "    negative_seeds = [s for s in negative_seeds if s in embeddings]\n",
    "    \n",
    "    \n",
    "    return method(embeddings, positive_seeds, negative_seeds, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LEXICON INDUCTION ON STANDARD ENGLISH "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_new_lexicon_polarities(parent_lexicon,positive_seeds,negative_seeds,technique) : \n",
    "    \n",
    "    ## CHOOSE SEEDS : \n",
    "\n",
    "    POSITIVE_SEEDS = [\"good\", \"lovely\", \"excellent\", \"fortunate\", \"pleasant\", \"delightful\", \"perfect\", \"loved\", \"love\", \"happy\"] \n",
    "    NEGATIVE_SEEDS = [\"bad\", \"horrible\", \"poor\",  \"unfortunate\", \"unpleasant\", \"disgusting\", \"evil\", \"hated\", \"hate\", \"unhappy\"]\n",
    "\n",
    "    ## LOAD THE WORD-EMBEDDINGS : \n",
    "\n",
    "    eval_words = set(parent_lexicon.keys())\n",
    "\n",
    "    EMBEDDING_TYPE = constants.GLOVE_EMBEDDINGS\n",
    "    EMBEDDING = create_representation(\"GIGA\", constants.GLOVE_EMBEDDINGS,eval_words.union(POSITIVE_SEEDS).union(NEGATIVE_SEEDS))\n",
    "\n",
    "    embed_words = set(EMBEDDING.iw)\n",
    "    eval_words = eval_words.intersection(EMBEDDING)\n",
    "    eval_words = [word for word in eval_words  if not word in POSITIVE_SEEDS and not word in NEGATIVE_SEEDS]\n",
    "\n",
    "    ## TRAIN THE BEST ALGORITHM : SENTPROP and get polarities re-scored\n",
    "    \n",
    "    polarities=defaultdict()\n",
    "    if technique=='label_propagate_prob' : \n",
    "        \n",
    "        polarities = run_method(POSITIVE_SEEDS, NEGATIVE_SEEDS, \n",
    "                    EMBEDDING.get_subembed(set(eval_words).union(NEGATIVE_SEEDS).union(POSITIVE_SEEDS)),\n",
    "                    method=polarity_induction_methods.label_propagate_probabilistic,beta=0.99, nn=10)\n",
    "    \n",
    "    elif technique == 'pmi' : \n",
    "        \n",
    "        EMBEDDING = create_representation(\"Explicit\", constants.GLOVE_EMBEDDINGS)\n",
    "        hist_counts = EMBEDDING.get_subembed(set(eval_words).union(positive_seeds).union(negative_seeds),restrict_context=False)\n",
    "        \n",
    "        \n",
    "        print(dir(EMBEDDING))\n",
    "        \n",
    "        polarities = run_method(positive_seeds, negative_seeds,\n",
    "                hist_counts,\n",
    "                method=polarity_induction_methods.bootstrap,\n",
    "                score_method=polarity_induction_methods.pmi)\n",
    "    \n",
    "    return polarities,eval_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method_performance(polarities,INITIAL_LEXICON_LIB,domain,eval_words) : \n",
    "    \n",
    "    ## EVALUATING THE EFFECTIVENESS OF THE NEW LEXICON POLARITIES.\n",
    "    from evaluate_methods import binary_metrics,ternary_metrics\n",
    "\n",
    "    acc, auc, avg_prec = binary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words)\n",
    "    if auc < 0.5:\n",
    "        polarities = {word:-1*polarities[word] for word in polarities}\n",
    "        acc, auc, avg_prec = binary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words)\n",
    "\n",
    "    print(\"============== DOMAIN : {} ==============\".format(domain))\n",
    "    print (\"Binary metrics:\")\n",
    "    print( \"==============\")\n",
    "    print (\"Accuracy with optimal threshold: {:.4f}\".format(acc))\n",
    "    print (\"ROC AUC Score: {:.4f}\".format(auc))\n",
    "    print (\"Average Precision Score: {:.4f}\".format(avg_prec))\n",
    "\n",
    "\n",
    "    tau, cmn_f1, maj_f1, conf_mat = ternary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words, tau_lexicon=THREE_WAY_LEXICON)\n",
    "    print (\"Ternary metrics:\")\n",
    "    print( \"==============\")\n",
    "    print (\"Majority macro F1 baseline {:.4f}\".format(maj_f1))\n",
    "    print (\"Macro F1 with cmn threshold: {:.4f}\".format(cmn_f1))\n",
    "    if tau:\n",
    "        print (\"Kendall Tau {:.4f}\".format(tau))\n",
    "    print (\"Confusion matrix: \")\n",
    "    print (conf_mat)\n",
    "    print( \"Neg :\", float(conf_mat[0,0]) / np.sum(conf_mat[0,:]))\n",
    "    print (\"Neut :\", float(conf_mat[1,1]) / np.sum(conf_mat[1,:]))\n",
    "    print (\"Pos :\", float(conf_mat[2,2]) / np.sum(conf_mat[2,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from representations.representation_factory import create_representation\n",
    "import constants\n",
    "from evaluate_methods import run_method\n",
    "import polarity_induction_methods\n",
    "\n",
    "\n",
    "## TRAINING THE LABEL-PROPAGATION FOR THE RE-SCORING OF POLARITIES FROM PRE-DETERMINED LEXICONS (MADE FROM WORD EMBEDDINGS)\n",
    "\n",
    "INQUIRER = load_lexicon(\"inquirer\", remove_neutral=False)\n",
    "\n",
    "FINANCE_LEXICONS=load_lexicon('finance')\n",
    "TWITTER_LEXICONS=load_lexicon('twitter')\n",
    "\n",
    "THREE_WAY_LEXICON = kuperman = load_lexicon(\"kuperman\", remove_neutral=False)\n",
    "\n",
    "POSITIVE_FINANCE = [\"successful\", \"excellent\", \"profit\", \"beneficial\", \"improving\", \"improved\", \"success\", \"gains\", \"positive\"]\n",
    "NEGATIVE_FINANCE = [\"negligent\", \"loss\", \"volatile\", \"wrong\", \"losses\", \"damages\", \"bad\", \"litigation\", \"failure\", \"down\", \"negative\"]\n",
    "\n",
    "\n",
    "POSITIVE_SE = [\"good\", \"lovely\", \"excellent\", \"fortunate\", \"pleasant\", \"delightful\", \"perfect\", \"loved\", \"love\", \"happy\"] \n",
    "NEGATIVE_SE = [\"bad\", \"horrible\", \"poor\",  \"unfortunate\", \"unpleasant\", \"disgusting\", \"evil\", \"hated\", \"hate\", \"unhappy\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training financial and standard-english lexicons with glove embeddings using probabilistic label propagation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finance lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== DOMAIN : FINANCE ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.7899\n",
      "ROC AUC Score: 0.9598\n",
      "Average Precision Score: 0.8393\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.4642\n",
      "Macro F1 with cmn threshold: 0.1180\n",
      "Kendall Tau 0.3777\n",
      "Confusion matrix: \n",
      "[[   0    1 2246]\n",
      " [   0    0    0]\n",
      " [   0    0  347]]\n",
      "Neg : 0.0\n",
      "Neut : nan\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finance_polarities,finance_eval=calculate_new_lexicon_polarities(FINANCE_LEXICONS,POSITIVE_FINANCE,NEGATIVE_FINANCE,'label_propagate_prob')\n",
    "evaluate_method_performance(finance_polarities,FINANCE_LEXICONS,'FINANCE',finance_eval)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== DOMAIN :  STANDARD ENGLISH  ==============\n",
      "Binary metrics:\n",
      "==============\n",
      "Accuracy with optimal threshold: 1.1631\n",
      "ROC AUC Score: 0.8072\n",
      "Average Precision Score: 0.7743\n",
      "Ternary metrics:\n",
      "==============\n",
      "Majority macro F1 baseline 0.2497\n",
      "Macro F1 with cmn threshold: 0.1024\n",
      "Kendall Tau 0.3625\n",
      "Confusion matrix: \n",
      "[[   0    1 1874]\n",
      " [   0    0 5106]\n",
      " [   0    0 1547]]\n",
      "Neg : 0.0\n",
      "Neut : 0.0\n",
      "Pos : 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_english_polarities,se_eval=calculate_new_lexicon_polarities(INQUIRER,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob')\n",
    "evaluate_method_performance(standard_english_polarities,INQUIRER,' STANDARD ENGLISH ',se_eval)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent140_polarities,s140eval = calculate_new_lexicon_polarities(senti140_lexicons,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiwn_polarities,swn_eval = calculate_new_lexicon_polarities(sentiwn_lexicons,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sst_lexicons,sst_eval = calculate_new_lexicon_polarities(sst_lexicons,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here I tried to make a dictionary of all sentiwordnet and use the evaluation on them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=swn.all_senti_synsets()\n",
    "SENTI_LEXICONS=defaultdict()\n",
    "\n",
    "for s in ss : \n",
    "    \n",
    "    lemmas = s.synset.lemma_names()\n",
    "    positive=s.pos_score()\n",
    "    \n",
    "    negative=s.neg_score()\n",
    "    \n",
    "    for l in lemmas : \n",
    "        net_sentiment=positive-negative\n",
    "        ## CONVERTING TO BINARY SENTIMENTS \n",
    "        if net_sentiment>0 : \n",
    "            SENTI_LEXICONS[l]=1\n",
    "        elif net_sentiment<0:\n",
    "            SENTI_LEXICONS[l]=-1\n",
    "            \n",
    "            \n",
    "## RIGHT NOW, RUNNING THIS CRASHES THE KERNEL, PROBABLY NEEDS TO BE RUN SEPARATELY ON A PYTHON FILE I GUESS.\n",
    "#senti_polarities,senti_eval=calculate_new_lexicon_polarities(SENTI_LEXICONS,POSITIVE_SE,NEGATIVE_SE,'label_propagate_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EVALUATING THE EFFECTIVENESS OF THE NEW LEXICON POLARITIES.\n",
    "\n",
    "#### Calculates the ROC auc scores with the new polarities comparing to the earlier lexicon binary classification (1 = positive and 0 = negative).\n",
    "#### Interpretation of the score. \n",
    "#### Higher the score, it means that new polarities (continous sentiment scores) confirms with the binary sentiment scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TASKS IN THE DOC : \n",
    "\n",
    "## 1. EXPLAIN ABOUT THE LEXICON INDUCING : LABEL PROPAGATION ALGORITHM\n",
    "## 2. GIVES SOME UNDERLYING MATHEMATICS FROM THE RESEARCH PAPER REGARDING HOW SCORES ARE COMPUTED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_method_performance(polarities,INITIAL_LEXICON_LIB,domain,eval_words) : \n",
    "    \n",
    "    ## EVALUATING THE EFFECTIVENESS OF THE NEW LEXICON POLARITIES.\n",
    "    from evaluate_methods import binary_metrics,ternary_metrics\n",
    "\n",
    "    acc, auc, avg_prec = binary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words)\n",
    "    if auc < 0.5:\n",
    "        polarities = {word:-1*polarities[word] for word in polarities}\n",
    "        acc, auc, avg_prec = binary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words)\n",
    "\n",
    "    print(\"============== DOMAIN : {} ==============\".format(domain))\n",
    "    print (\"Binary metrics:\")\n",
    "    print( \"==============\")\n",
    "    print (\"Accuracy with optimal threshold: {:.4f}\".format(acc))\n",
    "    print (\"ROC AUC Score: {:.4f}\".format(auc))\n",
    "    print (\"Average Precision Score: {:.4f}\".format(avg_prec))\n",
    "\n",
    "\n",
    "    tau, cmn_f1, maj_f1, conf_mat = ternary_metrics(polarities, INITIAL_LEXICON_LIB, eval_words, tau_lexicon=THREE_WAY_LEXICON)\n",
    "    print (\"Ternary metrics:\")\n",
    "    print( \"==============\")\n",
    "    print (\"Majority macro F1 baseline {:.4f}\".format(maj_f1))\n",
    "    print (\"Macro F1 with cmn threshold: {:.4f}\".format(cmn_f1))\n",
    "    if tau:\n",
    "        print (\"Kendall Tau {:.4f}\".format(tau))\n",
    "    print (\"Confusion matrix: \")\n",
    "    print (conf_mat)\n",
    "    print( \"Neg :\", float(conf_mat[0,0]) / np.sum(conf_mat[0,:]))\n",
    "    print (\"Neut :\", float(conf_mat[1,1]) / np.sum(conf_mat[1,:]))\n",
    "    print (\"Pos :\", float(conf_mat[2,2]) / np.sum(conf_mat[2,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_method_performance(standard_english_polarities,INQUIRER,' STANDARD ENGLISH ',se_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BASELINE : On the Movie Reviews using senti-wordnet lexicons\n",
    "\n",
    "-  Accuracy: 0.6\n",
    "-  Precision: 0.56\n",
    "-  Recall: 0.93\n",
    "-  F1 Score: 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is to compute the effectiveness of binary sentiment scores provided a lexicon library.\n",
    "### This can be used to see which lexicon libraries help achieving the closest sentiment scores.\n",
    "### Thus a supervised algorithm and evaluation is the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUTS : \n",
    "## review = single sentence \n",
    "## lexicon_dict = dict of the lexicon with key as word and value as the polarity\n",
    "\n",
    "def analyze_sentiment_domain(review,lexicon_dict,verbose=False):\n",
    "    \n",
    "    \n",
    "    #review = normalize_accented_characters(review)\n",
    "    #review = review.decode('utf-8')\n",
    "    review = html_parser.unescape(review)\n",
    "    review = strip_html(review)\n",
    "    \n",
    "    text_tokens = nltk.word_tokenize(review)\n",
    "    tagged_text = nltk.pos_tag(text_tokens)\n",
    "    pos_score = neg_score = token_count = obj_score = 0\n",
    "\n",
    "    ## postitve polarity counts as positive and negative polarities counts as negative\n",
    "    \n",
    "    \n",
    "    for token in text_tokens : \n",
    "        \n",
    "        if token in lexicon_dict : \n",
    "            \n",
    "            if lexicon_dict[token]>0 : \n",
    "                pos_score+=1\n",
    "            elif lexicon_dict[token]<0:\n",
    "                neg_score+=1\n",
    "\n",
    "        token_count+=1\n",
    "            \n",
    "    final_score = pos_score - neg_score\n",
    "    norm_final_score = round(float(final_score) / token_count, 2)\n",
    "    final_sentiment = 'positive' if norm_final_score >= 0 else 'negative'\n",
    "    if verbose:\n",
    "        norm_pos_score = round(float(pos_score) / token_count, 2)\n",
    "        norm_neg_score = round(float(neg_score) / token_count, 2)\n",
    "        \n",
    "        \n",
    "        \n",
    "        sentiment_frame = pd.DataFrame([[final_sentiment,\n",
    "                                         norm_pos_score, norm_neg_score,\n",
    "                                         norm_final_score]],\n",
    "                                         columns=pd.MultiIndex(levels=[['SENTIMENT STATS:'], \n",
    "                                                                      ['Predicted Sentiment',\n",
    "                                                                       'Positive', 'Negative', 'Overall']], \n",
    "                                                              labels=[[0,0,0,0],[0,1,2,3]]))\n",
    "        print (sentiment_frame)   \n",
    "    return final_sentiment\n",
    "            \n",
    "                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset size :  50000\n",
      "Train_X :  1000\n",
      "Test_X  :  1000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'polarities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-36c7667d8831>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_movie_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentiwordnet_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#evaluate_lexicons(test_y.tolist(),sentiwordnet_predictions,'positive','negative')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-36c7667d8831>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepare_movie_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msentiwordnet_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0manalyze_sentiment_domain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolarities\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#evaluate_lexicons(test_y.tolist(),sentiwordnet_predictions,'positive','negative')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'polarities' is not defined"
     ]
    }
   ],
   "source": [
    "train_x,test_x,test_y=prepare_movie_dataset(0,1000,1000,2000)\n",
    "\n",
    "sentiwordnet_predictions = [analyze_sentiment_domain(review,polarities) for review in test_x]\n",
    "#evaluate_lexicons(test_y.tolist(),sentiwordnet_predictions,'positive','negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO DO : \n",
    "\n",
    "## 1. COMPUTE THESE SCORES AGAIN WITH THE NEW DOMAIN SPECIFIC (GLOVE BASED) EMBEDDINGS TO SEE SENTIMENTAL SCORE CHANGE.\n",
    "## 2. GET THE TWITTER DATASET  AND DO AGAIN THE SAME THING. (TWITTER EMBEDDINGS/GLOVE EMBEDDINGS/SENTIWORDNET + TWITTER DATASET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR TWO DATASETS : \n",
    "### FOR THREE EMBEDDINGS : CUSTOM-MADE / GLOVE (1B) / ACTUAL-DOMAIN ONES.\n",
    "### GET THESE SCORES (2*3 MATRIX OF SCORES)\n",
    "### GET THE NEW POLARITIES AND USE THEM TO CALCULATE SENTIMENTAL SCORES(SENTENCE BASED).\n",
    "### ALSO GET THE BINARY_METRICS FOR THE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE WHOLE USE OF THE METHODS IS THAT TO GET INSIGHT INTO THE CONTEXT-SENSITIVE INFORMATION.\n",
    "\n",
    "\n",
    "### HOW? \n",
    "\n",
    "### 1. ARRANGE THE PRE-TAGGED LEXICONS (ATLEAST POSITIVE/NEGATIVE)\n",
    "\n",
    "### 2. WORD EMBEDDINGS TRAINED ON THE CONTEXT-MATERIAL. \n",
    "\n",
    "### 3. LABEL-PROPAGATE ALGORITHM TO MAKE LEXICONS SCORE LEXICONS TO THE CONTINOUS SENTIMENT SCORES.\n",
    "\n",
    "### 4. USE SUM (P+V)/T OR NEURAL NETWORK TO OBTAIN THE SCORE FOR THE WHOLE SENTENCE SENTIMENT \n",
    "\n",
    "##### Here, we earlier thought that we would be able to implement this phase if we had more time. The paper \"Context-Sensitive Lexicon Features for Neural Sentiment Analysis\" we can test baseline with normal lexicons and improvement as label-propagated lexicons with LSTM for scores, and evaluate them back on the binary classification scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117659"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
