{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework: Competitive Grammar Writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pcfg_parse_gen import Pcfg, PcfgGenerator, CkyParse\n",
    "import nltk\n",
    "\n",
    "def print_tree(tree_string):\n",
    "    tree_string = tree_string.strip()\n",
    "    tree = nltk.Tree.fromstring(tree_string)\n",
    "    tree.pretty_print()\n",
    "\n",
    "def draw_tree(tree_string):\n",
    "    tree_string = tree_string.strip()\n",
    "    tree = nltk.Tree.fromstring(tree_string)\n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing sentences with your grammar\n",
    "\n",
    "While you are developing your grammar you should parse with your grammar both example sentences and samples from your and other grammars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#reading grammar file: S1.gr\n",
      "#reading grammar file: S2.gr\n",
      "#reading grammar file: Vocab.gr\n"
     ]
    }
   ],
   "source": [
    "parse_gram = Pcfg([\"S1.gr\",\"S2.gr\",\"Vocab.gr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TOP (S1 (NP (Proper Arthur) ) (_VP (VP (VerbT is) (NP (Det the) (Nbar (Noun king) ))) (Punc .))) )\n",
      "-cross entropy: -3.7863679540999504\n",
      "             TOP              \n",
      "              |                \n",
      "              S1              \n",
      "   ___________|___             \n",
      "  |              _VP          \n",
      "  |            ___|________    \n",
      "  |           VP           |  \n",
      "  |       ____|___         |   \n",
      "  |      |        NP       |  \n",
      "  |      |     ___|___     |   \n",
      "  NP     |    |      Nbar  |  \n",
      "  |      |    |       |    |   \n",
      "Proper VerbT Det     Noun Punc\n",
      "  |      |    |       |    |   \n",
      "Arthur   is  the     king  .  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#parsing: ['Arthur', 'is', 'the', 'king', '.']\n",
      "#-cross entropy (bits/word): -3.78637\n"
     ]
    }
   ],
   "source": [
    "parser = CkyParse(parse_gram, beamsize=0.00001)\n",
    "ce, trees = parser.parse_sentences([\"Arthur is the king .\"])\n",
    "print(\"-cross entropy: {}\".format(ce))\n",
    "for tree_string in trees:\n",
    "    print_tree(tree_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TOP (S2 (_Misc (Misc five) (_Misc (Misc strangers) (_Misc (Misc are) (_Prep (Prep at) (_Det (Det the) (_Misc (Misc (_Round Round) (_Table Table)) (_Misc (Misc .) ))))))) ) )\n",
      "-cross entropy: -9.807330330570931\n",
      "        TOP                                             \n",
      "         |                                               \n",
      "         S2                                             \n",
      "         |                                               \n",
      "       _Misc                                            \n",
      "  _______|_______                                        \n",
      " |             _Misc                                    \n",
      " |        _______|__________                             \n",
      " |       |                _Misc                         \n",
      " |       |        __________|_____                       \n",
      " |       |       |              _Prep                   \n",
      " |       |       |     ___________|_____                 \n",
      " |       |       |    |                _Det             \n",
      " |       |       |    |      ___________|_____           \n",
      " |       |       |    |     |               _Misc       \n",
      " |       |       |    |     |            _____|______    \n",
      " |       |       |    |     |          Misc        _Misc\n",
      " |       |       |    |     |      _____|_____       |   \n",
      "Misc    Misc    Misc Prep  Det  _Round      _Table  Misc\n",
      " |       |       |    |     |     |           |      |   \n",
      "five strangers  are   at   the  Round       Table    .  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#parsing: ['five', 'strangers', 'are', 'at', 'the', 'Round', 'Table', '.']\n",
      "#-cross entropy (bits/word): -9.80733\n"
     ]
    }
   ],
   "source": [
    "ce, trees = parser.parse_sentences([\"five strangers are at the Round Table .\"])\n",
    "print(\"-cross entropy: {}\".format(ce))\n",
    "for tree_string in trees:\n",
    "    print_tree(tree_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `parse_file` to parse a file of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(TOP (S1 (NP (Proper Arthur) ) (_VP (VP (VerbT is) (NP (Det the) (Nbar (Noun king) ))) (Punc .))) )\n",
      "(TOP (S1 (NP (Proper Arthur) ) (_VP (VP (VerbT rides) (NP (Det the) (Nbar (Nbar (Noun horse) ) (PP (Prep near) (NP (Det the) (Nbar (Noun castle) )))))) (Punc .))) )\n",
      "(TOP (S2 (_Misc (Misc riding) (_Misc (Misc to) (_Misc (Misc Camelot) (_VerbT (VerbT is) (_Misc (Misc hard) (_Misc (Misc .) )))))) ) )\n",
      "(TOP (S2 (_Misc (Misc do) (_Misc (Misc coconuts) (_Misc (Misc speak) (_Misc (Misc ?) )))) ) )\n",
      "(TOP (S2 (_Misc (Misc what) (_Misc (Misc does) (_Proper (Proper Arthur) (_Misc (Misc ride) (_Misc (Misc ?) ))))) ) )\n",
      "(TOP (S2 (_Misc (Misc who) (_Misc (Misc does) (_Proper (Proper Arthur) (_Misc (Misc suggest) (_Misc (Misc she) (_Misc (Misc carry) (_Misc (Misc ?) ))))))) ) )\n",
      "(TOP (S2 (_Misc (Misc why) (_Misc (Misc does) (_Misc (Misc England) (_Misc (Misc have) (_Det (Det a) (_Noun (Noun king) (_Misc (Misc ?) ))))))) ) )\n",
      "(TOP (S2 (_Misc (Misc are) (_Misc (Misc they) (_Misc (Misc suggesting) (_Proper (Proper Arthur) (_Misc (Misc ride) (_Misc (Misc to) (_Misc (Misc Camelot) (_Misc (Misc ?) )))))))) ) )\n",
      "(TOP (S2 (_Misc (Misc five) (_Misc (Misc strangers) (_Misc (Misc are) (_Prep (Prep at) (_Det (Det the) (_Misc (Misc (_Round Round) (_Table Table)) (_Misc (Misc .) ))))))) ) )\n",
      "(TOP (S2 (_Proper (Proper Guinevere) (_Misc (Misc might) (_Misc (Misc have) (_Misc (Misc known) (_Misc (Misc .) ))))) ) )\n",
      "(TOP (S2 (_Proper (Proper Guinevere) (_Misc (Misc should) (_Misc (Misc be) (_Misc (Misc riding) (_Prep (Prep with) (_Proper (Proper Patsy) (_Misc (Misc .) ))))))) ) )\n",
      "(TOP (S2 (_Misc (Misc it) (_VerbT (VerbT is) (_Proper (Proper (_Sir Sir) (_Lancelot Lancelot)) (_Misc (Misc who) (_Misc (Misc knows) (_Proper (Proper Zoot) (_Misc (Misc !) ))))))) ) )\n",
      "(TOP (S2 (_Misc (Misc either) (_Proper (Proper Arthur) (_Misc (Misc knows) (_Misc (Misc or) (_Proper (Proper Patsy) (_Misc (Misc does) (_Misc (Misc .) ))))))) ) )\n",
      "(TOP (S2 (_Misc (Misc neither) (_Proper (Proper (_Sir Sir) (_Lancelot Lancelot)) (_Misc (Misc nor) (_Proper (Proper Guinevere) (_Misc (Misc will) (_Misc (Misc speak) (_Prep (Prep of) (_Misc (Misc it) (_Misc (Misc .) ))))))))) ) )\n",
      "(TOP (S2 (_Det (Det the) (_Misc (Misc (_Holy Holy) (_Grail Grail)) (_Misc (Misc was) (_Misc (Misc covered) (_Prep (Prep by) (_Det (Det a) (_Misc (Misc yellow) (_Noun (Noun fruit) (_Misc (Misc .) ))))))))) ) )\n",
      "(TOP (S2 (_Proper (Proper Zoot) (_Misc (Misc might) (_Misc (Misc have) (_Misc (Misc been) (_Misc (Misc carried) (_Prep (Prep by) (_Det (Det a) (_Noun (Noun swallow) (_Misc (Misc .) ))))))))) ) )\n",
      "(TOP (S2 (_Proper (Proper Arthur) (_Misc (Misc rode) (_Misc (Misc to) (_Misc (Misc Camelot) (_Misc (Misc and) (_Misc (Misc drank) (_Prep (Prep from) (_Misc (Misc his) (_Noun (Noun chalice) (_Misc (Misc .) )))))))))) ) )\n",
      "(TOP (S2 (_Misc (Misc they) (_Misc (Misc migrate) (_Misc (Misc precisely) (_Misc (Misc because) (_Misc (Misc they) (_Misc (Misc know) (_Misc (Misc they) (_Misc (Misc will) (_Misc (Misc grow) (_Misc (Misc .) )))))))))) ) )\n",
      "(TOP (S2 (_Misc (Misc do) (_Misc (Misc not) (_Misc (Misc speak) (_Misc (Misc !) )))) ) )\n",
      "(TOP (S2 (_Proper (Proper Arthur) (_Misc (Misc will) (_Misc (Misc have) (_Misc (Misc been) (_Misc (Misc riding) (_Prep (Prep for) (_Misc (Misc eight) (_Misc (Misc nights) (_Misc (Misc .) ))))))))) ) )\n",
      "(TOP (S2 (_Proper (Proper Arthur) (_Misc (Misc ,) (_Misc (Misc sixty) (_Misc (Misc inches) (_Misc (Misc ,) (_VerbT (VerbT is) (_Det (Det a) (_Misc (Misc tiny) (_Noun (Noun king) (_Misc (Misc .) )))))))))) ) )\n",
      "(TOP (S2 (_Proper (Proper Arthur) (_Misc (Misc knows) (_Proper (Proper Patsy) (_Misc (Misc ,) (_Det (Det the) (_Misc (Misc trusty) (_Noun (Noun servant) (_Misc (Misc .) )))))))) ) )\n",
      "(TOP (S2 (_Proper (Proper Arthur) (_Misc (Misc and) (_Proper (Proper Guinevere) (_Misc (Misc migrate) (_Misc (Misc frequently) (_Misc (Misc .) )))))) ) )\n",
      "(TOP (S2 (_Misc (Misc he) (_Misc (Misc knows) (_Misc (Misc what) (_Misc (Misc they) (_Misc (Misc are) (_Misc (Misc covering) (_Prep (Prep with) (_Det (Det that) (_Noun (Noun story) (_Misc (Misc .) )))))))))) ) )\n",
      "(TOP (S2 (_Proper (Proper Arthur) (_Misc (Misc suggested) (_Det (Det that) (_Det (Det the) (_Noun (Noun castle) (_Misc (Misc be) (_Misc (Misc carried) (_Misc (Misc .) )))))))) ) )\n",
      "(TOP (S2 (_Det (Det the) (_Noun (Noun king) (_Misc (Misc drank) (_Misc (Misc to) (_Det (Det the) (_Noun (Noun castle) (_Det (Det that) (_Misc (Misc was) (_Misc (Misc his) (_Noun (Noun home) (_Misc (Misc .) ))))))))))) ) )\n",
      "(TOP (S2 (_Misc (Misc when) (_Det (Det the) (_Noun (Noun king) (_VerbT (VerbT drinks) (_Misc (Misc ,) (_Proper (Proper Patsy) (_VerbT (VerbT drinks) (_Misc (Misc .) )))))))) ) )\n",
      "-cross entropy: -10.276152770660259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#parsing: ['Arthur', 'is', 'the', 'king', '.']\n",
      "#parsing: ['Arthur', 'rides', 'the', 'horse', 'near', 'the', 'castle', '.']\n",
      "#parsing: ['riding', 'to', 'Camelot', 'is', 'hard', '.']\n",
      "#parsing: ['do', 'coconuts', 'speak', '?']\n",
      "#parsing: ['what', 'does', 'Arthur', 'ride', '?']\n",
      "#parsing: ['who', 'does', 'Arthur', 'suggest', 'she', 'carry', '?']\n",
      "#parsing: ['why', 'does', 'England', 'have', 'a', 'king', '?']\n",
      "#parsing: ['are', 'they', 'suggesting', 'Arthur', 'ride', 'to', 'Camelot', '?']\n",
      "#parsing: ['five', 'strangers', 'are', 'at', 'the', 'Round', 'Table', '.']\n",
      "#parsing: ['Guinevere', 'might', 'have', 'known', '.']\n",
      "#parsing: ['Guinevere', 'should', 'be', 'riding', 'with', 'Patsy', '.']\n",
      "#parsing: ['it', 'is', 'Sir', 'Lancelot', 'who', 'knows', 'Zoot', '!']\n",
      "#parsing: ['either', 'Arthur', 'knows', 'or', 'Patsy', 'does', '.']\n",
      "#parsing: ['neither', 'Sir', 'Lancelot', 'nor', 'Guinevere', 'will', 'speak', 'of', 'it', '.']\n",
      "#parsing: ['the', 'Holy', 'Grail', 'was', 'covered', 'by', 'a', 'yellow', 'fruit', '.']\n",
      "#parsing: ['Zoot', 'might', 'have', 'been', 'carried', 'by', 'a', 'swallow', '.']\n",
      "#parsing: ['Arthur', 'rode', 'to', 'Camelot', 'and', 'drank', 'from', 'his', 'chalice', '.']\n",
      "#parsing: ['they', 'migrate', 'precisely', 'because', 'they', 'know', 'they', 'will', 'grow', '.']\n",
      "#parsing: ['do', 'not', 'speak', '!']\n",
      "#parsing: ['Arthur', 'will', 'have', 'been', 'riding', 'for', 'eight', 'nights', '.']\n",
      "#parsing: ['Arthur', ',', 'sixty', 'inches', ',', 'is', 'a', 'tiny', 'king', '.']\n",
      "#parsing: ['Arthur', 'knows', 'Patsy', ',', 'the', 'trusty', 'servant', '.']\n",
      "#parsing: ['Arthur', 'and', 'Guinevere', 'migrate', 'frequently', '.']\n",
      "#parsing: ['he', 'knows', 'what', 'they', 'are', 'covering', 'with', 'that', 'story', '.']\n",
      "#parsing: ['Arthur', 'suggested', 'that', 'the', 'castle', 'be', 'carried', '.']\n",
      "#parsing: ['the', 'king', 'drank', 'to', 'the', 'castle', 'that', 'was', 'his', 'home', '.']\n",
      "#parsing: ['when', 'the', 'king', 'drinks', ',', 'Patsy', 'drinks', '.']\n",
      "#-cross entropy (bits/word): -10.2762\n"
     ]
    }
   ],
   "source": [
    "ce, trees = parser.parse_file('example_sentences.txt')\n",
    "print(\"-cross entropy: {}\".format(ce))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating sentences with your grammar\n",
    "\n",
    "While you are developing your grammar you should generate sentences with your grammar\n",
    "to check what your grammar is doing. Try to write your grammars to that it will \n",
    "generate hard to parse sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "#reading grammar file: S1.gr\n",
      "#reading grammar file: Vocab.gr\n"
     ]
    }
   ],
   "source": [
    "gen_gram = Pcfg([\"S1.gr\",\"Vocab.gr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this sovereign drinks that castle\n",
      "no coconut covers any sun above that winter\n",
      "another weight covers this fruit .\n",
      "this story rides this husk into each swallow\n",
      "no swallow is no master\n",
      "each servant drinks that quest\n",
      "any swallow rides the king .\n",
      "this horse through this coconut has the king .\n",
      "each master drinks any quest .\n",
      "each defeater carries each coconut .\n",
      "every winter rides each castle\n",
      "that fruit covers a story .\n",
      "another pound is that king near Uther Pendragon\n",
      "each swallow is that home\n",
      "that fruit has a corner\n",
      "every quest covers a weight\n",
      "the fruit is any coconut .\n",
      "that castle has a home\n",
      "any chalice covers that sun above no horse\n",
      "this master has that husk .\n"
     ]
    }
   ],
   "source": [
    "gen = PcfgGenerator(gen_gram)\n",
    "for _ in range(20):\n",
    "    print(\" \".join(gen.generate()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using nltk for pos tagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## CREATING NewVocab.gr\n",
    "\n",
    "## have pos for already existing tokens\n",
    "## add new tokens from allowed_words and get pos for them also.\n",
    "\n",
    "import re \n",
    "vocab_file = open(\"Vocab.gr\")\n",
    "VOCAB_RULES=[]\n",
    "new_vocab_file=open(\"NewVocab.gr\",'w')\n",
    "NEW_POS=[]\n",
    "\n",
    "NEW_POS_LINE=[]\n",
    "\n",
    "ALL_POS=[]\n",
    "\n",
    "DELIMITERS=[\" \"*i for i in range(10)]\n",
    "\n",
    "for line in vocab_file : \n",
    "    if line[0] != \"#\" and re.match(r'[0-9|\\.]',line[0]) and line[0] != \" \" : ## A VALID GRAMMAR \n",
    "            \n",
    "            REGEX=r\"([0-9]*)\\ +([A-Za-z_]*)\\ +([A-Za-z_0-9\\?\\.\\,\\'\\;'\\!\\\"\\--\\-\\:]*)\"\n",
    "            contents = re.findall(REGEX,line)[0]\n",
    "            \n",
    "            weight = contents[0]\n",
    "            original_pos = contents[1]\n",
    "            token = contents[2]\n",
    "            \n",
    "            if original_pos == \"Misc\" : \n",
    "                new_pos = nltk.pos_tag([token])[0][1]  \n",
    "                new_line=re.sub(r'Misc',new_pos,line)\n",
    "                NEW_POS.append(new_pos)\n",
    "                ALL_POS.append(new_pos)\n",
    "                #print(new_pos)\n",
    "                #print(new_line)\n",
    "                new_vocab_file.write(new_line)\n",
    "                NEW_POS_LINE.append(new_line)\n",
    "            else : \n",
    "                #print(line)\n",
    "                new_vocab_file.write(line)\n",
    "                NEW_POS_LINE.append(line)\n",
    "                ALL_POS.append(original_pos)\n",
    "    else : \n",
    "        #print(line)\n",
    "        new_vocab_file.write(line)\n",
    "        NEW_POS_LINE.append(line)\n",
    "\n",
    "\n",
    "new_vocab_file.write(\"# NEW VOCABULARY FROM allowed_words.txt\\n\")\n",
    "\n",
    "allowed_word_file = open(\"allowed_words.txt\",\"r\")       \n",
    "\n",
    "for token in allowed_word_file : \n",
    "    \n",
    "    token = token.strip()\n",
    "    pos_token = nltk.pos_tag([token])[0][1]\n",
    "    token_weight = 1\n",
    "    #print(\"{}\".format(pos_token))\n",
    "    new_vocab_file.write(\"{}   {}   {}\\n\".format(token_weight,pos_token,token))\n",
    "    \n",
    "        \n",
    "\n",
    "new_vocab_file.close()\n",
    "\n",
    "## make the set of all part of speeches \n",
    "POS_set=set(ALL_POS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- PENDING  : BIGRAM FOR S2\n",
    "\n",
    "from nltk import bigrams, trigrams \n",
    "from collections import defaultdict\n",
    "example_sents_file = open('example_sentences.txt','r')\n",
    "sent_corpus = example_sents_file.readlines()\n",
    "tokenize_text = [nltk.word_tokenize(sent) for sent in sent_corpus]\n",
    "bigram_corpus = [list(bigrams(sent)) for sent in tokenize_text]\n",
    "\n",
    "\n",
    "model = defaultdict(int)\n",
    "pos_model = defaultdict(int)\n",
    "\n",
    "\n",
    "## COMPUTING BIGRAMS\n",
    "for i in range(len(bigram_corpus)) : \n",
    "    for w1, w2 in bigram_corpus[i]:\n",
    "        model[w1,w2]+= 1\n",
    "        \n",
    "        pos_w1 = nltk.pos_tag([w1])[0][1]\n",
    "        pos_w2 = nltk.pos_tag([w2])[0][1]\n",
    "        \n",
    "        \n",
    "        ## ACTUALLY GET THE POS FROM THE VOCAB.GR\n",
    "        pos_model[pos_w1,pos_w2]+=1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ESTIMATING SINGLE TOKEN FREQUENCIES IN example_sents_file\n",
    "## POPULATE THE df's freq from more sampling.\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "example_sents_file = open('example_sentences.txt','r')\n",
    "corpus = example_sents_file.readlines()\n",
    "tokenized_corpus = [nltk.word_tokenize(sent) for sent in corpus ]\n",
    "FLAT_TOKEN_LIST=[]\n",
    "for a in tokenized_corpus : \n",
    "    for b in a : \n",
    "        FLAT_TOKEN_LIST.append(b)\n",
    "        \n",
    "vocab_dist = FreqDist(FLAT_TOKEN_LIST)\n",
    "vocab_dist_df = pd.DataFrame(data=list(vocab_dist.values()),columns=['freq'])\n",
    "vocab_dist_df['token'] = list(vocab_dist.keys())\n",
    "vocab_dist_df['pos'] = vocab_dist_df.apply(lambda x: nltk.pos_tag([x['token']])[0][1],axis=1)\n",
    "\n",
    "total_count = vocab_dist_df['freq'].sum()\n",
    "vocab_dist_df['prob'] = vocab_dist_df.apply(lambda x: int(x['freq'])/total_count,axis=1 )\n",
    "#vocab_dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CREATING S2_new.gr\n",
    "\n",
    "\n",
    "## SCRIPT FOR NEW RULES IN S2.gr\n",
    "## THE IDEA IS TO GENERATE a -> b c \n",
    "## THE WEIGHTS ARE KEPT ONE FOR SYMPLICITY NOW.\n",
    "\n",
    "A_RULES = [\"_{}\".format(non_terminal) for non_terminal in POS_set ]\n",
    "A_RULES_SPACE = []\n",
    "A_RULES_SPACE.append(\"\")\n",
    "\n",
    "for i in range(len(A_RULES)) : \n",
    "    A_RULES_SPACE.append(A_RULES[i])\n",
    "\n",
    "\n",
    "S2_ALL_RULES=[]\n",
    "s2_new_file=open(\"S2_new.gr\",\"w\")\n",
    "\n",
    "\n",
    "for a in A_RULES : \n",
    "    s2_new_file.write(\"1\\tS2\\t{}\\n\".format(a))\n",
    "    \n",
    "## CREATE S2_new.gr with 1 a b c rules\n",
    "for a in A_RULES : \n",
    "    for b in POS_set : \n",
    "        for c in A_RULES_SPACE : \n",
    "            #print(\"1 {}\\t{}\\t{}\".format(a,b,c))\n",
    "            \n",
    "            if c == \"\" : \n",
    "                S2_ALL_RULES.append(\"1\\t{}\\t{}\\n\".format(a,b))\n",
    "                s2_new_file.write(\"1\\t{}\\t{}\\n\".format(a,b))\n",
    "            else : \n",
    "                S2_ALL_RULES.append(\"1\\t{}\\t{}\\t{}\\n\".format(a,b,c))\n",
    "                s2_new_file.write(\"1\\t{}\\t{}\\t{}\\n\".format(a,b,c))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "s2_new_file.close()\n",
    "#print(S2_ALL_RULES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_vocab_weights(df,vocab_file) : \n",
    "    for line in vocab_file : \n",
    "        if line[0] != \"#\" and re.match(r'[0-9|\\.]',line[0]) and line[0] != \" \" : ## A VALID GRAMMAR \n",
    "                \n",
    "            REGEX=r\"([0-9]*)\\ +([A-Za-z_]*)\\ +([A-Za-z_0-9\\?\\.\\,\\'\\;'\\!\\\"\\--\\-\\:]*)\"\n",
    "            contents = re.findall(REGEX,line)[0]\n",
    "            \n",
    "            old_weight = contents[0]\n",
    "            original_pos = contents[1]\n",
    "            token = contents[2]\n",
    "            \n",
    "            ## SEARCH IN THE DF\n",
    "            new_weight = df[df['token']==token]['freq']\n",
    "            if new_weight is None or new_weight==0 : \n",
    "                new_weight = old_weight\n",
    "            \n",
    "            NEW_LINE = \"{}\\t{}\\t{}\"\n",
    "                \n",
    "        else : \n",
    "            \n",
    "            vocab_file.write(line)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>penntree_lhs</th>\n",
       "      <th>penntree_rhs</th>\n",
       "      <th>combined</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5303</th>\n",
       "      <td>PP</td>\n",
       "      <td>(IN, NP)</td>\n",
       "      <td>PP -&gt; (IN, NP)</td>\n",
       "      <td>4045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18941</th>\n",
       "      <td>S</td>\n",
       "      <td>(NP-SBJ, VP)</td>\n",
       "      <td>S -&gt; (NP-SBJ, VP)</td>\n",
       "      <td>3391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9348</th>\n",
       "      <td>NP</td>\n",
       "      <td>(NP, PP)</td>\n",
       "      <td>NP -&gt; (NP, PP)</td>\n",
       "      <td>2188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1764</th>\n",
       "      <td>NP</td>\n",
       "      <td>(DT, NN)</td>\n",
       "      <td>NP -&gt; (DT, NN)</td>\n",
       "      <td>2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12697</th>\n",
       "      <td>VP</td>\n",
       "      <td>(TO, VP)</td>\n",
       "      <td>VP -&gt; (TO, VP)</td>\n",
       "      <td>1257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15772</th>\n",
       "      <td>PP-LOC</td>\n",
       "      <td>(IN, NP)</td>\n",
       "      <td>PP-LOC -&gt; (IN, NP)</td>\n",
       "      <td>1146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17928</th>\n",
       "      <td>SBAR</td>\n",
       "      <td>(-NONE-, S)</td>\n",
       "      <td>SBAR -&gt; (-NONE-, S)</td>\n",
       "      <td>844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3784</th>\n",
       "      <td>PP-CLR</td>\n",
       "      <td>(IN, NP)</td>\n",
       "      <td>PP-CLR -&gt; (IN, NP)</td>\n",
       "      <td>833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13954</th>\n",
       "      <td>VP</td>\n",
       "      <td>(VB, NP)</td>\n",
       "      <td>VP -&gt; (VB, NP)</td>\n",
       "      <td>805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25428</th>\n",
       "      <td>PP-TMP</td>\n",
       "      <td>(IN, NP)</td>\n",
       "      <td>PP-TMP -&gt; (IN, NP)</td>\n",
       "      <td>769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>VP</td>\n",
       "      <td>(MD, VP)</td>\n",
       "      <td>VP -&gt; (MD, VP)</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NP</td>\n",
       "      <td>(NNP, NNP)</td>\n",
       "      <td>NP -&gt; (NNP, NNP)</td>\n",
       "      <td>734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30702</th>\n",
       "      <td>NP</td>\n",
       "      <td>(JJ, NNS)</td>\n",
       "      <td>NP -&gt; (JJ, NNS)</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17297</th>\n",
       "      <td>VP</td>\n",
       "      <td>(VBD, SBAR)</td>\n",
       "      <td>VP -&gt; (VBD, SBAR)</td>\n",
       "      <td>631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27427</th>\n",
       "      <td>SBAR</td>\n",
       "      <td>(IN, S)</td>\n",
       "      <td>SBAR -&gt; (IN, S)</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14759</th>\n",
       "      <td>VP</td>\n",
       "      <td>(VBZ, VP)</td>\n",
       "      <td>VP -&gt; (VBZ, VP)</td>\n",
       "      <td>459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30227</th>\n",
       "      <td>NP-SBJ</td>\n",
       "      <td>(DT, NN)</td>\n",
       "      <td>NP-SBJ -&gt; (DT, NN)</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23377</th>\n",
       "      <td>NP-SBJ</td>\n",
       "      <td>(NP, PP)</td>\n",
       "      <td>NP-SBJ -&gt; (NP, PP)</td>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24484</th>\n",
       "      <td>NP</td>\n",
       "      <td>(NP, SBAR)</td>\n",
       "      <td>NP -&gt; (NP, SBAR)</td>\n",
       "      <td>409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>NP</td>\n",
       "      <td>(JJ, NN)</td>\n",
       "      <td>NP -&gt; (JJ, NN)</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28205</th>\n",
       "      <td>VP</td>\n",
       "      <td>(VBD, NP)</td>\n",
       "      <td>VP -&gt; (VBD, NP)</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24109</th>\n",
       "      <td>VP</td>\n",
       "      <td>(VBG, NP)</td>\n",
       "      <td>VP -&gt; (VBG, NP)</td>\n",
       "      <td>375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29630</th>\n",
       "      <td>S</td>\n",
       "      <td>(NP-SBJ-1, VP)</td>\n",
       "      <td>S -&gt; (NP-SBJ-1, VP)</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35068</th>\n",
       "      <td>NP</td>\n",
       "      <td>(QP, -NONE-)</td>\n",
       "      <td>NP -&gt; (QP, -NONE-)</td>\n",
       "      <td>365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31885</th>\n",
       "      <td>NP</td>\n",
       "      <td>(NP, PP-LOC)</td>\n",
       "      <td>NP -&gt; (NP, PP-LOC)</td>\n",
       "      <td>363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11926</th>\n",
       "      <td>VP</td>\n",
       "      <td>(VBD, VP)</td>\n",
       "      <td>VP -&gt; (VBD, VP)</td>\n",
       "      <td>361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22593</th>\n",
       "      <td>NP</td>\n",
       "      <td>(DT, NNS)</td>\n",
       "      <td>NP -&gt; (DT, NNS)</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4633</th>\n",
       "      <td>NP-SBJ</td>\n",
       "      <td>(NNP, NNP)</td>\n",
       "      <td>NP-SBJ -&gt; (NNP, NNP)</td>\n",
       "      <td>357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27027</th>\n",
       "      <td>VP</td>\n",
       "      <td>(VBP, VP)</td>\n",
       "      <td>VP -&gt; (VBP, VP)</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23794</th>\n",
       "      <td>PP</td>\n",
       "      <td>(TO, NP)</td>\n",
       "      <td>PP -&gt; (TO, NP)</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42174</th>\n",
       "      <td>NP-ADV-1</td>\n",
       "      <td>(DT, NN)</td>\n",
       "      <td>NP-ADV-1 -&gt; (DT, NN)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42175</th>\n",
       "      <td>NP-PRD</td>\n",
       "      <td>(NP, ADVP)</td>\n",
       "      <td>NP-PRD -&gt; (NP, ADVP)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42176</th>\n",
       "      <td>SBAR</td>\n",
       "      <td>(WHNP-163, S)</td>\n",
       "      <td>SBAR -&gt; (WHNP-163, S)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42177</th>\n",
       "      <td>SBAR-NOM-SBJ</td>\n",
       "      <td>(WHNP-164, S)</td>\n",
       "      <td>SBAR-NOM-SBJ -&gt; (WHNP-164, S)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42192</th>\n",
       "      <td>SBAR</td>\n",
       "      <td>(WHNP-165, S)</td>\n",
       "      <td>SBAR -&gt; (WHNP-165, S)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42193</th>\n",
       "      <td>QP-1</td>\n",
       "      <td>(CC, RB)</td>\n",
       "      <td>QP-1 -&gt; (CC, RB)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42194</th>\n",
       "      <td>SBAR</td>\n",
       "      <td>(WHNP-166, S)</td>\n",
       "      <td>SBAR -&gt; (WHNP-166, S)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42201</th>\n",
       "      <td>S</td>\n",
       "      <td>(NP-SBJ-97, VP)</td>\n",
       "      <td>S -&gt; (NP-SBJ-97, VP)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42202</th>\n",
       "      <td>NP-SBJ-97</td>\n",
       "      <td>(DT, NN)</td>\n",
       "      <td>NP-SBJ-97 -&gt; (DT, NN)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42205</th>\n",
       "      <td>ADVP-TMP</td>\n",
       "      <td>(JJ, RP)</td>\n",
       "      <td>ADVP-TMP -&gt; (JJ, RP)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42209</th>\n",
       "      <td>SBAR</td>\n",
       "      <td>(WHNP-167, S)</td>\n",
       "      <td>SBAR -&gt; (WHNP-167, S)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42210</th>\n",
       "      <td>NP</td>\n",
       "      <td>(NP, NNPS)</td>\n",
       "      <td>NP -&gt; (NP, NNPS)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42211</th>\n",
       "      <td>SBAR</td>\n",
       "      <td>(WHNP-168, S)</td>\n",
       "      <td>SBAR -&gt; (WHNP-168, S)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42213</th>\n",
       "      <td>S</td>\n",
       "      <td>(NP-SBJ-99, VP)</td>\n",
       "      <td>S -&gt; (NP-SBJ-99, VP)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42243</th>\n",
       "      <td>NP-SBJ-103</td>\n",
       "      <td>(NP, SBAR)</td>\n",
       "      <td>NP-SBJ-103 -&gt; (NP, SBAR)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42214</th>\n",
       "      <td>NP-SBJ-99</td>\n",
       "      <td>(DT, NN)</td>\n",
       "      <td>NP-SBJ-99 -&gt; (DT, NN)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42215</th>\n",
       "      <td>SBAR</td>\n",
       "      <td>(WHNP-170, S)</td>\n",
       "      <td>SBAR -&gt; (WHNP-170, S)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42216</th>\n",
       "      <td>SBAR</td>\n",
       "      <td>(WHNP-171, S)</td>\n",
       "      <td>SBAR -&gt; (WHNP-171, S)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42217</th>\n",
       "      <td>S</td>\n",
       "      <td>(NP-SBJ-100, VP)</td>\n",
       "      <td>S -&gt; (NP-SBJ-100, VP)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42218</th>\n",
       "      <td>S</td>\n",
       "      <td>(NP-SBJ, PP-LOC-PRD)</td>\n",
       "      <td>S -&gt; (NP-SBJ, PP-LOC-PRD)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42219</th>\n",
       "      <td>VP</td>\n",
       "      <td>(VBG, SBAR-TMP)</td>\n",
       "      <td>VP -&gt; (VBG, SBAR-TMP)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42220</th>\n",
       "      <td>VP</td>\n",
       "      <td>(POS, ADJP-PRD)</td>\n",
       "      <td>VP -&gt; (POS, ADJP-PRD)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42221</th>\n",
       "      <td>SBAR</td>\n",
       "      <td>(WHNP-172, S)</td>\n",
       "      <td>SBAR -&gt; (WHNP-172, S)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42226</th>\n",
       "      <td>ADVP-DIR</td>\n",
       "      <td>(RBR, PP)</td>\n",
       "      <td>ADVP-DIR -&gt; (RBR, PP)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42227</th>\n",
       "      <td>PP-CLR</td>\n",
       "      <td>(TO, NP-1)</td>\n",
       "      <td>PP-CLR -&gt; (TO, NP-1)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42228</th>\n",
       "      <td>NP-1</td>\n",
       "      <td>(DT, NNP)</td>\n",
       "      <td>NP-1 -&gt; (DT, NNP)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42231</th>\n",
       "      <td>NP</td>\n",
       "      <td>(DT, RB)</td>\n",
       "      <td>NP -&gt; (DT, RB)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42239</th>\n",
       "      <td>NP-SBJ-102</td>\n",
       "      <td>(NNP, NNS)</td>\n",
       "      <td>NP-SBJ-102 -&gt; (NNP, NNS)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42242</th>\n",
       "      <td>S</td>\n",
       "      <td>(NP-SBJ-103, VP)</td>\n",
       "      <td>S -&gt; (NP-SBJ-103, VP)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43349</th>\n",
       "      <td>NP-SBJ</td>\n",
       "      <td>(NNP, NX)</td>\n",
       "      <td>NP-SBJ -&gt; (NNP, NX)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1848 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       penntree_lhs          penntree_rhs                       combined  freq\n",
       "5303             PP              (IN, NP)                 PP -> (IN, NP)  4045\n",
       "18941             S          (NP-SBJ, VP)              S -> (NP-SBJ, VP)  3391\n",
       "9348             NP              (NP, PP)                 NP -> (NP, PP)  2188\n",
       "1764             NP              (DT, NN)                 NP -> (DT, NN)  2020\n",
       "12697            VP              (TO, VP)                 VP -> (TO, VP)  1257\n",
       "15772        PP-LOC              (IN, NP)             PP-LOC -> (IN, NP)  1146\n",
       "17928          SBAR           (-NONE-, S)            SBAR -> (-NONE-, S)   844\n",
       "3784         PP-CLR              (IN, NP)             PP-CLR -> (IN, NP)   833\n",
       "13954            VP              (VB, NP)                 VP -> (VB, NP)   805\n",
       "25428        PP-TMP              (IN, NP)             PP-TMP -> (IN, NP)   769\n",
       "1005             VP              (MD, VP)                 VP -> (MD, VP)   759\n",
       "0                NP            (NNP, NNP)               NP -> (NNP, NNP)   734\n",
       "30702            NP             (JJ, NNS)                NP -> (JJ, NNS)   653\n",
       "17297            VP           (VBD, SBAR)              VP -> (VBD, SBAR)   631\n",
       "27427          SBAR               (IN, S)                SBAR -> (IN, S)   492\n",
       "14759            VP             (VBZ, VP)                VP -> (VBZ, VP)   459\n",
       "30227        NP-SBJ              (DT, NN)             NP-SBJ -> (DT, NN)   458\n",
       "23377        NP-SBJ              (NP, PP)             NP-SBJ -> (NP, PP)   417\n",
       "24484            NP            (NP, SBAR)               NP -> (NP, SBAR)   409\n",
       "11536            NP              (JJ, NN)                 NP -> (JJ, NN)   390\n",
       "28205            VP             (VBD, NP)                VP -> (VBD, NP)   378\n",
       "24109            VP             (VBG, NP)                VP -> (VBG, NP)   375\n",
       "29630             S        (NP-SBJ-1, VP)            S -> (NP-SBJ-1, VP)   371\n",
       "35068            NP          (QP, -NONE-)             NP -> (QP, -NONE-)   365\n",
       "31885            NP          (NP, PP-LOC)             NP -> (NP, PP-LOC)   363\n",
       "11926            VP             (VBD, VP)                VP -> (VBD, VP)   361\n",
       "22593            NP             (DT, NNS)                NP -> (DT, NNS)   358\n",
       "4633         NP-SBJ            (NNP, NNP)           NP-SBJ -> (NNP, NNP)   357\n",
       "27027            VP             (VBP, VP)                VP -> (VBP, VP)   337\n",
       "23794            PP              (TO, NP)                 PP -> (TO, NP)   315\n",
       "...             ...                   ...                            ...   ...\n",
       "42174      NP-ADV-1              (DT, NN)           NP-ADV-1 -> (DT, NN)     1\n",
       "42175        NP-PRD            (NP, ADVP)           NP-PRD -> (NP, ADVP)     1\n",
       "42176          SBAR         (WHNP-163, S)          SBAR -> (WHNP-163, S)     1\n",
       "42177  SBAR-NOM-SBJ         (WHNP-164, S)  SBAR-NOM-SBJ -> (WHNP-164, S)     1\n",
       "42192          SBAR         (WHNP-165, S)          SBAR -> (WHNP-165, S)     1\n",
       "42193          QP-1              (CC, RB)               QP-1 -> (CC, RB)     1\n",
       "42194          SBAR         (WHNP-166, S)          SBAR -> (WHNP-166, S)     1\n",
       "42201             S       (NP-SBJ-97, VP)           S -> (NP-SBJ-97, VP)     1\n",
       "42202     NP-SBJ-97              (DT, NN)          NP-SBJ-97 -> (DT, NN)     1\n",
       "42205      ADVP-TMP              (JJ, RP)           ADVP-TMP -> (JJ, RP)     1\n",
       "42209          SBAR         (WHNP-167, S)          SBAR -> (WHNP-167, S)     1\n",
       "42210            NP            (NP, NNPS)               NP -> (NP, NNPS)     1\n",
       "42211          SBAR         (WHNP-168, S)          SBAR -> (WHNP-168, S)     1\n",
       "42213             S       (NP-SBJ-99, VP)           S -> (NP-SBJ-99, VP)     1\n",
       "42243    NP-SBJ-103            (NP, SBAR)       NP-SBJ-103 -> (NP, SBAR)     1\n",
       "42214     NP-SBJ-99              (DT, NN)          NP-SBJ-99 -> (DT, NN)     1\n",
       "42215          SBAR         (WHNP-170, S)          SBAR -> (WHNP-170, S)     1\n",
       "42216          SBAR         (WHNP-171, S)          SBAR -> (WHNP-171, S)     1\n",
       "42217             S      (NP-SBJ-100, VP)          S -> (NP-SBJ-100, VP)     1\n",
       "42218             S  (NP-SBJ, PP-LOC-PRD)      S -> (NP-SBJ, PP-LOC-PRD)     1\n",
       "42219            VP       (VBG, SBAR-TMP)          VP -> (VBG, SBAR-TMP)     1\n",
       "42220            VP       (POS, ADJP-PRD)          VP -> (POS, ADJP-PRD)     1\n",
       "42221          SBAR         (WHNP-172, S)          SBAR -> (WHNP-172, S)     1\n",
       "42226      ADVP-DIR             (RBR, PP)          ADVP-DIR -> (RBR, PP)     1\n",
       "42227        PP-CLR            (TO, NP-1)           PP-CLR -> (TO, NP-1)     1\n",
       "42228          NP-1             (DT, NNP)              NP-1 -> (DT, NNP)     1\n",
       "42231            NP              (DT, RB)                 NP -> (DT, RB)     1\n",
       "42239    NP-SBJ-102            (NNP, NNS)       NP-SBJ-102 -> (NNP, NNS)     1\n",
       "42242             S      (NP-SBJ-103, VP)          S -> (NP-SBJ-103, VP)     1\n",
       "43349        NP-SBJ             (NNP, NX)            NP-SBJ -> (NNP, NX)     1\n",
       "\n",
       "[1848 rows x 4 columns]"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DISTRIBUTION FROM TREE-BANK\n",
    "\n",
    "\n",
    "from nltk import Nonterminal, nonterminals, Production, CFG, Tree\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from nltk import bigrams, trigrams \n",
    "from collections import defaultdict\n",
    "penn_rules_freq = defaultdict(int)\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "\n",
    "def find_freq(row) : \n",
    "    lhs = row['penntree_lhs']\n",
    "    rhs = row['penntree_rhs']\n",
    "    penn_rules_freq[lhs,rhs]+=1\n",
    "    \n",
    "sents_df.apply(find_freq,axis=1)\n",
    "\n",
    "def set_freq(row,freq_set) : \n",
    "    dict_set = dict(freq_set)\n",
    "    lhs = row['penntree_lhs'] ## type is non-terminal\n",
    "    rhs = row['penntree_rhs']\n",
    "    \n",
    "    for elems in dict_set : \n",
    "        lhs_1 = elems[0]\n",
    "        rhs_1 = elems[1]\n",
    "        \n",
    "        if lhs_1 == lhs and rhs_1==rhs : \n",
    "            #print(penn_rules_freq)\n",
    "            return penn_rules_freq[elems]\n",
    "        \n",
    "        \n",
    "## TESTING ONLY AT FIRST 100 PARSED SENTS\n",
    "\n",
    "PARSED_SENTS=[]\n",
    "LHS=[]\n",
    "RHS=[]\n",
    "\n",
    "NUMBER_OF_TREE_SAMPLES=10000\n",
    "for tree in treebank.parsed_sents()[:NUMBER_OF_TREE_SAMPLES] : \n",
    "    \n",
    "    single_sent_prods = tree.productions()\n",
    "    for single_prod in single_sent_prods : \n",
    "       # print(single_prod)\n",
    "        \n",
    "        rhs_tuple = single_prod.rhs()\n",
    "        lhs_tuple = single_prod.lhs()\n",
    "        ## TAKE ONLY RULES WITH TWO NON-TERMS ON RHS, AND IGNORE THE REST (NOT IN CNF FORM)\n",
    "        if len(rhs_tuple) == 2 : \n",
    "            LHS.append(lhs_tuple)\n",
    "            RHS.append(rhs_tuple)\n",
    "            \n",
    "            \n",
    "sents_df=pd.DataFrame()\n",
    "\n",
    "\n",
    "sents_df['penntree_lhs']=LHS\n",
    "sents_df['penntree_rhs']=RHS\n",
    "sents_df['combined']= sents_df.apply(lambda x:\"{} -> {}\".format(x['penntree_lhs'],x['penntree_rhs']),axis=1)\n",
    "new_df=sents_df['combined'].value_counts().to_frame().reset_index()\n",
    "new_df.columns=['combined','freq']\n",
    "sents_df=sents_df.merge(new_df,on=['combined'])\n",
    "sents_df=sents_df.drop_duplicates()\n",
    "sents_df.sort_values(['freq'],ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_df.groupby('penntree_lhs')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4028"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
